{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fbprophet \n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "# don't do this at home\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# high resolution \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate():\n",
    "\"\"\"\n",
    ">> takes in \n",
    "    > Census 1970-2010 dataframe (1 df)\n",
    "        >> total population by Place measurements\n",
    "    > American Community Survey (ACS) 2011-2017 dataframes (7 dfs)\n",
    "        >> total population (age & sex) by Place \n",
    "\n",
    ">> forges DataFrame of places that have \n",
    "    > at least one (1) recording for Census years 1970-2010\n",
    "    > at least one (1) recording for ACS years 2011-2015\n",
    "\n",
    ">> test our model v. base on\n",
    "    > random sample 100 Places\n",
    "    > random sample 100 Places from bottom half population size\n",
    "    > random sample 100 Places from top half population size\n",
    "\"\"\"\n",
    "\n",
    "'''load Train data'''\n",
    "# population by Place Census 1970-2010 measurements\n",
    "load_census_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "\n",
    "'''load Test data'''\n",
    "# population by Place ACS 2016\n",
    "load_acs_20l6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2017\n",
    "load_acs_20l7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "\n",
    "'''find common places across Census and each train ACS'''\n",
    "# identify Places measured in 2011 ACS [0 == 'Geography']\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2012 ACS \n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS \n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS \n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]\n",
    "\n",
    "# cross 2011-2015, keep coexisting Places\n",
    "train_places = [place for place in acs11places if place in acs12places and acs13places and acs14places and acs15places]\n",
    "\n",
    "\n",
    "'''find common places across 2016 & 2017 (test ACSs)'''\n",
    "# identify Places measured in 2016 ACS (29575) [0 == 'Geography']\n",
    "acs16places = [place for place in load_acs_20l6['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2017 ACS (29577)\n",
    "acs17places = [place for place in load_acs_20l7['GEO.display-label']]\n",
    "\n",
    "# cross 2017 Places w/ 2016 Places, keep coexisting Places (29551)\n",
    "base_places = [place for place in acs17places if place in acs16places]\n",
    "\n",
    "\n",
    "'''find common Places across the Places our model will train on {train_places} \n",
    "    and the Places our model can predict on {base_places}'''\n",
    "# identify Places we can compare our predictions with\n",
    "measureable_places = [place for place in train_places if place in base_places]\n",
    "\n",
    "\n",
    "'''clean Census 1970-2010 df (Train)'''\n",
    "# identify columns needed to make GEO.display-label column (so can pair with ACS DataFrames) \n",
    "for_geo_displays = ['PLACE','STATE']\n",
    "# pull those columns \n",
    "to_geo_displays = load_census_place[for_geo_displays]\n",
    "\n",
    "# mold PLACE column into list with Place formatted as is in GEO.display-label\n",
    "places_70_10 = [place + ', ' for place in to_geo_displays.PLACE]\n",
    "\n",
    "# list paired State for each Place\n",
    "states_70_10 = [state for state in to_geo_displays.STATE]\n",
    "\n",
    "# merge places_70_10 and states_70_10 into list formatted as GEO.display-label column\n",
    "GEO_display_label = [ places_70_10[i] + states_70_10[i] for i in range(len(places_70_10))]\n",
    "\n",
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "# set base dataframe using Census (1970-2010) measurements \n",
    "pop_place_70_10_ = load_census_place[place_cols_of_interest]\n",
    "\n",
    "# add GEO.display-label column from GEO_display_label list\n",
    "pop_place_70_10_['GEO.display-label'] = GEO_display_label\n",
    "\n",
    "\n",
    "'''clean American Community Survey (ACS) 2011-2015 dataframes (Train)'''\n",
    "# ID columns we will be using\n",
    "columns = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "# convert 2011\n",
    "acs_20l1 = load_acs_20l1[columns]\n",
    "# convert 2012\n",
    "acs_20l2 = load_acs_20l2[columns]\n",
    "# convert 2013\n",
    "acs_20l3 = load_acs_20l3[columns]\n",
    "# convert 2014\n",
    "acs_20l4 = load_acs_20l4[columns]\n",
    "# convert 2015\n",
    "acs_20l5 = load_acs_20l5[columns]\n",
    "\n",
    "\n",
    "'''convert Train years to reflect Places only seen in measureable_places'''\n",
    "# drop Census Places not ideal for measurement (29346)\n",
    "census_place_populations = pop_place_70_10_.loc[pop_place_70_10_['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2011_place_populations = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2012_place_populations = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2013_place_populations = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2014_place_populations = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2015_place_populations = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "\n",
    "'''clean ACS 2016 & 2017 dataframes (Test)\n",
    "    take a sample of 100 Places to score our model'''\n",
    "# identify 2016/2017 columns of interest (to measure against)\n",
    "test_col_of_i = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_16_ = load_acs_20l6[test_col_of_i]\n",
    "# realize ACS 2016 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_16_df_ = testd_16_.loc[testd_16_['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_17_ = load_acs_20l7[test_col_of_i]\n",
    "# realize ACS 2017 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_17_df_ = testd_17_.loc[testd_17_['GEO.display-label'].isin(measureable_places)]\n",
    "# change 2017 populations from strings to ints\n",
    "test_17_ints = [int(i) for i in test_17_df_.HC01_EST_VC01]\n",
    "test_17_df_.HC01_EST_VC01 = test_17_ints\n",
    "# forget Places with Total Population less than 1000 value in 2017 (will be sampling from 2017, values of 0 in 2016 are also 0 in 2017 or do not exits)\n",
    "test_17_df_ = test_17_df_.loc[test_17_df_.HC01_EST_VC01 > 999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pleasanton city, California']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "identify place of interest\n",
    "'''\n",
    "\n",
    "# locate and pull cities of interest\n",
    "city_sfo = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'San Francisco city, California']\n",
    "city_nyc = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'New York city, New York']\n",
    "city_nola = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'New Orleans city, Louisiana']\n",
    "city_houston = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'Houston city, Texas']\n",
    "city_bville = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'Bentonville city, Arkansas']\n",
    "city_sidney = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'Sidney city, Nebraska']\n",
    "city_pleasanton = test_17_df_.loc[test_17_df_['GEO.display-label'] == 'Pleasanton city, California']\n",
    "sample_one_hunnit = pd.concat([city_sfo,city_nyc,city_nola,city_houston,city_bville,city_sidney,city_pleasanton])\n",
    "# list Places for conversion of other Datas\n",
    "sample_places = [place for place in city_pleasanton['GEO.display-label']]\n",
    "sample_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''adjust Train dataframes to sampled Places'''\n",
    "# shrink Census DataFrame to sampled Places\n",
    "_s_census_ = census_place_populations.loc[census_place_populations['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2011 ACS df to sampled Places \n",
    "_s_acs_2011_ = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2012 ACS DataFrame to sampled Places \n",
    "_s_acs_2012_ = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2013 ACS df to Places in sample  \n",
    "_s_acs_2013_ = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2014 ACS DataFrame to sampled Places \n",
    "_s_acs_2014_ = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2015 ACS df to sampled Places \n",
    "_s_acs_2015_ = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(sample_places)]\n",
    "\n",
    "\n",
    "'''adjust Test dataframes to sampled Places'''\n",
    "# 2016 ACS df to sampled Places \n",
    "test_16_df = test_16_df_.loc[test_16_df_['GEO.display-label'].isin(sample_places)]\n",
    "# 2017 ACS DataFrame to sampled Places \n",
    "test_17_df = test_17_df_.loc[test_17_df_['GEO.display-label'].isin(sample_places)]\n",
    "\n",
    "\n",
    "'''forge Train DataFrame'''\n",
    "# set Census index to Places, and forget Place column \n",
    "s_census_ = _s_census_.copy().set_index(_s_census_['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# rename Census columns to years for later datetime conversion\n",
    "s_census_.columns = ['1970','1980','1990','2000','2010']\n",
    "\n",
    "# set 2011 index to Places \n",
    "s_acs_2011_ = _s_acs_2011_.copy().set_index(_s_acs_2011_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2011_.columns = ['no','2011']\n",
    "s_acs_2011_ = s_acs_2011_['2011']\n",
    "\n",
    "# set 2012 index to Places \n",
    "s_acs_2012_ = _s_acs_2012_.copy().set_index(_s_acs_2012_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2012_.columns = ['no','2012']\n",
    "s_acs_2012_ = s_acs_2012_['2012']\n",
    "\n",
    "# set 2013 index to Places \n",
    "s_acs_2013_ = _s_acs_2013_.copy().set_index(_s_acs_2013_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2013_.columns = ['no','2013']\n",
    "s_acs_2013_ = s_acs_2013_['2013']\n",
    "\n",
    "# set 2014 index to Places \n",
    "s_acs_2014_ = _s_acs_2014_.copy().set_index(_s_acs_2014_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2014_.columns = ['no','2014']\n",
    "s_acs_2014_ = s_acs_2014_['2014']\n",
    "\n",
    "# set 2015 index to Places \n",
    "s_acs_2015_ = _s_acs_2015_.copy().set_index(_s_acs_2015_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2015_.columns = ['no','2015']\n",
    "s_acs_2015_ = s_acs_2015_['2015']\n",
    "\n",
    "# forge Train DataFrame and convert NaN values to 0 (assumes population not measured is 0) \n",
    "train_df = pd.concat([s_census_,s_acs_2011_,s_acs_2012_,s_acs_2013_,s_acs_2014_,s_acs_2015_],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.0.\n"
     ]
    }
   ],
   "source": [
    "'''forecast 2016, 2017 & 2018 populations using model for each sample Place'''\n",
    "# set out route for forecast tables\n",
    "out = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds = []\n",
    "# make DataFrame of column values as datetime\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.1)\n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=4, freq='Y' )\n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    # tag and bag (forecast table)\n",
    "    out.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds.append([\n",
    "        forecast.loc[forecast.ds == '2016-12-31'].yhat.values[0],\n",
    "        forecast.loc[forecast.ds == '2017-12-31'].yhat.values[0],\n",
    "        forecast.loc[forecast.ds == '2018-12-31'].yhat.values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make Baseline predictions of 2016 and 2017 population on sample Places'''\n",
    "# set out route\n",
    "baseline_preds = []\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    # identify 2011 population\n",
    "    p11 = int(df['2011'])\n",
    "    # identify 2012 population\n",
    "    p12 = int(df['2012'])\n",
    "    # identify 2013 population\n",
    "    p13 = int(df['2013'])\n",
    "    # identify 2014 population\n",
    "    p14 = int(df['2014'])\n",
    "    # identify 2015 population\n",
    "    p15 = int(df['2015'])\n",
    "    # calculate average change over time for 2016\n",
    "    avg_16_change = np.mean(((p15-p14)+(p14-p13)))\n",
    "    # make 2016 prediction \n",
    "    p_16 = p15 + avg_16_change\n",
    "    # calculate average change over time for 2017\n",
    "    avg_17_change = np.mean(((p15-p14)+(p_16-p15)))\n",
    "    # make 2017 prediction \n",
    "    p_17 = p_16 + avg_16_change\n",
    "    # pair prediction, tag & bag\n",
    "    baseline_preds.append([p_16,p_17])\n",
    "\n",
    "'''pull actual measurements for 2016 and 2017 population for each sample Place'''\n",
    "# actual populations for 2016\n",
    "test_16 = [actual_population for actual_population in test_16_df.HC01_EST_VC01]\n",
    "# actual populations for 2017\n",
    "test_17 = [actual_population for actual_population in test_17_df.HC01_EST_VC01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2495.5513721943134 1484.0\n",
      "5791.195187820558 2710.0\n"
     ]
    }
   ],
   "source": [
    "'''2016'''\n",
    "train_preds_16 = [float(pred[0]) for pred in train_preds]\n",
    "test_16_ = [float(act) for act in test_16]\n",
    "MODEL_rmse_exrate16 = sqrt(mean_squared_error(y_true=test_16_,y_pred=train_preds_16))\n",
    "base_preds_16 = [float(pred[0]) for pred in baseline_preds]\n",
    "test_16_ = [float(act) for act in test_16]\n",
    "BASE_rmse_exrate16 = sqrt(mean_squared_error(y_true=test_16_,y_pred=base_preds_16))\n",
    "print(MODEL_rmse_exrate16,BASE_rmse_exrate16 ) \n",
    "\n",
    "'''2017'''\n",
    "train_preds_17 = [float(pred[1]) for pred in train_preds]\n",
    "test_17_ = [float(act) for act in test_17]\n",
    "MODEL_rmse_exrate17 = sqrt(mean_squared_error(y_true=test_17_,y_pred=train_preds_17))\n",
    "base_preds_17 = [float(pred[1]) for pred in baseline_preds]\n",
    "test_17_ = [float(act) for act in test_17]\n",
    "BASE_rmse_exrate17 = sqrt(mean_squared_error(y_true=test_17_,y_pred=base_preds_17))\n",
    "print(MODEL_rmse_exrate17,BASE_rmse_exrate17)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([74550.44862780569], [73549.80481217944])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds_16,train_preds_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([78530.0], [82051.0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_preds_16,base_preds_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['77046'], [79341])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_16,test_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
