{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fbprophet \n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "# don't do this at home\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# def validate():\n",
    "\"\"\"\n",
    ">> takes in \n",
    "    > Census 1970-2010 dataframe (1 df)\n",
    "        >> total population by Place measurements\n",
    "    > American Community Survey (ACS) 2011-2017 dataframes (7 dfs)\n",
    "        >> total population (age & sex) by Place \n",
    "\n",
    ">> forges DataFrame of places that have \n",
    "    > at least one (1) recording for Census years 1970-2010\n",
    "    > at least one (1) recording for ACS years 2011-2015\n",
    "\n",
    ">> test our model v. base on\n",
    "    > random sample 100 Places\n",
    "    > random sample 100 Places from bottom half population size\n",
    "    > random sample 100 Places from top half population size\n",
    "\"\"\"\n",
    "\n",
    "'''load Train data'''\n",
    "# population by Place Census 1970-2010 measurements\n",
    "load_census_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "\n",
    "'''load Test data'''\n",
    "# population by Place ACS 2016\n",
    "load_acs_20l6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2017\n",
    "load_acs_20l7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "'''find common places across Census and each train ACS'''\n",
    "# identify Places measured in 2011 ACS [0 == 'Geography'] (# 29517)\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2012 ACS  (# 29510)\n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS (# 29510)\n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS (# 29550)\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS (# 29575)\n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]\n",
    "\n",
    "# cross 2011-2015, keep coexisting Places (# 29475)\n",
    "train_places = [place for place in acs11places if place in acs12places and acs13places and acs14places and acs15places]\n",
    "\n",
    "\"\"\"find common places across 2016 & 2017 (test ACSs)\n",
    "\"\"\"\n",
    "# identify Places measured in 2016 ACS (# 29574) [0 == 'Geography']\n",
    "acs16places = [place for place in load_acs_20l6['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2017 ACS (# 29577)\n",
    "acs17places = [place for place in load_acs_20l7['GEO.display-label']]\n",
    "\n",
    "# cross 2017 Places w/ 2016 Places, keep coexisting Places (# 29550)\n",
    "base_places = [place for place in acs17places if place in acs16places]\n",
    "\n",
    "\"\"\"find common Places across the Places our model will train on {train_places} \n",
    "    and the Places our model can predict on {base_places}\n",
    "\"\"\"\n",
    "# identify Places we can compare our predictions with (# 29341)\n",
    "pre_measureable_places = [place for place in train_places if place in base_places]\n",
    "\n",
    "\"\"\"clean Census 1970-2010 df (Train)\n",
    "\"\"\"\n",
    "# identify columns needed to make GEO.display-label column (so can pair with ACS DataFrames) \n",
    "for_geo_displays = ['PLACE','STATE']\n",
    "# pull those columns \n",
    "to_geo_displays = load_census_place[for_geo_displays]\n",
    "\n",
    "# mold PLACE column into list with Place formatted as is in GEO.display-label\n",
    "places_70_10 = [place + ', ' for place in to_geo_displays.PLACE]\n",
    "\n",
    "# list paired State for each Place\n",
    "states_70_10 = [state for state in to_geo_displays.STATE]\n",
    "\n",
    "# merge places_70_10 and states_70_10 into list formatted as GEO.display-label column\n",
    "GEO_display_label = [ places_70_10[i] + states_70_10[i] for i in range(len(places_70_10))]\n",
    "\n",
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "# set base dataframe using Census (1970-2010) measurements \n",
    "pop_place_70_10_ = load_census_place[place_cols_of_interest]\n",
    "\n",
    "# add GEO.display-label column from GEO_display_label list (# 31436)\n",
    "pop_place_70_10_['GEO.display-label'] = GEO_display_label\n",
    "\n",
    "# forget places without measurements for at least 3 of the 5 census measurement years (# 23027)\n",
    "at_least_3_70_10_ = pop_place_70_10_.dropna(axis=0,thresh=4)\n",
    "# forget places with measurements of 0 for 2000 (# 23018)\n",
    "not_0_for_2000_ = at_least_3_70_10_.loc[at_least_3_70_10_.AV0AA2000 != 0]\n",
    "# forget places with measurements of 0 for 2010 (# 23016)\n",
    "pop_place_70_10_ = not_0_for_2000_.loc[not_0_for_2000_.AV0AA2010 != 0]\n",
    "\n",
    "# note the remaining places (total # = 23016)\n",
    "census_places = [place for place in pop_place_70_10_['GEO.display-label']]\n",
    "# adjust measurable places to reflect places with census measurements (total # = 22506)\n",
    "measureable_places = [place for place in pre_measureable_places if place in census_places]\n",
    "\n",
    "\"\"\"clean American Community Survey (ACS) 2011-2015 dataframes (Train)\n",
    "\"\"\"\n",
    "# ID columns we will be using\n",
    "columns = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "# convert 2011\n",
    "acs_20l1 = load_acs_20l1[columns]\n",
    "# convert 2012\n",
    "acs_20l2 = load_acs_20l2[columns]\n",
    "# convert 2013\n",
    "acs_20l3 = load_acs_20l3[columns]\n",
    "# convert 2014\n",
    "acs_20l4 = load_acs_20l4[columns]\n",
    "# convert 2015\n",
    "acs_20l5 = load_acs_20l5[columns]\n",
    "\n",
    "\"\"\"convert Train years to reflect Places only seen in measureable_places\n",
    "\"\"\"\n",
    "# drop Census Places not ideal for measurement (29346)\n",
    "census_place_populations = pop_place_70_10_.loc[pop_place_70_10_['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2011_place_populations = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2012 ACS Places not ideal for measurement (29341)\n",
    "acs_2012_place_populations = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2013 ACS Places not ideal for measurement (29341) \n",
    "acs_2013_place_populations = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2014 ACS Places not ideal for measurement (29341) \n",
    "acs_2014_place_populations = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2015 ACS Places not ideal for measurement (29341) \n",
    "acs_2015_place_populations = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "\"\"\"clean ACS 2016 & 2017 dataframes (Test)\n",
    "    take a sample of n Places to score our model\n",
    "\"\"\"\n",
    "# identify 2016/2017 columns of interest (to measure against)\n",
    "test_col_of_i = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_16_ = load_acs_20l6[test_col_of_i]\n",
    "# realize ACS 2016 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_16_df_ = testd_16_.loc[testd_16_['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_17_ = load_acs_20l7[test_col_of_i]\n",
    "# realize ACS 2017 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_17_df_ = testd_17_.loc[testd_17_['GEO.display-label'].isin(measureable_places)]\n",
    "# conver\n",
    "test_17_1000_pops = [float(population) for population in test_17_df_.HC01_EST_VC01]\n",
    "# convert test_17_df_ populations to floats (numbers, from strings) \n",
    "test_17_df_.HC01_EST_VC01 = test_17_1000_pops\n",
    "# forget Places with 2017 measured population less than 1,000 (13218 places remain)\n",
    "test_17_df_ = test_17_df_.loc[test_17_df_.HC01_EST_VC01 >= 1000]\n",
    "\n",
    "# sample Baseline data for Places to evaluate model \n",
    "sample_one_hunnit = test_17_df_.sample(2500)\n",
    "# list Places for conversion of other Datas\n",
    "sample_places = [place for place in sample_one_hunnit['GEO.display-label']]\n",
    "\n",
    "\"\"\"adjust Train dataframes to sampled Places\n",
    "\"\"\"\n",
    "# shrink Census DataFrame to sampled Places\n",
    "_s_census_ = census_place_populations.loc[census_place_populations['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2011 ACS df to sampled Places \n",
    "_s_acs_2011_ = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2012 ACS DataFrame to sampled Places \n",
    "_s_acs_2012_ = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2013 ACS df to Places in sample  \n",
    "_s_acs_2013_ = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2014 ACS DataFrame to sampled Places \n",
    "_s_acs_2014_ = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2015 ACS df to sampled Places \n",
    "_s_acs_2015_ = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(sample_places)]\n",
    "\n",
    "\"\"\"adjust Test dataframes to sampled Places\n",
    "\"\"\"\n",
    "# 2016 ACS df to sampled Places \n",
    "test_16_df = test_16_df_.loc[test_16_df_['GEO.display-label'].isin(sample_places)]\n",
    "# reset 2016 ACS df index\n",
    "test_16_df = test_16_df.set_index(test_16_df['GEO.display-label'])\n",
    "\n",
    "# 2017 ACS DataFrame to sampled Places \n",
    "test_17_df = test_17_df_.loc[test_17_df_['GEO.display-label'].isin(sample_places)]\n",
    "# reset 2017 ACS df index\n",
    "test_17_df = test_17_df.set_index(test_17_df['GEO.display-label'])\n",
    "\n",
    "\"\"\"forge combined Train/Test DataFrame \n",
    "    --ensure Place order remains unchanged\n",
    "\"\"\"\n",
    "# set Census index to Places, and forget Place column \n",
    "s_census_ = _s_census_.copy().set_index(_s_census_['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# rename Census columns to years for later datetime conversion\n",
    "s_census_.columns = ['1970','1980','1990','2000','2010']\n",
    "\n",
    "# set 2011 index to Places \n",
    "s_acs_2011_ = _s_acs_2011_.copy().set_index(_s_acs_2011_['GEO.display-label'])\n",
    "# rename columns \n",
    "s_acs_2011_.columns = ['no','2011']\n",
    "# only continue with year/measurement column\n",
    "s_acs_2011_ = s_acs_2011_['2011']\n",
    "\n",
    "# set 2012 index to Places \n",
    "s_acs_2012_ = _s_acs_2012_.copy().set_index(_s_acs_2012_['GEO.display-label'])\n",
    "# rename columns \n",
    "s_acs_2012_.columns = ['no','2012']\n",
    "# only continue with year/measurement column\n",
    "s_acs_2012_ = s_acs_2012_['2012']\n",
    "\n",
    "# set 2013 index to Places \n",
    "s_acs_2013_ = _s_acs_2013_.copy().set_index(_s_acs_2013_['GEO.display-label'])\n",
    "# rename columns \n",
    "s_acs_2013_.columns = ['no','2013']\n",
    "# only continue with year/measurement column\n",
    "s_acs_2013_ = s_acs_2013_['2013']\n",
    "\n",
    "# set 2014 index to Places \n",
    "s_acs_2014_ = _s_acs_2014_.copy().set_index(_s_acs_2014_['GEO.display-label'])\n",
    "# rename columns \n",
    "s_acs_2014_.columns = ['no','2014']\n",
    "# only continue with year/measurement column\n",
    "s_acs_2014_ = s_acs_2014_['2014']\n",
    "\n",
    "# set 2015 index to Places \n",
    "s_acs_2015_ = _s_acs_2015_.copy().set_index(_s_acs_2015_['GEO.display-label'])\n",
    "# rename columns \n",
    "s_acs_2015_.columns = ['no','2015']\n",
    "# only continue with year/measurement column\n",
    "s_acs_2015_ = s_acs_2015_['2015']\n",
    "\n",
    "# rename columns \n",
    "test_16_df.columns = ['no','2016']\n",
    "# only continue with year/measurement column\n",
    "test_16_df = test_16_df['2016']\n",
    "\n",
    "# rename columns \n",
    "test_17_df.columns = ['no','2017']\n",
    "# only continue with year/measurement column\n",
    "test_17_df = test_17_df['2017']\n",
    "\n",
    "# forge Train DataFrame and convert NaN values to 0 (assumes population not measured is 0) \n",
    "combined_df = pd.concat([s_census_,s_acs_2011_,s_acs_2012_,s_acs_2013_,s_acs_2014_,s_acs_2015_,test_16_df,test_17_df],axis=1).fillna(0)\n",
    "\n",
    "# split train_df from combined_df\n",
    "train_df = combined_df[['1970', '1980', '1990', '2000', '2010', '2011', '2012', '2013', '2014','2015']]\n",
    "# split test_df form combined_df\n",
    "test_df = combined_df[['2016', '2017']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"forecast 2016 and 2017 populations using model for each sample Place\n",
    "\"\"\"\n",
    "# set out route for forecast tables\n",
    "out = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds = []\n",
    "\n",
    "# make DataFrame of column values as datetime (first converting to Series)\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to forge place specific DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    \n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15,\n",
    "                                      daily_seasonality=False,\n",
    "                                      weekly_seasonality=False,\n",
    "                                      yearly_seasonality=True,\n",
    "                                      n_changepoints=7)\n",
    "    \n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    \n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    \n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe(periods=3, freq='Y')\n",
    "    \n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    \n",
    "    # check where we are\n",
    "    if i % 25 == 0:\n",
    "        # indicate where we are\n",
    "        print(i)\n",
    "    # tag and bag (forecast table)\n",
    "    out.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds.append([\n",
    "        forecast.loc[forecast.ds == '2016-12-31'].yhat.values[0],\n",
    "        forecast.loc[forecast.ds == '2017-12-31'].yhat.values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make Baseline predictions of 2016 and 2017 population on sample Places\n",
    "\"\"\"\n",
    "# set out route\n",
    "baseline_preds = []\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    \n",
    "    # tag 1970 population\n",
    "    measure_70 = int(df['1970'])\n",
    "    # tag 1980 population\n",
    "    measure_80 = int(df['1980'])\n",
    "    # tag 1990 population\n",
    "    measure_90 = int(df['1990'])\n",
    "    # tag 2000 population \n",
    "    measure_00 = int(df['2000'])\n",
    "    # tag 2010 population\n",
    "    measure_10 = int(df['2010'])\n",
    "    # tag 2011 population\n",
    "    measure_11 = int(df['2011'])\n",
    "    # tag 2012 population\n",
    "    measure_12 = int(df['2012'])\n",
    "    # tag 2013 population\n",
    "    measure_13 = int(df['2013'])\n",
    "    # tag 2014 population\n",
    "    measure_14 = int(df['2014'])\n",
    "    # tag 2015 population\n",
    "    measure_15 = int(df['2015'])\n",
    "    \n",
    "    change = (measure_15 - measure_11)/5\n",
    "\n",
    "    # make 2016 prediction \n",
    "    p_16 = measure_15 + change\n",
    "\n",
    "    # make 2017 prediction \n",
    "    p_17 = p_16 + change\n",
    "    \n",
    "    # print(measure_00,measure_15,p_16,p_17)\n",
    "    if p_16 > abs(measure_15 * 1.1):\n",
    "        print(f\"measure_15 = {measure_15}\\np_16 = {p_16}\\n\")\n",
    "    \n",
    "    # pair prediction, tag & bag\n",
    "    baseline_preds.append([p_16,p_17])\n",
    "\n",
    "\"\"\"pull actual measurements for 2016 and 2017 population for each sample Place\n",
    "\"\"\"\n",
    "# list actual populations for 2016\n",
    "test_16 = [actual_population for actual_population in test_df['2016']]\n",
    "\n",
    "# list actual populations for 2017\n",
    "test_17 = [actual_population for actual_population in test_df['2017']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2016\n",
    "\"\"\"\n",
    "train_preds_16 = [float(pred[0]) for pred in train_preds]\n",
    "test_16_ = [float(act) for act in test_16]\n",
    "\n",
    "MODEL_rmse_exrate16 = sqrt(mean_squared_error(y_true=test_16_,y_pred=train_preds_16))\n",
    "\n",
    "base_preds_16 = [float(pred[0]) for pred in baseline_preds]\n",
    "test_16_ = [float(act) for act in test_16]\n",
    "BASE_rmse_exrate16 = sqrt(mean_squared_error(y_true=test_16_,y_pred=base_preds_16))\n",
    "\n",
    "MODEL_rmse_exrate16,BASE_rmse_exrate16  # (32119.17325005084, 32635.422809490734)\n",
    "# 91907.10440197852, 91928.04165272956 -- sample size 2,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2017\n",
    "\"\"\"\n",
    "train_preds_17 = [float(pred[1]) for pred in train_preds]\n",
    "test_17_ = [float(act) for act in test_17]\n",
    "MODEL_rmse_exrate17 = sqrt(mean_squared_error(y_true=test_17_,y_pred=train_preds_17))\n",
    "\n",
    "base_preds_17 = [float(pred[1]) for pred in baseline_preds]\n",
    "test_17_ = [float(act) for act in test_17]\n",
    "BASE_rmse_exrate17 = sqrt(mean_squared_error(y_true=test_17_,y_pred=base_preds_17))\n",
    "\n",
    "MODEL_rmse_exrate17,BASE_rmse_exrate17  # (31958.00988989089, 32717.02106434803)\n",
    "# 92797.69436875812, 92834.60502207742 -- sample size 2,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=test_16_,y_pred=train_preds_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=test_16_,y_pred=base_preds_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=test_17_,y_pred=train_preds_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=test_17_,y_pred=base_preds_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_percent_error(y_true, y_pred):\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise Exception(f\"len(y_true) != len(y_pred)\\n{len(y_true)} != {len(y_pred)}\")\n",
    "    sum_percent_error = 0\n",
    "    for population in range(len(y_true)):\n",
    "        percent_error = (abs(y_true[population] - y_pred[population]) / y_true[population])*100\n",
    "        if percent_error > 5:\n",
    "            print(population)\n",
    "            print(y_true[population],y_pred[population],percent_error)\n",
    "            print('\\n')\n",
    "        sum_percent_error += percent_error\n",
    "    avg_percent_error = sum_percent_error / len(y_true)\n",
    "    return avg_percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\"\"\"basically nowhere are we predicting more than 20% change in population\n",
    "    from 2015 (actual) to 2016 (model or baseline)\n",
    "    so why the hell are the avg_percent_error so damn high\n",
    "    -- is order getting changed? are model and base staying same but actual order is changed?\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "n=0\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    # tag 2015 population\n",
    "    measure_15 = int(df['2015'])\n",
    "    # tag 2016 prediction\n",
    "    model_16 = train_preds[j][0]\n",
    "    if model_16 > abs(measure_15 * 1.2):\n",
    "        #print(f\"measure_15 = {measure_15}\\np_16 = {model_16}\\n\")\n",
    "        count+=1\n",
    "    n+=1\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "nn=0\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    # tag 2015 population\n",
    "    measure_15 = int(df['2015'])\n",
    "    # tag 2016 prediction\n",
    "    base_16 = baseline_preds[j][0]\n",
    "    if base_16 > abs(measure_15 * 1.2):\n",
    "        #print(f\"measure_15 = {measure_15}\\np_16 = {model_16}\\n\")\n",
    "        count+=1\n",
    "    nn+=1\n",
    "print(count)\n",
    "print()\n",
    "print(n,nn)'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import matplotlib.pyplot as plt\n",
    "\n",
    "# high resolution \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set fig\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "# scatter 2016 Predictions \n",
    "plt.scatter(x_axis_2016, y_axis_2016,alpha=1,label='Model 2016')\n",
    "# scatter 2017 Predictions \n",
    "plt.scatter(x_axis_2017, y_axis_2017,alpha=0.5,label='Model 2017')\n",
    "# title graph\n",
    "plt.title(label='Prediction Residuals 2016 vs 2017',fontsize=20,fontname=\"Times New Roman Bold\")\n",
    "# x axis label \n",
    "plt.xlabel('Total Population of Place (Sample 5000 Places)', fontsize=13)\n",
    "# y axis label\n",
    "plt.ylabel('Residuals (Prediction - Actual)' , fontsize=13)\n",
    "# display legend\n",
    "plt.legend(loc='top right')\n",
    "\n",
    "# convert to semilogx\n",
    "plt.semilogx()\n",
    "plt.xticks(rotation=35)\n",
    "\n",
    "# display graph\n",
    "plt.show()\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"avg population for 2016: {int(sum(test_16_)/len(sample_one_hunnit))}\\navg  estimate  for 2016: {int(sum(train_preds_16)/len(sample_one_hunnit))}\\navg  baseline  for 2016: {int(sum(base_preds_16)/len(sample_one_hunnit))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"avg population for 2017: {int(sum(test_17_)/len(sample_one_hunnit))}\\navg  estimate  for 2017: {int(sum(train_preds_17)/len(sample_one_hunnit))}\\navg  baseline  for 2017: {int(sum(base_preds_17)/len(sample_one_hunnit))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CHANGEPOINT PRIOR FORM 0.15 TO 0.1\"\"\"\n",
    "'''forecast 2016 and 2017 populations using model for each sample Place'''\n",
    "# set out route for forecast tables\n",
    "out2 = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds2 = []\n",
    "# make DataFrame of column values as datetime\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=10)\n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe(periods=21, freq='Y')\n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    # tag and bag (forecast table)\n",
    "    out2.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds2.append([\n",
    "        forecast.loc[forecast.ds == '2016-12-31'].yhat.values[0],\n",
    "        forecast.loc[forecast.ds == '2017-12-31'].yhat.values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2016'''\n",
    "train_preds_16_1 = [float(pred[0]) for pred in train_preds2]\n",
    "\n",
    "# RMSE for new 2016 preds\n",
    "MODEL_rmse_exrate16_1 = sqrt(mean_squared_error(y_true=test_16_,y_pred=train_preds_16_1))\n",
    "\n",
    "MODEL_rmse_exrate16_1,BASE_rmse_exrate16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2017'''\n",
    "train_preds_17_1 = [float(pred[1]) for pred in train_preds2]\n",
    "\n",
    "# RMSE for new 2017 preds\n",
    "MODEL_rmse_exrate17_1 = sqrt(mean_squared_error(y_true=test_17_,y_pred=train_preds_17_1))\n",
    "\n",
    "MODEL_rmse_exrate17_1,BASE_rmse_exrate17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CHANGEPOINT PRIOR FORM 0.1 TO 0.05\"\"\"\n",
    "'''forecast 2016 and 2017 populations using model for each sample Place'''\n",
    "# set out route for forecast tables\n",
    "out3 = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds3 = []\n",
    "# make DataFrame of column values as datetime\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.05)\n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=3, freq='Y' )\n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    # tag and bag (forecast table)\n",
    "    out3.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds3.append([\n",
    "        forecast.loc[forecast.ds == '2016-12-31'].yhat.values[0],\n",
    "        forecast.loc[forecast.ds == '2017-12-31'].yhat.values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2016'''\n",
    "train_preds_16_2 = [float(pred[0]) for pred in train_preds3]\n",
    "\n",
    "# RMSE for new 2016 preds\n",
    "MODEL_rmse_exrate16_2 = sqrt(mean_squared_error(y_true=test_16_,y_pred=train_preds_16_2))\n",
    "\n",
    "MODEL_rmse_exrate16_2,BASE_rmse_exrate16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2017'''\n",
    "train_preds_17_2 = [float(pred[1]) for pred in train_preds2]\n",
    "\n",
    "# RMSE for new 2017 preds\n",
    "MODEL_rmse_exrate17_2 = sqrt(mean_squared_error(y_true=test_17_,y_pred=train_preds_17_2))\n",
    "\n",
    "MODEL_rmse_exrate17_2,BASE_rmse_exrate17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CHANGEPOINT PRIOR FORM 0.05 TO 0.01\"\"\"\n",
    "'''forecast 2016 and 2017 populations using model for each sample Place'''\n",
    "# set out route for forecast tables\n",
    "out4 = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds4 = []\n",
    "# make DataFrame of column values as datetime\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.01)\n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=3, freq='Y' )\n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    # tag and bag (forecast table)\n",
    "    out4.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds4.append([\n",
    "        forecast.loc[forecast.ds == '2016-12-31'].yhat.values[0],\n",
    "        forecast.loc[forecast.ds == '2017-12-31'].yhat.values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2016'''\n",
    "train_preds_16_3 = [float(pred[0]) for pred in train_preds4]\n",
    "\n",
    "# RMSE for new 2016 preds\n",
    "MODEL_rmse_exrate16_3 = sqrt(mean_squared_error(y_true=test_16_,y_pred=train_preds_16_3))\n",
    "\n",
    "MODEL_rmse_exrate16_3,BASE_rmse_exrate16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2017'''\n",
    "train_preds_17_3 = [float(pred[1]) for pred in train_preds4]\n",
    "\n",
    "# RMSE for new 2017 preds\n",
    "MODEL_rmse_exrate17_3 = sqrt(mean_squared_error(y_true=test_17_,y_pred=train_preds_17_3))\n",
    "\n",
    "MODEL_rmse_exrate17_3,BASE_rmse_exrate17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make Baseline predictions of 2016 and 2017 population on sample Places\"\"\"\n",
    "# set out route\n",
    "baseline_preds = []\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    \n",
    "    # tag 1970 population\n",
    "    measure_70 = int(df['1970'])\n",
    "    # tag 1980 population\n",
    "    measure_80 = int(df['1980'])\n",
    "    # tag 1990 population\n",
    "    measure_90 = int(df['1990'])\n",
    "    # tag 2000 population \n",
    "    measure_00 = int(df['2000'])\n",
    "    # tag 2010 population\n",
    "    measure_10 = int(df['2010'])\n",
    "    # tag 2011 population\n",
    "    measure_11 = int(df['2011'])\n",
    "    # tag 2012 population\n",
    "    measure_12 = int(df['2012'])\n",
    "    # tag 2013 population\n",
    "    measure_13 = int(df['2013'])\n",
    "    # tag 2014 population\n",
    "    measure_14 = int(df['2014'])\n",
    "    # tag 2015 population\n",
    "    measure_15 = int(df['2015'])\n",
    "    \n",
    "    # calculate avg yearly change 1970-1980\n",
    "    change_70_80 = (measure_80 - measure_70) / 10\n",
    "    # calculate avg yearly change 1780-1990\n",
    "    change_80_90 = (measure_90 - measure_80) / 10\n",
    "    # calculate avg yearly change 1990-2000\n",
    "    change_90_00 = (measure_00 - measure_90) / 10\n",
    "    # calculate avg yearly change 2000-2010\n",
    "    change_00_10 = (measure_10 - measure_00) / 10\n",
    "    \n",
    "    # calculate yearly change 2010-2011\n",
    "    change_10_11 = measure_11 - measure_10\n",
    "    # calculate yearly change 2011-2012\n",
    "    change_11_12 = measure_12 - measure_11\n",
    "    # calculate yearly change 2012-2013\n",
    "    change_12_13 = measure_13 - measure_12\n",
    "    # calculate yearly change 2013-2014\n",
    "    change_13_14 = measure_14 - measure_13\n",
    "    # calculate yearly change 2014-2015\n",
    "    change_14_15 = measure_15 - measure_14\n",
    "    \n",
    "    # calculate mean census change \n",
    "    avg_census_change = np.mean([change_70_80, change_80_90, change_90_00, change_00_10])\n",
    "    \n",
    "    # calculate mean ACS change\n",
    "    avg_acs_change = np.mean([change_10_11, change_11_12, change_12_13, change_13_14, change_14_15])\n",
    "    \n",
    "    # weight census change\n",
    "    weighted_census_change = avg_census_change * (40/45)\n",
    "    # weight ACS change\n",
    "    weighted_acs_change = avg_acs_change * (5/45)\n",
    "    \n",
    "    # combine weighted changes for average change\n",
    "    change = weighted_census_change + weighted_acs_change\n",
    "\n",
    "    # make 2016 prediction \n",
    "    p_16 = measure_15 + change\n",
    "\n",
    "    # make 2017 prediction \n",
    "    p_17 = p_16 + change\n",
    "    \n",
    "    print(measure_00,measure_15,p_16,p_17)\n",
    "    \n",
    "    # pair prediction, tag & bag\n",
    "    baseline_preds.append([p_16,p_17])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
