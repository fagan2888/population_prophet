{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "STEP 0 \n",
    ">> imports; def clean_census & other functions\n",
    "\"\"\"\n",
    "\n",
    "# default cleaning method until proven otherwise\n",
    "def clean_census_frame(csv_path , head=False , reset=True , set_index=False ):\n",
    "    '''\n",
    "    inputs) \n",
    "        >> csv_path\n",
    "            > path to csv\n",
    "        >> head\n",
    "            > default=False\n",
    "                >> if != False\n",
    "                    > integer\n",
    "                        >> returns the first {head} rows (using .head() method) \n",
    "                            > instead of enitre dataframe\n",
    "        >> reset\n",
    "            > default=True\n",
    "                >> resets index after taking out rows\n",
    "            > if set to False\n",
    "                >> will not reset index\n",
    "        >> set_index\n",
    "            > default=False\n",
    "            > if != False\n",
    "                >> will set_index of new df to set_index\n",
    "    output)\n",
    "        >> dataframe cleaned like 2000 Census age&sex by 5-digit Zip Code (also how 2010 for same is cleaned)\n",
    "    how)\n",
    "        1. reads in csv , assumes it's large\n",
    "        2. makes a copy for editing \n",
    "            > and potential future use\n",
    "        3. locates readable column names  and non-readable names \n",
    "            > readable\n",
    "                    > e.g. Estimate; SEX AND AGE - Total population\n",
    "                >> assumes they are currently in row 0\n",
    "            > non-readable\n",
    "                    > e.g. HC01_VC03\n",
    "                >> assumes they are currently == dataframe.columns\n",
    "        4. replaces dataframe.columns (non-readable) with readable column names\n",
    "            > and drops the old 0th column (column where readable names were stored)\n",
    "        \n",
    "    '''\n",
    "    # load data\n",
    "    df = pd.read_csv( csv_path , low_memory=False )\n",
    "\n",
    "    # and copy\n",
    "    _df = df.copy()\n",
    "\n",
    "    # reset column names to current 0th row values\n",
    "    _df.columns = _df.iloc[0]\n",
    "    # new 2000 dataframe without row where values are from\n",
    "    clean_df = _df[1:]\n",
    "    \n",
    "    # default\n",
    "    if reset==True:\n",
    "        # reset index\n",
    "        clean_df = clean_df.reset_index()\n",
    "        \n",
    "    # set_index\n",
    "    if set_index:\n",
    "        clean_df = clean_df.set_index(set_index)\n",
    "    \n",
    "    if head:\n",
    "        # return first {head} rows of dataframe\n",
    "        return clean_df.head(head)\n",
    "    else:\n",
    "        # return dataframe\n",
    "        return clean_df\n",
    "\n",
    "'''\n",
    "STEP 1 \n",
    ">> load data, reset; make copies/**sample\n",
    "'''\n",
    "\n",
    "def load_clean_frames(i=0,n=False):\n",
    "    '''\n",
    "    function) loads data\n",
    "    \n",
    "    input)\n",
    "        >> i\n",
    "            > if 0\n",
    "                >> .reset_index() after deleting row contining column names\n",
    "            > if 1\n",
    "                >> do not .reset_index()\n",
    "        >> head\n",
    "            > default=False (ignore)\n",
    "            > if != False\n",
    "                >> must be int\n",
    "                    > dataframe = dataframe.head(n)\n",
    "    '''\n",
    "    if i==0:\n",
    "        # load with reset\n",
    "        # 2011 \n",
    "        twenty_eleven = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_11_5YR_DP05_with_ann.csv')\n",
    "        # 2012\n",
    "        twenty_twelve = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_12_5YR_DP05_with_ann.csv')\n",
    "        #2013\n",
    "        twenty_thirteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_13_5YR_DP05_with_ann.csv')\n",
    "        # 2014\n",
    "        twenty_fourteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_14_5YR_DP05_with_ann.csv')\n",
    "        # 2015\n",
    "        twenty_fifteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_15_5YR_DP05_with_ann.csv')\n",
    "        #2016\n",
    "        twenty_sixteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_16_5YR_DP05_with_ann.csv')\n",
    "        #2017\n",
    "        twenty_seventeen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_17_5YR_DP05_with_ann.csv')\n",
    "    if i==1:\n",
    "        # load without reset\n",
    "        # 2011 \n",
    "        twenty_eleven = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_11_5YR_DP05_with_ann.csv',reset=False)\n",
    "        # 2012\n",
    "        twenty_twelve = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_12_5YR_DP05_with_ann.csv',reset=False)\n",
    "        #2013\n",
    "        twenty_thirteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_13_5YR_DP05_with_ann.csv',reset=False)\n",
    "        # 2014\n",
    "        twenty_fourteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_14_5YR_DP05_with_ann.csv',reset=False)\n",
    "        # 2015\n",
    "        twenty_fifteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_15_5YR_DP05_with_ann.csv',reset=False)\n",
    "        #2016\n",
    "        twenty_sixteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_16_5YR_DP05_with_ann.csv',reset=False)\n",
    "        #2017\n",
    "        twenty_seventeen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_17_5YR_DP05_with_ann.csv',reset=False)\n",
    "    \n",
    "    # default\n",
    "    if n==False:\n",
    "        # copy \n",
    "        # 2011 \n",
    "        _y2k11 = twenty_eleven.copy()\n",
    "        # 2012\n",
    "        _y2k12 = twenty_twelve.copy()\n",
    "        #2013\n",
    "        _y2k13 = twenty_thirteen.copy()\n",
    "        # 2014\n",
    "        _y2k14 = twenty_fourteen.copy()\n",
    "        # 2015\n",
    "        _y2k15 = twenty_fifteen.copy()\n",
    "        #2016\n",
    "        _y2k16 = twenty_sixteen.copy()\n",
    "        #2017\n",
    "        _y2k17 = twenty_seventeen.copy()\n",
    "        \n",
    "    # non default, want only first n rows\n",
    "    if n:\n",
    "        # adjust frames to .head(n) \n",
    "        # 2011 \n",
    "        _y2k11 = twenty_eleven.copy().head(n)\n",
    "        # 2012\n",
    "        _y2k12 = twenty_twelve.copy().head(n)\n",
    "        #2013\n",
    "        _y2k13 = twenty_thirteen.copy().head(n)\n",
    "        # 2014\n",
    "        _y2k14 = twenty_fourteen.copy().head(n)\n",
    "        # 2015\n",
    "        _y2k15 = twenty_fifteen.copy().head(n)\n",
    "        #2016\n",
    "        _y2k16 = twenty_sixteen.copy().head(n)\n",
    "        #2017\n",
    "        _y2k17 = twenty_seventeen.copy().head(n)\n",
    "    \n",
    "    # output list of copied frames\n",
    "    return [_y2k11,_y2k12,_y2k13,_y2k14,_y2k15,_y2k16,_y2k17]\n",
    "\n",
    "'''\n",
    "STEP 2 \n",
    ">> identify unique (mostly used in testing); \n",
    ">> convert DataFrame to numeric; convert Geography (Zip Codes) && Ids\n",
    "'''\n",
    "\n",
    "def test_non_unique(column_names):\n",
    "    '''\n",
    "    input) \n",
    "        >> list of column names {column_names}\n",
    "            > columns to check for duplicate instances\n",
    "    output)\n",
    "        >> indexed list of names occouring more than once \n",
    "    '''\n",
    "    # store first instance\n",
    "    first_occour = []\n",
    "    # store 2nd+ instance(s)\n",
    "    non_unique = []\n",
    "    # we're going to want index\n",
    "    for i,_ in enumerate(column_names):\n",
    "        # not first time\n",
    "        if _ not in first_occour:\n",
    "            first_occour.append(_)\n",
    "        # if not first, tag&bag\n",
    "        else:\n",
    "            non_unique.append([i,_])\n",
    "    # output index w/ non-first instances\n",
    "    return non_unique\n",
    "\n",
    "\n",
    "def to_numeric_but(dataframe,save_these_columns='none',e='coerce'):\n",
    "    '''\n",
    "    split into 2 df and rejoin after convert to int\n",
    "    \n",
    "    inputs:\n",
    "        >> save_these_columns=number of columns to save\n",
    "            > currently must include one end of df \n",
    "                >> might could run function multiple times to edit slices\n",
    "                >> single number, not range (yet)\n",
    "                    > if 'none', saves no columns\n",
    "        >> dataframe\n",
    "            > dataframe to shif to numeric (but)\n",
    "        >> e\n",
    "            > for pd.to_numeric, errors=e\n",
    "    output:\n",
    "        >> concatted pd.DataFrame of \n",
    "            > og columns you chose to save\n",
    "            > columns converted to numeric\n",
    "    '''\n",
    "    # copy df for editing\n",
    "    k = dataframe.copy()\n",
    "    \n",
    "    # split\n",
    "    if save_these_columns != 'none':\n",
    "        # columns to save\n",
    "        save_k = k[k.columns[:save_these_columns]]\n",
    "        # columns to edit\n",
    "        switch_k = k[k.columns[save_these_columns:]]\n",
    "    # don't split\n",
    "    else:\n",
    "        # k as is\n",
    "        switch_k = k\n",
    "\n",
    "    # edited columns  # coerce , ignore , raise\n",
    "    swapped_k = switch_k.apply(pd.to_numeric, errors=e)\n",
    "    \n",
    "    # check saving columns\n",
    "    if save_these_columns != 'none':\n",
    "        # new (edited) dataframe (ogsave|swapped)\n",
    "        new_k = pd.concat( [save_k,swapped_k] ,axis=1 )\n",
    "    else:\n",
    "        new_k = swapped_k\n",
    "\n",
    "    return new_k\n",
    "\n",
    "\n",
    "def geography_to_zipcode_ids_to_numeric(dataframe):\n",
    "    '''\n",
    "    convert \n",
    "        >> .Geography values \n",
    "            > like 'ZCTA5 00601' \n",
    "            > to int(00601)\n",
    "        >> .Id values\n",
    "            > like '8600000US00601' \n",
    "            > to int(860000000601)\n",
    "        >> .Id2 values\n",
    "            > like '00601'\n",
    "            > to int(00601)\n",
    "    '''\n",
    "    # copy\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # set old Geography\n",
    "    geo = df.Geography\n",
    "    # set old Id\n",
    "    _id = df.Id\n",
    "    # set old Id2\n",
    "    __id2 = df.Id2\n",
    "    \n",
    "    # make new 'Geography' values\n",
    "    new_geos = [int(i[-5:]) for i in geo]\n",
    "    # new 'Id' values\n",
    "    new_id = [int(''.join(i.split('US'))) for i in _id]\n",
    "    # new .Id2 instances\n",
    "    new__id2 = [int(d) for d in __id2]\n",
    "    \n",
    "    # convert dataframe\n",
    "    new_df = df.copy()\n",
    "    new_df.Geography = new_geos\n",
    "    new_df.Id = new_id\n",
    "    new_df.Id2 = new__id2\n",
    "    \n",
    "    # return new df\n",
    "    return new_df\n",
    "\n",
    "'''\n",
    "STEP 3\n",
    ">> run KMeans on dataframe\n",
    "'''\n",
    "\n",
    "def kmeans_by(dataframe,n_clusters=10,converted=False):\n",
    "    '''\n",
    "    inputs:\n",
    "        >> dataframe\n",
    "            > dataframe to be edited\n",
    "        >> n_clusters \n",
    "            > default = 10\n",
    "            > number of clusters for KMeans\n",
    "        >> converted\n",
    "            > default = False\n",
    "            > assumes data is not ready for KMeans \n",
    "                >> if True, assumes df is ready for KMeans\n",
    "    output:\n",
    "        > pd.Dataframe of \n",
    "    '''\n",
    "    # copy data \n",
    "    d = dataframe.copy()  \n",
    "    \n",
    "    '''df conversion'''\n",
    "    # default\n",
    "    if converted!=True:\n",
    "        # copy data for editing\n",
    "        _data_ = d.copy()\n",
    "        \n",
    "        # convert first 3 columns ('Id', 'Id2', 'Geography')\n",
    "        _data = geography_to_zipcode_ids_to_numeric(dataframe=_data_)\n",
    "        \n",
    "        # convert remainder of dataframe\n",
    "        data = to_numeric_but(save_these_columns='none', dataframe=_data)\n",
    "        print(len(data),len(data.columns))\n",
    "\n",
    "    # dataframe has already been converted / otherwise\n",
    "    if converted==True:\n",
    "        data = d\n",
    "    \n",
    "    '''KMeans'''\n",
    "    # fill NaN values\n",
    "    t = data.copy().fillna(0)\n",
    "    \n",
    "    # Convert DataFrame to matrix\n",
    "    mat = t.values\n",
    "    \n",
    "    # Using sklearn\n",
    "    km = KMeans(n_clusters)\n",
    "    # fit our values\n",
    "    km.fit(mat)\n",
    "    \n",
    "    # Get cluster assignment labels\n",
    "    labels = km.labels_\n",
    "    \n",
    "    # Format results as a DataFrame\n",
    "    results = pd.DataFrame([t.index,labels])\n",
    "\n",
    "    # display results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(X, k=5, max_iter=1000):\n",
    "    \"\"\"Performs k means\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - k - number of clusters\n",
    "    - max_iter - maximum iterations\n",
    "    Returns:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "    centers = [tuple(pt) for pt in random.sample(list(X), k)]\n",
    "    for i in range(max_iter):\n",
    "        clusters = defaultdict(list)\n",
    "\n",
    "        for datapoint in X:\n",
    "            distances = [euclidean(datapoint, center) for center in centers]\n",
    "            center = centers[np.argmin(distances)]\n",
    "            clusters[center].append(datapoint)\n",
    "\n",
    "        new_centers = []\n",
    "        for center, pts in clusters.items():\n",
    "            new_center = np.mean(pts, axis=0)\n",
    "            new_centers.append(tuple(new_center))\n",
    "\n",
    "        if set(new_centers) == set(centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def sse(clusters):\n",
    "    \"\"\"Sum squared euclidean distance of all points to their cluster center\"\"\"\n",
    "    sum_squared_residuals = 0\n",
    "    for center, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            sum_squared_residuals += euclidean(pt, center)**2\n",
    "    return sum_squared_residuals\n",
    "\n",
    "\n",
    "def plot_k_sse(X, min_k, max_k):\n",
    "    \"\"\"Plots sse for values of k between min_k and max_k\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - min_k, max_k - smallest and largest k to plot sse for\n",
    "    \"\"\"\n",
    "    k_values = range(min_k, max_k+1)\n",
    "    sse_values = []\n",
    "    for k in k_values:\n",
    "        clusters = k_means(X, k=k)\n",
    "        # sum squared euclidean; i>>c\n",
    "        sse_values.append(sse(clusters))\n",
    "    plt.plot(k_values, sse_values)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('sum squared error')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def turn_clusters_into_labels(clusters):\n",
    "    \"\"\"Converts clusters dict returned by k_means into X, y (labels)\n",
    "    Args:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    new_X = []\n",
    "    label = 0\n",
    "    for cluster, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            new_X.append(pt)\n",
    "            labels.append(label)\n",
    "        label += 1\n",
    "    return np.array(new_X), np.array(labels)\n",
    "\n",
    "\n",
    "def plot_k_silhouette(X, min_k, max_k):\n",
    "    \"\"\"Plots sse for values of k between min_k and max_k\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - min_k, max_k - smallest and largest k to plot sse for\n",
    "    \"\"\"\n",
    "    k_values = range(min_k, max_k+1)\n",
    "    silhouette_scores = []\n",
    "    for k in k_values:\n",
    "        clusters = k_means(X, k=k)\n",
    "        new_X, labels = turn_clusters_into_labels(clusters)\n",
    "        silhouette_scores.append(silhouette_score(new_X, labels))\n",
    "\n",
    "    plt.plot(k_values, silhouette_scores)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('silhouette score')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all_2d(X, feature_names, k=3):\n",
    "    \"\"\"Generates all possible 2d plots of observations color coded by cluster ID\"\"\"\n",
    "    pairs = list(combinations(range(X.shape[1]), 2))\n",
    "    fig, axes = plt.subplots((len(pairs) // 2), 2)\n",
    "    flattened_axes = [ax for ls in axes for ax in ls]\n",
    "\n",
    "    for pair, ax in zip(pairs, flattened_axes):\n",
    "        pair = np.array(pair)\n",
    "        plot_data_2d(X[:, pair], feature_names[pair], ax, k=k)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_2d(X, plot_labels, ax, k=3):\n",
    "    \"\"\"Generates single 2d plot of observations color coded by cluster ID\"\"\"\n",
    "    clusters = k_means(X, k=k)\n",
    "    new_X, labels = turn_clusters_into_labels(clusters)\n",
    "    ax.scatter(new_X[:, 0], new_X[:, 1], c=labels)\n",
    "    ax.set_xlabel(plot_labels[0])\n",
    "    ax.set_ylabel(plot_labels[1])\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     iris = datasets.load_iris()\n",
    "#     X = iris.data\n",
    "#     plot_k_sse(X, 2, 10)\n",
    "#     plot_k_silhouette(X, 2, 10)\n",
    "#     plot_all_2d(X, np.array(iris.feature_names), k=5)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load first 10,000 rows w/o reset\n",
    "f = load_clean_frames( i=1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in f:\n",
    "    print(h.info(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 33120 /1000\n",
    "p,int(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store out\n",
    "out = []\n",
    "\n",
    "for i in range(len(f)):\n",
    "    z = kmeans_by( dataframe=f[i] , n_clusters=int(p) )\n",
    "    # df, zipcode instance, cluster\n",
    "    out.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2011 \n",
    "x = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(clusters):\n",
    "    \"\"\"Sum squared euclidean distance of all points to their cluster center\"\"\"\n",
    "    sum_squared_residuals = 0\n",
    "    for center, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            sum_squared_residuals += euclidean(pt, center)**2\n",
    "    return sum_squared_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "for i in out:\n",
    "    print(sse(i))\n",
    "    \n",
    "# n=10000 , 10 clusters\n",
    "# 330034929.0\n",
    "# 329073883.0\n",
    "# 330082783.0\n",
    "# 327515094.0\n",
    "# 330612055.0\n",
    "# 331199375.0\n",
    "# 327962433.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load w/o reset\n",
    "f = load_clean_frames(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy for safeguard and hedge reload\n",
    "frames = f.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract copy of 2011 \n",
    "y2k11 = frames.copy()  #[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine (2011)\n",
    "y2k11.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuy11 = test_non_unique(y2k11)\n",
    "len(nuy11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all but first 3 columns to numeric\n",
    "data = y2k11.copy()\n",
    "k2011 = to_numeric_but(save_these_columns=3,dataframe=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2011.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonuni = test_non_unique(k2011)\n",
    "len(nonuni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuy11[:2] , nonuni[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(k2011.columns[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in k2011.columns:\n",
    "    if i not in y2k11.columns:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert the first 3 columns\n",
    "adjust_first_3 = k2011.copy()\n",
    "_2011df_ = geography_to_zipcode_ids_to_numeric(adjust_first_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_test = _2011df_.copy()\n",
    "# rand_test['Margin of Error; SEX AND AGE - Total population'].apply(lambda x : 0 if x =='*****' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2011df_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _2011df_ = _2011df_.dropna()  # , how='any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no change\n",
    "_2011df_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _2011df_.head(15)\n",
    "# _2011df_ = _2011df_.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _2011df_['Margin of Error; SEX AND AGE - Total population'].apply(lambda x : 0 for x in i if x =='*****' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=-1\n",
    "x_out=set()\n",
    "star_out=set()\n",
    "other_out=set()\n",
    "for i in _2011df_.sample(100,axis=0).values:\n",
    "    x+=1\n",
    "    y=0\n",
    "    for j in i:\n",
    "        y+=1\n",
    "        if j == '(X)':\n",
    "            x_out.add((x-1,y))\n",
    "        if j == '*****':\n",
    "            star_out.add((x-1,y))\n",
    "        if j == '**':\n",
    "            star_out.add((x-1,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectj = []\n",
    "for i,j in x_out: \n",
    "    collectj.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collecti = []\n",
    "for j,i in star_out: \n",
    "    collecti.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_out=[]\n",
    "for n in set(collectj):\n",
    "    q = collectj.count(n)\n",
    "    count_out.append((n,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_in=[]\n",
    "for m in set(collecti):\n",
    "    p = collectj.count(m)\n",
    "    count_in.append((m,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.DataFrame(data=count_out,columns=['column','count'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(data=count_in,columns=['row','count'])\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = _2011df_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [_2011df_.columns[q-1] for q,b in count_out]\n",
    "stars = [_2011df_.columns[q-1] for q,b in count_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cols_to_drop), len(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert '*****' to 0\n",
    "for i in stars: \n",
    "    # test.drop(stars,axis=1)\n",
    "    test[i].apply(lambda x : 0 if x == '*****' else x)\n",
    "# drop columns with '(X)'\n",
    "# for cols in \n",
    "test = test.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()  #['Percent Margin of Error; HISPANIC OR LATINO AND RACE - Total housing units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ivalues = [i for i in test.values]\n",
    "# # # [i for i in ivalues if i == '**']\n",
    "# # for i in range(len(test.values)):\n",
    "# #     if test.values[i].any() == '**':\n",
    "# #         print(i,test.values[i])\n",
    "# # sus=set()\n",
    "# # for i in test:\n",
    "# #     for x in test[i]:\n",
    "# #         if x == '**':\n",
    "# #             sus.add(i)\n",
    "\n",
    "# for i in test.columns:\n",
    "#     test[i].apply(lambda q: 0 if x == '**' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - that wasn't too hard\n",
    "- ***actions***:\n",
    "    - drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test.copy().fillna(0)\n",
    "# Convert DataFrame to matrix\n",
    "mat = t.values\n",
    "# Using sklearn\n",
    "km = KMeans(n_clusters=5)\n",
    "# fit our values\n",
    "km.fit(mat)\n",
    "# Get cluster assignment labels\n",
    "labels = km.labels_\n",
    "# Format results as a DataFrame\n",
    "results = pd.DataFrame([t.index,labels]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df for editing\n",
    "k11 = y2k11.copy()\n",
    "\n",
    "'''split into 2 df and rejoin after convert to int'''\n",
    "# df to save\n",
    "save_k11 = k11.copy()\n",
    "# columns to save\n",
    "save_k11 = save_k11.copy()[save_k11.columns[:3]]\n",
    "\n",
    "# df to edit\n",
    "switch_k11 = k11.copy()\n",
    "# columns to edit\n",
    "switch_k11 = switch_k11.copy()[switch_k11.columns[3:]]\n",
    "\n",
    "# edited columns\n",
    "swapped_k11 = switch_k11.copy().apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# new (edited) dataframe\n",
    "new_k11 = pd.concat([save_k11,swapped_k11],axis=1)\n",
    "\n",
    "len(new_k11.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pull column names\n",
    "# k11_cols = y2k11.copy().columns[4:]\n",
    "# for c in range(len(k11_cols)):\n",
    "#     a = pd.Series(k11[k11_cols])\n",
    "# #     k11.loc[[k11_cols][c]] = pd.to_numeric(a,errors='ignore')\n",
    "# # pd.Series(k11[k11_cols[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k11_cols = y2k11.copy().columns\n",
    "__k11_cols__ = y2k11.copy().columns[4:]\n",
    "# len(k11_cols),len(__k11_cols__)\n",
    "print(f'{__k11_cols__[:3]}\\n{k11_cols[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    print(frame.info(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copies = frames[:2].copy()\n",
    "for df in copies:\n",
    "    for column in df.columns: \n",
    "        df[column] = pd.to_numeric([df[column]], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***NOTE***:\n",
    "    - path to mvp\n",
    "        - whiteboard_pics/acs_5yr_11-17_path-to-mvp.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_array = frames[0].values\n",
    "dataset_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2 >> find all columns which coexist across all dataframes at current position\n",
    "'''\n",
    "columns_by_frame = [frame.columns for frame in frames]\n",
    "count_columns_by_frame = [len(frame) for frame in columns_by_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "# for range of df with most columns\n",
    "for count in range(len(max(count_columns_by_frame))):\n",
    "    # if index of every frame is same as index of frame with most columns\n",
    "    if [frame for frame in columns_by_frame][count] == frames[6][count]:\n",
    "        out.append(count)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all\n",
    "years = frames  # [y2k11,y2k12,y2k13,y2k14,y2k15,y2k16,y2k17]\n",
    "for year in years:\n",
    "    print(len(year.columns),'\\n',year.info(),'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "# kmeans.labels_\n",
    "\n",
    "# # array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
    "# kmeans.predict([[0, 0], [12, 3]])\n",
    "# # array([1, 0], dtype=int32)\n",
    "# kmeans.cluster_centers_\n",
    "# # array([[10.,  2.], [ 1.,  2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scikit learn plays really well with Pandas, so I suggest you use it. Here's an example:\n",
    "\n",
    "# # In [1]: \n",
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "# # from sklearn.cross_validation import train_test_split\n",
    "# data = np.reshape(np.random.randn(20),(10,2)) # 10 training examples\n",
    "# labels = np.random.randint(2, size=10) # 10 labels\n",
    "\n",
    "# # In [2]: \n",
    "# X = pd.DataFrame(data)\n",
    "# y = pd.Series(labels)\n",
    "\n",
    "# # In [3]:\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "#                                                     random_state=0)\n",
    "\n",
    "# # In [4]: X_test\n",
    "# # Out[4]:\n",
    "\n",
    "# #      0       1\n",
    "# # 2   -1.39   -1.86\n",
    "# # 8    0.48   -0.81\n",
    "# # 4   -0.10   -1.83\n",
    "\n",
    "# # In [5]: y_test\n",
    "# # Out[5]:\n",
    "\n",
    "# # 2    1\n",
    "# # 8    1\n",
    "# # 4    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizfsdazapizzaadsf = (2,1,0,4,32,7,2,9,5)\n",
    "max(pizfsdazapizzaadsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[3,2,1,4,6,5]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data for editing\n",
    "data = frames[1].copy()\n",
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _2011df_ = geography_to_zipcode_ids_to_numeric(adjust_first_3)\n",
    "non_copy = geography_to_zipcode_ids_to_numeric(data)\n",
    "# len(_2011df_.columns)\n",
    "len(non_copy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all but first 3 columns to numeric\n",
    "k2012 = to_numeric_but(save_these_columns=3,dataframe=data)\n",
    "# len(k2012.columns)\n",
    "# k2012 = to_numeric_but(save_these_columns=3,dataframe=data)\n",
    "len(k2012.columns), len(non_copy.apply(pd.to_numeric,errors='coerce').columns)\n",
    "# (407, 327)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - above indicates \n",
    "        - may be better to convert first 3 columns first\n",
    "        - then wouldn't have to save\n",
    "- ***actions***:\n",
    "    - reeval to_numeric_but\n",
    "    - get this whole 407 columns thing back to 327 \n",
    "    \n",
    "```\n",
    "# reeval to_numeric_but\n",
    "f = load_clean_frames(i=1,n=1000)\n",
    "# 2012\n",
    "twelve = f[1].copy()\n",
    "print(len(twelve.columns))\n",
    "t = geography_to_zipcode_ids_to_numeric(twelve)\n",
    "print(len(t.columns))\n",
    "x = to_numeric_but(save_these_columns='none',dataframe=t)\n",
    "print(len(x.columns))\n",
    ">>327\n",
    ">>327\n",
    ">>327\n",
    "```\n",
    "- ***conclusion***:\n",
    "    - swapped order in to_numeric_but\n",
    "    - defaulted save_these_columns='none' argument (in to_numeric_but)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from itertools import combinations\n",
    "\n",
    "def k_means(X, k=5, max_iter=1000):\n",
    "    \"\"\"Performs k means\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - k - number of clusters\n",
    "    - max_iter - maximum iterations\n",
    "    Returns:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "    centers = [tuple(pt) for pt in random.sample(list(X), k)]\n",
    "    for i in range(max_iter):\n",
    "        clusters = defaultdict(list)\n",
    "\n",
    "        for datapoint in X:\n",
    "            distances = [euclidean(datapoint, center) for center in centers]\n",
    "            center = centers[np.argmin(distances)]\n",
    "            clusters[center].append(datapoint)\n",
    "\n",
    "        new_centers = []\n",
    "        for center, pts in clusters.items():\n",
    "            new_center = np.mean(pts, axis=0)\n",
    "            new_centers.append(tuple(new_center))\n",
    "\n",
    "        if set(new_centers) == set(centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def sse(clusters):\n",
    "    \"\"\"Sum squared euclidean distance of all points to their cluster center\"\"\"\n",
    "    sum_squared_residuals = 0\n",
    "    for center, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            sum_squared_residuals += euclidean(pt, center)**2\n",
    "    return sum_squared_residuals\n",
    "\n",
    "\n",
    "def plot_k_sse(X, min_k, max_k):\n",
    "    \"\"\"Plots sse for values of k between min_k and max_k\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - min_k, max_k - smallest and largest k to plot sse for\n",
    "    \"\"\"\n",
    "    k_values = range(min_k, max_k+1)\n",
    "    sse_values = []\n",
    "    for k in k_values:\n",
    "        clusters = k_means(X, k=k)\n",
    "        # sum squared euclidean; i>>c\n",
    "        sse_values.append(sse(clusters))\n",
    "    plt.plot(k_values, sse_values)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('sum squared error')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def turn_clusters_into_labels(clusters):\n",
    "    \"\"\"Converts clusters dict returned by k_means into X, y (labels)\n",
    "    Args:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    new_X = []\n",
    "    label = 0\n",
    "    for cluster, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            new_X.append(pt)\n",
    "            labels.append(label)\n",
    "        label += 1\n",
    "    return np.array(new_X), np.array(labels)\n",
    "\n",
    "\n",
    "def plot_k_silhouette(X, min_k, max_k):\n",
    "    \"\"\"Plots sse for values of k between min_k and max_k\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - min_k, max_k - smallest and largest k to plot sse for\n",
    "    \"\"\"\n",
    "    k_values = range(min_k, max_k+1)\n",
    "    silhouette_scores = []\n",
    "    for k in k_values:\n",
    "        clusters = k_means(X, k=k)\n",
    "        new_X, labels = turn_clusters_into_labels(clusters)\n",
    "        silhouette_scores.append(silhouette_score(new_X, labels))\n",
    "\n",
    "    plt.plot(k_values, silhouette_scores)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('silhouette score')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all_2d(X, feature_names, k=3):\n",
    "    \"\"\"Generates all possible 2d plots of observations color coded by cluster ID\"\"\"\n",
    "    pairs = list(combinations(range(X.shape[1]), 2))\n",
    "    fig, axes = plt.subplots((len(pairs) // 2), 2)\n",
    "    flattened_axes = [ax for ls in axes for ax in ls]\n",
    "\n",
    "    for pair, ax in zip(pairs, flattened_axes):\n",
    "        pair = np.array(pair)\n",
    "        plot_data_2d(X[:, pair], feature_names[pair], ax, k=k)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_2d(X, plot_labels, ax, k=3):\n",
    "    \"\"\"Generates single 2d plot of observations color coded by cluster ID\"\"\"\n",
    "    clusters = k_means(X, k=k)\n",
    "    new_X, labels = turn_clusters_into_labels(clusters)\n",
    "    ax.scatter(new_X[:, 0], new_X[:, 1], c=labels)\n",
    "    ax.set_xlabel(plot_labels[0])\n",
    "    ax.set_ylabel(plot_labels[1])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    plot_k_sse(X, 2, 10)\n",
    "    plot_k_silhouette(X, 2, 10)\n",
    "    plot_all_2d(X, np.array(iris.feature_names), k=5)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
