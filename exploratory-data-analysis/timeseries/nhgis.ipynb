{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numeric_but(dataframe,save_these_columns='none',e='coerce'):\n",
    "    '''\n",
    "    split into 2 df and rejoin after convert to int\n",
    "    \n",
    "    inputs:\n",
    "        >> save_these_columns=number of columns to save\n",
    "            > currently must include one end of df \n",
    "                >> might could run function multiple times to edit slices\n",
    "                >> single number, not range (yet)\n",
    "                    > if 'none', saves no columns\n",
    "        >> dataframe\n",
    "            > dataframe to shif to numeric (but)\n",
    "        >> e\n",
    "            > for pd.to_numeric, errors=e\n",
    "    output:\n",
    "        >> concatted pd.DataFrame of \n",
    "            > og columns you chose to save\n",
    "            > columns converted to numeric\n",
    "    '''\n",
    "    # copy df for editing\n",
    "    k = dataframe.copy()\n",
    "    \n",
    "    # split\n",
    "    if save_these_columns != 'none':\n",
    "        # columns to save\n",
    "        save_k = k[k.columns[:save_these_columns]]\n",
    "        # columns to edit\n",
    "        switch_k = k[k.columns[save_these_columns:]]\n",
    "    # don't split\n",
    "    else:\n",
    "        # k as is\n",
    "        switch_k = k\n",
    "\n",
    "    # edited columns  # coerce , ignore , raise\n",
    "    swapped_k = switch_k.apply(pd.to_numeric, errors=e)\n",
    "    \n",
    "    # check saving columns\n",
    "    if save_these_columns != 'none':\n",
    "        # new (edited) dataframe (ogsave|swapped)\n",
    "        new_k = pd.concat( [save_k,swapped_k] ,axis=1 )\n",
    "    else:\n",
    "        new_k = swapped_k\n",
    "\n",
    "    return new_k\n",
    "\n",
    "\n",
    "def geography_to_zipcode_ids_to_numeric(dataframe):\n",
    "    '''\n",
    "    convert \n",
    "        >> .Geography values \n",
    "            > like 'ZCTA5 00601' \n",
    "            > to int(00601)\n",
    "        >> .Id values\n",
    "            > like '8600000US00601' \n",
    "            > to int(860000000601)\n",
    "        >> .Id2 values\n",
    "            > like '00601'\n",
    "            > to int(00601)\n",
    "    '''\n",
    "    # copy\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # set old Geography\n",
    "    geo = df.Geography\n",
    "    # set old Id\n",
    "    _id = df.Id\n",
    "    # set old Id2\n",
    "    __id2 = df.Id2\n",
    "    \n",
    "    # make new 'Geography' values\n",
    "    new_geos = [int(i[-5:]) for i in geo]\n",
    "    # new 'Id' values\n",
    "    new_id = [int(''.join(i.split('US'))) for i in _id]\n",
    "    # new .Id2 instances\n",
    "    new__id2 = [int(d) for d in __id2]\n",
    "    \n",
    "    # convert dataframe\n",
    "    new_df = df.copy()\n",
    "    new_df.Geography = new_geos\n",
    "    new_df.Id = new_id\n",
    "    new_df.Id2 = new__id2\n",
    "    \n",
    "    # return new df\n",
    "    return new_df\n",
    "\n",
    "def kmeans_by(dataframe,n_clusters=10,converted=False):\n",
    "    '''\n",
    "    inputs:\n",
    "        >> dataframe\n",
    "            > dataframe to be edited\n",
    "        >> n_clusters \n",
    "            > default = 10\n",
    "            > number of clusters for KMeans\n",
    "        >> converted\n",
    "            > default = False\n",
    "            > assumes data is not ready for KMeans \n",
    "                >> if True, assumes df is ready for KMeans\n",
    "    output:\n",
    "        > pd.Dataframe of \n",
    "    '''\n",
    "    # copy data \n",
    "    d = dataframe.copy()  \n",
    "    \n",
    "    '''df conversion'''\n",
    "    # default\n",
    "    if converted!=True:\n",
    "        # copy data for editing\n",
    "        _data_ = d.copy()\n",
    "        \n",
    "        # convert first 3 columns ('Id', 'Id2', 'Geography')\n",
    "        _data = geography_to_zipcode_ids_to_numeric(dataframe=_data_)\n",
    "        \n",
    "        # convert remainder of dataframe\n",
    "        data = to_numeric_but(save_these_columns='none', dataframe=_data)\n",
    "        print(len(data),len(data.columns))\n",
    "\n",
    "    # dataframe has already been converted / otherwise\n",
    "    if converted==True:\n",
    "        data = d  \n",
    "    \n",
    "    '''KMeans'''\n",
    "    # fill NaN values\n",
    "    t = data.copy().fillna(0)\n",
    "    \n",
    "    # Convert DataFrame to matrix\n",
    "    mat = t.values\n",
    "    \n",
    "    # Using sklearn\n",
    "    km = KMeans(n_clusters)\n",
    "    # fit our values\n",
    "    km.fit(mat)\n",
    "    \n",
    "    # Get cluster assignment labels\n",
    "    labels = km.labels_\n",
    "    \n",
    "    # Format results as a DataFrame\n",
    "    results = pd.DataFrame([t.index,labels])\n",
    "\n",
    "    # display results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total population by county (adjusted to 2010 controls)\n",
    "county_2010=pd.read_csv('../../data/NHGIS/nhgis0001_csv/nhgis0001_ts_geog2010_county.csv')\n",
    "# total population by place\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: (CL8) Total Population\n",
    "#         CL8AA:       Persons: Total\n",
    "#         CL8AAL:      Lower bound: Persons: Total\n",
    "#         CL8AAU:      Upper bound: Persons: Total\n",
    "\n",
    " \n",
    "# Context Fields \n",
    "#         NHGISCODE:   NHGIS Integrated Geographic Unit Code\n",
    "#         GJOIN1970:   GIS Join Match Code, 1970\n",
    "#         GJOIN1980:   GIS Join Match Code, 1980\n",
    "#         GJOIN1990:   GIS Join Match Code, 1990\n",
    "#         GJOIN2000:   GIS Join Match Code, 2000\n",
    "#         GJOIN2010:   GIS Join Match Code, 2010\n",
    "#         GJOIN2012:   GIS Join Match Code, 2012\n",
    "#         STATE:       NHGIS Integrated State Name\n",
    "#         STATEFP:     FIPS State Code\n",
    "#         STATENH:     NHGIS Integrated State Code\n",
    "#         PLACE:       NHGIS Integrated Place Name\n",
    "#         PLACEA:      NHGIS Integrated Place Code\n",
    "#         NAME1970:    Area Name, 1970\n",
    "#         NAME1980:    Area Name, 1980\n",
    "#         NAME1990:    Area Name, 1990\n",
    "#         NAME2000:    Area Name, 2000\n",
    "#         NAME2010:    Area Name, 2010\n",
    "#         NAME2012:    Area Name, 2012\n",
    " \n",
    "# Table 1: (AV0) Total Population\n",
    "#     Time series AA: Persons: Total\n",
    "#         AV0AA1970:   1970: Persons: Total\n",
    "#         AV0AA1980:   1980: Persons: Total\n",
    "#         AV0AA1990:   1990: Persons: Total\n",
    "#         AV0AA2000:   2000: Persons: Total\n",
    "#         AV0AA2010:   2010: Persons: Total\n",
    "#         AV0AA125:    2008-2012: Persons: Total\n",
    "#         AV0AA125M:   Margin of error: 2008-2012: Persons: Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_pop_2010 = pop_by_place.loc[pop_by_place.AV0AA2010.idxmax()]\n",
    "for i in highest_pop_2010[22:24]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***: \n",
    "    - 2010: Persons: Total != 008-2012: Persons: Total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pop_by_place.NHGISCODE),len(pop_by_place.NHGISCODE.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x=(len(county_2010.DATAYEAR)/3)-1\n",
    "print(_x,_x*2,_x*3,'\\n',len(county_2010.DATAYEAR),len(county_2010.DATAYEAR.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_2010.iloc[9428]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Understanding STATEA'''\n",
    "print(len(county_2010[:3142].STATEA.unique()),'\\n',county_2010[:3142].STATEA.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=county_2010.copy()\n",
    "idx=q.DATAYEAR\n",
    "q=q.set_index(idx)\n",
    "q[['GISJOIN', 'DATAYEAR', 'STATEA',\n",
    "       'COUNTYA', 'CL8AA', 'CL8AAL', 'CL8AAU']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=county_2010[:3142].copy()\n",
    "y=county_2010[3143:6286].copy()\n",
    "z=county_2010[6286:].copy()\n",
    "\n",
    "x=x.apply(pd.to_numeric, errors='coerce')\n",
    "x=x.dropna(axis=1, how='all')\n",
    "y=y.apply(pd.to_numeric, errors='coerce')\n",
    "y=y.dropna(axis=1, how='all')\n",
    "z=z.apply(pd.to_numeric, errors='coerce')\n",
    "z=z.dropna(axis=1, how='all')\n",
    "\n",
    "a=kmeans_by(dataframe=x,n_clusters=31,converted=True)\n",
    "b=kmeans_by(dataframe=y,n_clusters=31,converted=True)\n",
    "c=kmeans_by(dataframe=z,n_clusters=31,converted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_i = a.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(pop_by_place.NHGISCODE),len(pop_by_place.NHGISCODE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pop_by_place.NHGISCODE\n",
    "# Table 1: (AV0) Total Population\n",
    "#     Time series AA: Persons: Total\n",
    "#         AV0AA1970:   1970: Persons: Total\n",
    "#         AV0AA1980:   1980: Persons: Total\n",
    "#         AV0AA1990:   1990: Persons: Total\n",
    "#         AV0AA2000:   2000: Persons: Total\n",
    "#         AV0AA2010:   2010: Persons: Total\n",
    "#         AV0AA125:    2008-2012: Persons: Total\n",
    "#         AV0AA125M:   Margin of error: 2008-2012: Persons: Total\n",
    "# pop_by_place.loc(places[1])\n",
    "places[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['column_name'] == some_value]\n",
    "# pop_by_place.loc[pop_by_place.STATE=='Arkansas']\n",
    "for i,place in enumerate(pop_by_place.PLACE):\n",
    "    if 'San Francisco' in place:\n",
    "        print(i,place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of \n",
    "# pleasanton 3200\n",
    "# bentonville 1549\n",
    "nyc=pop_by_place.iloc[[18672]][['NHGISCODE','PLACE','STATE','AV0AA1970','AV0AA1980', 'AV0AA1990', 'AV0AA2000','AV0AA2010']]\n",
    "sfo= pop_by_place.iloc[[3334]][['NHGISCODE','PLACE','STATE','AV0AA1970','AV0AA1980', 'AV0AA1990', 'AV0AA2000','AV0AA2010']]\n",
    "sfo\n",
    "# testa=pop_by_place[['NHGISCODE','PLACE','STATE','AV0AA1970','AV0AA1980', 'AV0AA1990', 'AV0AA2000','AV0AA2010']]\n",
    "# only keep columns with at least 5 non-NaN (aka 2 population measurements)\n",
    "# testa.dropna(thresh=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testb=pd.DataFrame(data=['place','1970','1980','1990','2000','2010','2008-2012'], index=['NHGISCODE','AV0AA1970','AV0AA1980', 'AV0AA1990', 'AV0AA2000','AV0AA2010', 'AV0AA125']).T\n",
    "# testb\n",
    "# testb=pd.DataFrame({'year': ['1970','1980','1990','2000','2010'],\n",
    "#                        'month': [12,12,12,12,12],\n",
    "#                        'day': [31,31,31,31,31]})\n",
    "# testb=testb.T.apply(pd.to_datetime)\n",
    "# testb\n",
    "\n",
    "# df of years w/ columns same as pop_by_place\n",
    "testb=pd.DataFrame(index=[1],data={'NHGISCODE':'x','AV0AA1970': '1970','AV0AA1980':'1980','AV0AA1990':'1990',\"AV0AA2000\":'2000',\"AV0AA2010\":'2010'})\n",
    "\n",
    "# convert to datetime\n",
    "testb=testb.apply(pd.to_datetime,errors='coerce')\n",
    "testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together \n",
    "# G01000124=pd.concat([testb,testa],axis=0)\n",
    "G01000124=pd.concat([testb,sfo],axis=0)\n",
    "# drop nan values columns (non-measured, etc)\n",
    "G01000124=G01000124.dropna(axis=1)\n",
    "# reset index (now == 0,1), transpose, then rename columns to Prophet standards\n",
    "G01000124=G01000124.reset_index().T.rename(columns={0: 'ds', 1:'y'})\n",
    "# has weird 'index' row at 0\n",
    "G01000124=G01000124[1:]\n",
    "G01000124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "e.g. 1-measure city\n",
    "ValueError: Dataframe has less than 2 non-NaN rows.\"\"\"\n",
    "# Make the prophet model and fit on the data\n",
    "gm_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "gm_prophet.fit(G01000124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a future dataframe for 2 years\n",
    "gm_forecast = gm_prophet.make_future_dataframe(periods=1 * 10, freq='Y')\n",
    "\n",
    "# Make predictions\n",
    "gm_forecast = gm_prophet.predict(gm_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prophet model \n",
    "gm_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "\n",
    "# fit model on our data\n",
    "gm_prophet.fit(G01000124)\n",
    "\n",
    "# Make a future dataframe for 2 years\n",
    "gm_forecast = gm_prophet.make_future_dataframe(periods=1 * 10, freq='Y')\n",
    "\n",
    "# Make predictions\n",
    "gm_forecast = gm_prophet.predict(gm_forecast)\n",
    "\n",
    "# identify change points\n",
    "city_changepoints = [str(date) for date in tesla_prophet.changepoints]\n",
    "\n",
    "gm_prophet.plot(gm_forecast, xlabel = 'Year', ylabel = 'Population')\n",
    "plt.title('Population of San Francisco');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_prophet.plot(gm_forecast, xlabel = 'Year', ylabel = 'Population')\n",
    "\n",
    "plt.title('Population of New York City');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
