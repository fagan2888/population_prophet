{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# total population by place\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc=pop_by_place.iloc[[18672]]\n",
    "nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place_ = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "pop_by_place_#.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - forget places with only 1 measurement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def population_by_place(years=20):\n",
    "# total population by place (1970 to 2010)\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "\"\"\"\n",
    "- generate DataFrame of population:\n",
    "    > from 1970 to 2010\n",
    "    > by unique place (use NHGISCODE as Id)\n",
    "- drop \n",
    "    > counties with less than 2 measurements\n",
    "        > can only predict counties which have been measured 2+ times \n",
    "- extract list of counties\n",
    "    > each as a DataFrame ready for prediction \n",
    "    > column0='ds' , column1='y'\n",
    "\"\"\"\n",
    "\n",
    "# df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "# generate list of remaining NHGISCODE codes \n",
    "codes_of_measureable_unique_places = [code for code in measureable_unique_places.NHGISCODE]\n",
    "# drop NHGISCODE column (25103 rows × 4 columns)\n",
    "measureable_unique_places = measureable_unique_places.drop('NHGISCODE',axis=1)\n",
    "\n",
    "# list of str column names as years (for conversion to datetime)\n",
    "year_only_columns = [i[5:] for i in measureable_unique_places.columns]\n",
    "# convert year_only_columns to DatetimeIndex of Timestamps\n",
    "dt_columns = pd.to_datetime(arg=year_only_columns)\n",
    "\n",
    "# convert dt_columns into dataframe \n",
    "datetime_df = pd.DataFrame(dt_columns).T\n",
    "# w/ columns, so concatable with measureable_unique_counties\n",
    "datetime_df.columns = measureable_unique_places.columns\n",
    "\n",
    "# generate list of remaining places (each as pd.Series)\n",
    "dfs_of_measureable_unique_places = [measureable_unique_places.iloc[place] for place in range(len(measureable_unique_places))]\n",
    "\n",
    "# add datetime_df to each dataframe as first row\n",
    "prophet_places = [pd.concat((datetime_df,pd.DataFrame(place).T),axis=0) for place in dfs_of_measureable_unique_places]\n",
    "# then transpose to 2 rows x 23 columns \n",
    "prophet_almost_ready_places = [place.T for place in prophet_places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fbprophet \n",
    "\n",
    "def population_by_place(years=20,n_places=1000,changepoint_prior=0.15,indicate=False):\n",
    "    # total population by place (1970 to 2010)\n",
    "    pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "    \"\"\"\n",
    "    inputs) \n",
    "    >> years\n",
    "        > number of years to forecast\n",
    "    >> places\n",
    "        > number of places to forecast +1 \n",
    "            >> e.g. 99 = first 100 places (max==25102)\n",
    "    >> changepoint_prior\n",
    "        > set changepoint_prior_scale for prophet model\n",
    "    >> indicate\n",
    "        > default False\n",
    "        > if True, print number of place forecasted after each forecast\n",
    "    >> time\n",
    "        > default False\n",
    "        > if True, prints time the function took to run right before returning output\n",
    "    \n",
    "    function: \n",
    "    >> generate DataFrame of population:\n",
    "        > from 1970 to 2010\n",
    "        > by unique place (use NHGISCODE as Id)\n",
    "    >> drop \n",
    "        > places with less than 2 measurements\n",
    "            > can only predict places which have been measured 2+ times \n",
    "    >> extract list of places\n",
    "        > each as a DataFrame ready for prediction \n",
    "        > column0='ds' , column1='y'\n",
    "    >> make and fit prophet model on each place\n",
    "    >> return prophet model's predictions\n",
    "        > of each place\n",
    "        > for {years} years\n",
    "    \"\"\"\n",
    "\n",
    "    # df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "    unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "    # drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "    measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "    # convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "    measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "    # generate list of remaining NHGISCODE codes \n",
    "    codes_of_measureable_unique_places = [code for code in measureable_unique_places.NHGISCODE]\n",
    "    # drop NHGISCODE column (25103 rows × 4 columns)\n",
    "    measureable_unique_places = measureable_unique_places.drop('NHGISCODE',axis=1)\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    year_only_columns = [i[5:] for i in measureable_unique_places.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=year_only_columns)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = measureable_unique_places.columns\n",
    "\n",
    "    # generate list of remaining places (each as pd.Series)\n",
    "    dfs_of_measureable_unique_places = [measureable_unique_places.iloc[place] for place in range(len(measureable_unique_places))]\n",
    "\n",
    "    # add datetime_df to each dataframe as first row\n",
    "    prophet_places = [pd.concat((datetime_df,pd.DataFrame(place).T),axis=0) for place in dfs_of_measureable_unique_places]\n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_places = [place.T for place in prophet_places]\n",
    "\n",
    "    # set collection of prophets \n",
    "    prophet_by_place = []\n",
    "\n",
    "    # run prophet model on first 1000 places\n",
    "    for place in range(len(prophet_almost_ready_places[:n_places])):\n",
    "        # make the prophet model\n",
    "        place_prophet = fbprophet.Prophet(changepoint_prior_scale=changepoint_prior)\n",
    "        # identify county \n",
    "        a = prophet_almost_ready_places[place]\n",
    "        # rename place df's columns to agree with prophet formatting\n",
    "        a.columns = ['ds','y']\n",
    "        # fit place on prophet model \n",
    "        b = place_prophet.fit(a)\n",
    "        # make a future dataframe for 20 years\n",
    "        place_forecast = place_prophet.make_future_dataframe( periods=1*years, freq='Y' )\n",
    "        # establish predictions\n",
    "        place_forecast = place_prophet.predict(place_forecast)\n",
    "        # add to collection \n",
    "        prophet_by_place.append(place_forecast)\n",
    "        # did we ask for indication (hint: do this if calculating for > 1000 places unless you enjoy anxiety)\n",
    "        if indicate==True:\n",
    "            # let us know the count\n",
    "            print(place)\n",
    "        \n",
    "    # return forecasts\n",
    "    return prophet_by_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_place = population_by_place(indicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_place[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "sample=[]\n",
    "for code in measureable_unique_places[:10].NHGISCODE:\n",
    "    a=pop_by_place.loc[pop_by_place['NHGISCODE'] == code]\n",
    "    sample.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample)):\n",
    "    o=sample[i][['PLACE','STATE']]\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abbeville city  Alabama; \n",
    "2,721 2586.673761\n",
    "2,699 2576.742940\n",
    "2,684 2538.960959\n",
    "2,654 2500.844134\n",
    "2,646 2462.401057\n",
    "2,627 2452.470236\n",
    "2,594 2414.688255\n",
    "\"\"\"\n",
    "print(by_place[0][['yhat']][5:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "\n",
    "unique_places = pop_by_place.copy()[['PLACE','STATE','AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "_measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "_measureable_unique_places = _measureable_unique_places.fillna(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_measureable_unique_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.NHGISCODE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.copy()[['PLACE','STATE']].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Abbeville city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "all_places = [place for place in pop_by_place.PLACE]\n",
    " \n",
    "\n",
    "def forecast_this(place,n_years=20):\n",
    "    a=0\n",
    "    for p in all_places:\n",
    "        if p == place:\n",
    "            a=pop_by_place.loc[pop_by_place['PLACE'] == place]\n",
    "        elif p == place + ' city':\n",
    "            a=pop_by_place.loc[pop_by_place['PLACE'] == place + ' city']\n",
    "    print(a.columns)\n",
    "#         # make the prophet model\n",
    "#         place_prophet = fbprophet.Prophet(changepoint_prior_scale=changepoint_prior)\n",
    "#         # identify county \n",
    "#         a = prophet_almost_ready_places[place]\n",
    "#         # rename place df's columns to agree with prophet formatting\n",
    "#         a.columns = ['ds','y']\n",
    "#         # fit place on prophet model \n",
    "#         b = place_prophet.fit(a)\n",
    "#         # make a future dataframe for 20 years\n",
    "#         place_forecast = place_prophet.make_future_dataframe( periods=1*n_years, freq='Y' )\n",
    "#         # establish predictions\n",
    "#         place_forecast = place_prophet.predict(place_forecast)\n",
    "#         # add to collection \n",
    "#         prophet_by_place.append(place_forecast)\n",
    "forecast_this('San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_places:\n",
    "    if i == 'Abbeville city':\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a17 = actual7.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a16 = actual6.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a15 = actual5.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a14 = actual4.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a13 = actual3.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a12 = actual2.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a11 = actual1.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "\n",
    "# len(a17),len(a16),len(a15),len(a14),len(a13),len(a12),len(a11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a17places = [place for place in a17['GEO.display-label']]\n",
    "a16places = [place for place in a16['GEO.display-label']]\n",
    "a15places = [place for place in a15['GEO.display-label']]\n",
    "a14places = [place for place in a14['GEO.display-label']]\n",
    "a13places = [place for place in a13['GEO.display-label']]\n",
    "a12places = [place for place in a12['GEO.display-label']]\n",
    "a11places = [place for place in a11['GEO.display-label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_places=[]\n",
    "for place in a17places:\n",
    "    if place in a16places:\n",
    "        if place in a15places:\n",
    "            if place in a14places:\n",
    "                if place in a13places:\n",
    "                    if place in a12places:\n",
    "                        if place in a11places:\n",
    "                            combo_places.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combo_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- convert dataframes to only contain shared values\n",
    "\"\"\"\n",
    "a17=a17.loc[a17['GEO.display-label'].isin(combo_places)]\n",
    "a16=a16.loc[a16['GEO.display-label'].isin(combo_places)]\n",
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "a14=a14.loc[a14['GEO.display-label'].isin(combo_places)]\n",
    "a13=a13.loc[a13['GEO.display-label'].isin(combo_places)]\n",
    "a12=a12.loc[a12['GEO.display-label'].isin(combo_places)]\n",
    "a11=a11.loc[a11['GEO.display-label'].isin(combo_places)]\n",
    "# len(a17),len(a16),len(a15),len(a14),len(a13),len(a12),len(a11)\n",
    "a17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "base=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# select columns of measurement\n",
    "get_base=base.copy()[['PLACE','STATE','AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "_measureable_base = get_base.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "_measureable_base = _measureable_base.fillna(0)\n",
    "_measureable_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_measureable_unique_places),len(_measureable_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "a14=a14.loc[a14['GEO.display-label'].isin(combo_places)]\n",
    "a13=a13.loc[a13['GEO.display-label'].isin(combo_places)]\n",
    "a12=a12.loc[a12['GEO.display-label'].isin(combo_places)]\n",
    "a11=a11.loc[a11['GEO.display-label'].isin(combo_places)]\n",
    "a15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split GEO.display-label into list [place, state]\n",
    "places_11_17 = [label.split(',') for label in a15['GEO.display-label']]\n",
    "# make DataFrame of Place, State for compairson to _measureable_unique_places and _measureable_base\n",
    "df_places_11_17 = pd.DataFrame(places_11_17,columns=['PLACE','STATE'])\n",
    "# add population estimates \n",
    "eleven_seventeen = [df_places_11_17,a11.HC01_EST_VC01,a12.HC01_EST_VC01,a13.HC01_EST_VC01,a14.HC01_EST_VC01,a15.HC01_EST_VC01]\n",
    "df_pop_by_place_11_17 = pd.concat(eleven_seventeen,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_by_place_11_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_measureable_unique_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_this(place,state,prior):\n",
    "    # set place state\n",
    "    place_w_state=place+', '+state\n",
    "    \n",
    "    # 1970-2010\n",
    "    sf70to10 = _measureable_unique_places.loc[_measureable_unique_places['PLACE'] == place]\n",
    "    # 2011\n",
    "    sf11 = a11.loc[a11['GEO.display-label'] == place_w_state]\n",
    "    # 2012\n",
    "    sf12 = a12.loc[a12['GEO.display-label'] == place_w_state]\n",
    "    # 2013\n",
    "    sf13 = a13.loc[a13['GEO.display-label'] == place_w_state]\n",
    "    # 2014\n",
    "    sf14 = a14.loc[a14['GEO.display-label'] == place_w_state]\n",
    "    # 2015\n",
    "    sf15 = a15.loc[a15['GEO.display-label'] == place_w_state]\n",
    "\n",
    "    # combine all population measurements \n",
    "    sf_pops = sf70to10,sf11.HC01_EST_VC01,sf12.HC01_EST_VC01,sf13.HC01_EST_VC01,sf14.HC01_EST_VC01,sf15.HC01_EST_VC01\n",
    "\n",
    "    # combine dataframes\n",
    "    combo_sf = pd.concat(sf_pops,axis=1,sort=True)\n",
    "    # fill lowest row NaN values with value from above rows (if exists)\n",
    "    combo_sf = combo_sf.fillna(method='ffill')\n",
    "    # drop rows with NaN values\n",
    "    combo_sf = combo_sf.dropna(axis=0,thresh=7)\n",
    "\n",
    "    # drop Place and State columns\n",
    "    combo_sf = combo_sf.drop( ['PLACE' , 'STATE'] ,axis=1 )\n",
    "\n",
    "    # change column names to years 1970-2010 & 2011-2015\n",
    "    combo_sf.columns = ['1970', '1980', '1990', '2000', '2010', '2011','2012','2013','2014','2015']\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    pre_dt_cols = [i for i in combo_sf.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=pre_dt_cols)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = combo_sf.columns\n",
    "\n",
    "    # add datetime_df as first row\n",
    "    prophet_place = pd.concat((datetime_df,combo_sf),axis=0) \n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_place = prophet_place.T\n",
    "\n",
    "    # convert df to short \n",
    "    a = prophet_almost_ready_place\n",
    "\n",
    "    # make the prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=prior)\n",
    "    \n",
    "    print(a)\n",
    "    if len(a.columns) == 3:\n",
    "        a = a.drop(a.columns[2],axis=1)\n",
    "        print(a)\n",
    "\n",
    "    # rename place df's columns to agree with prophet formatting\n",
    "    a.columns = ['ds','y']\n",
    "    print(a)\n",
    "    # fit place on prophet model \n",
    "    b = place_prophet.fit(a)\n",
    "\n",
    "    # make a future dataframe for 20 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=1*20, freq='Y' )\n",
    "    # establish predictions\n",
    "    sf_forecast = place_prophet.predict(place_forecast)\n",
    "    \n",
    "    # output predictions\n",
    "    return [sf_forecast,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "sf = prophet_this(place='Denver city',state='Colorado',prior=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "base_16 = int(sf[1].y[9]) + (int(sf[1].y[9])-int(sf[1].y[8]))\n",
    "base_17 = base_16 + (int(sf[1].y[9])-int(sf[1].y[8]))\n",
    "\n",
    "# predictions 2016 & 2017\n",
    "p_16 = sf[0]['yhat'][11]\n",
    "p_17 = sf[0]['yhat'][12]\n",
    "\n",
    "# actual 2016 & 2017\n",
    "sf16 = a16.loc[a16['GEO.display-label'] == 'Denver city, Colorado']\n",
    "sf17 = a17.loc[a17['GEO.display-label'] == 'Denver city, Colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_16 = int(sf16.HC01_EST_VC01)\n",
    "act_17 = int(sf17.HC01_EST_VC01)\n",
    "# actual - base 2016\n",
    "ab16 = act_16-base_16\n",
    "# pred - base 2016\n",
    "ap16 = act_16-p_16\n",
    "# pred - base 2017\n",
    "ab17 = act_17-base_17\n",
    "# pred - base 2017\n",
    "ap17 = act_17-p_17\n",
    "\n",
    "print(f\"{abs(ab16)}\\n{abs(int(ap16))}\\n\\n{abs(ab17)}\\n{abs(int(ap17))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "San Francisco city, California\n",
    "sf = prophet_this(place='San Francisco city',state='California')\n",
    "changepoint_prior_scale=0.05\n",
    "-2172 27830\n",
    "-66162 -24158\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Bentonville city, Arkansas\n",
    "sf = prophet_this(place='Bentonville city',state='Arkansas',prior=0.05)\n",
    "335 3508\n",
    "-9938 -5722\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "New York city, New York\n",
    "sf = prophet_this(place='New York city',state='New York',prior=0.15)\n",
    "-36636 -66472\n",
    "-441471 -429353\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "New Orleans city, New Orleans\n",
    "sf = prophet_this(place='New Orleans city',state='Louisiana',prior=0.15)\n",
    "-2083 44438\n",
    "-71863 -13624\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# san francisco forecast population through 2017\n",
    "sf[0][['ds','yhat']][:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf16.HC01_EST_VC01.values[0],sf17.HC01_EST_VC01.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_16 = 42499\n",
    "r_17 = 34022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_16-r_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_16-r_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_17-r_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_17-r_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_11 = a11.loc[a11['GEO.display-label'] == 'Pleasanton city, California']\n",
    "pl_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[]\n",
    "def prophet_this(place,state):\n",
    "    # set place state\n",
    "    place_w_state=place+', '+state\n",
    "    \n",
    "    # 1970-2010\n",
    "    sf70to10 = _measureable_unique_places.loc[_measureable_unique_places['PLACE'] == place]\n",
    "    # 2011\n",
    "    sf11 = a11.loc[a11['GEO.display-label'] == place_w_state]\n",
    "    # 2012\n",
    "    sf12 = a12.loc[a12['GEO.display-label'] == place_w_state]\n",
    "    # 2013\n",
    "    sf13 = a13.loc[a13['GEO.display-label'] == place_w_state]\n",
    "    # 2014\n",
    "    sf14 = a14.loc[a14['GEO.display-label'] == place_w_state]\n",
    "    # 2015\n",
    "    sf15 = a15.loc[a15['GEO.display-label'] == place_w_state]\n",
    "\n",
    "    # combine all population measurements \n",
    "    sf_pops = sf70to10,sf11.HC01_EST_VC01,sf12.HC01_EST_VC01,sf13.HC01_EST_VC01,sf14.HC01_EST_VC01,sf15.HC01_EST_VC01\n",
    "\n",
    "    # combine dataframes\n",
    "    combo_sf = pd.concat(sf_pops,axis=1,sort=True)\n",
    "    # fill lowest row NaN values with value from above rows (if exists)\n",
    "    combo_sf = combo_sf.fillna(method='ffill')\n",
    "    # drop rows with NaN values\n",
    "    combo_sf = combo_sf.dropna(axis=0,thresh=7)\n",
    "\n",
    "    # drop Place and State columns\n",
    "    combo_sf = combo_sf.drop(['PLACE', 'STATE'],axis=1)\n",
    "\n",
    "    # change column names to years 1970-2010 & 2011-2015\n",
    "    combo_sf.columns = ['1970', '1980', '1990', '2000', '2010', '2011','2012','2013','2014','2015']\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    pre_dt_cols = [i for i in combo_sf.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=pre_dt_cols)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = combo_sf.columns\n",
    "\n",
    "    # add datetime_df as first row\n",
    "    prophet_place = pd.concat((datetime_df,combo_sf),axis=0) \n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_place = prophet_place.T\n",
    "\n",
    "    # convert df to short \n",
    "    a = prophet_almost_ready_place\n",
    "\n",
    "    # make the prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "\n",
    "    # rename place df's columns to agree with prophet formatting\n",
    "    a.columns = ['ds','y']\n",
    "    # fit place on prophet model \n",
    "    b = place_prophet.fit(a)\n",
    "\n",
    "    # make a future dataframe for 20 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=1*20, freq='Y' )\n",
    "    # establish predictions\n",
    "    sf_forecast = place_prophet.predict(place_forecast)\n",
    "\n",
    "    # benchmark 2016 & 2017\n",
    "    base_16=int(prophet_almost_ready_place.y[9])+(int(prophet_almost_ready_place.y[9])-int(prophet_almost_ready_place.y[8]))\n",
    "    base_17=base_16+(int(prophet_almost_ready_place.y[9])-int(prophet_almost_ready_place.y[8]))\n",
    "    \n",
    "    # predictions 2016 & 2017\n",
    "    p_16 = sf_forecast['yhat'][11]\n",
    "    p_17 = sf_forecast['yhat'][12]\n",
    "    \n",
    "    # actual 2016 & 2017\n",
    "    sf16 = a16.loc[a16['GEO.display-label'] == place_w_state]\n",
    "    sf17 = a17.loc[a17['GEO.display-label'] == place_w_state]\n",
    "    \n",
    "    # output base 2016 vs pred 2016 ; base 2017 vs real 2017\n",
    "    return [[base_16-r_16,p_16-r_16],[base_17-r_17,p_17-r_17]]\n",
    "\n",
    "# run\n",
    "aaa=prophet_this(place='Bentonville city',state='Arkansas')\n",
    "# bbb=prophet_this(place='Fayetteville city',state='Arkansas')\n",
    "out.append(aaa)\n",
    "# out.append(bbb)\n",
    "# prophet \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# def mega_place_df():\n",
    "\"\"\"\n",
    ">> takes in \n",
    "    > Census 1970-2010 dataframe (1 df)\n",
    "        >> total population by Place measurements\n",
    "    > American Community Survey (ACS) 2011-2017 dataframes (7 dfs)\n",
    "        >> total population (age & sex) by Place \n",
    "        \n",
    ">> forges DataFrame of places that have \n",
    "    > at least one (1) recording for Census years 1970-2010\n",
    "    > at least one (1) recording for ACS years 2011-2015\n",
    "\n",
    ">> test our model v. base on\n",
    "    > random sample 100 Places\n",
    "    > random sample 100 Places from bottom half population size\n",
    "    > random sample 100 Places from top half population size\n",
    "\"\"\"\n",
    "\n",
    "'''load Train data'''\n",
    "# population by Place Census 1970-2010 measurements\n",
    "load_census_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "\n",
    "'''load Test data'''\n",
    "# population by Place ACS 2016\n",
    "load_acs_20l6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2017\n",
    "load_acs_20l7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'''find common places across Census and each train ACS'''\n",
    "# identify Places measured in 2011 ACS [0 == 'Geography']\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2012 ACS \n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS \n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS \n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]\n",
    "\n",
    "# cross 2011-2015, keep coexisting Places\n",
    "train_places = [place for place in acs11places if place in acs12places and acs13places and acs14places and acs15places]\n",
    "\n",
    "\n",
    "'''find common places across 2016 & 2017 (test ACSs)'''\n",
    "# identify Places measured in 2016 ACS (29575) [0 == 'Geography']\n",
    "acs16places = [place for place in load_acs_20l6['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2017 ACS (29577)\n",
    "acs17places = [place for place in load_acs_20l7['GEO.display-label']]\n",
    "\n",
    "# cross 2017 Places w/ 2016 Places, keep coexisting Places (29551)\n",
    "base_places = [place for place in acs17places if place in acs16places]\n",
    "\n",
    "\n",
    "'''find common Places across the Places our model will train on {train_places} \n",
    "    and the Places our model can predict on {base_places}'''\n",
    "# identify Places we can compare our predictions with\n",
    "measureable_places = [place for place in train_places if place in base_places]\n",
    "\n",
    "'''clean Census 1970-2010 df (Train)'''\n",
    "# identify columns needed to make GEO.display-label column (so can pair with ACS DataFrames) \n",
    "for_geo_displays = ['PLACE','STATE']\n",
    "# pull those columns \n",
    "to_geo_displays = load_census_place[for_geo_displays]\n",
    "\n",
    "# mold PLACE column into list with Place formatted as is in GEO.display-label\n",
    "places_70_10 = [place + ', ' for place in to_geo_displays.PLACE]\n",
    "\n",
    "# list paired State for each Place\n",
    "states_70_10 = [state for state in to_geo_displays.STATE]\n",
    "\n",
    "# merge places_70_10 and states_70_10 into list formatted as GEO.display-label column\n",
    "GEO_display_label = [ places_70_10[i] + states_70_10[i] for i in range(len(places_70_10))]\n",
    "\n",
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "# set base dataframe using Census (1970-2010) measurements \n",
    "pop_place_70_10_ = load_census_place[place_cols_of_interest]\n",
    "\n",
    "# add GEO.display-label column from GEO_display_label list\n",
    "pop_place_70_10_['GEO.display-label'] = GEO_display_label\n",
    "\n",
    "\n",
    "'''clean American Community Survey (ACS) 2011-2015 dataframes (Train)'''\n",
    "# ID columns we will be using\n",
    "columns = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "# convert 2011\n",
    "acs_20l1 = load_acs_20l1[columns]\n",
    "# convert 2012\n",
    "acs_20l2 = load_acs_20l2[columns]\n",
    "# convert 2013\n",
    "acs_20l3 = load_acs_20l3[columns]\n",
    "# convert 2014\n",
    "acs_20l4 = load_acs_20l4[columns]\n",
    "# convert 2015\n",
    "acs_20l5 = load_acs_20l5[columns]\n",
    "\n",
    "\n",
    "'''convert Train years to reflect Places only seen in measureable_places'''\n",
    "# drop Census Places not ideal for measurement (29346)\n",
    "census_place_populations = pop_place_70_10_.loc[pop_place_70_10_['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2011_place_populations = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2012_place_populations = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2013_place_populations = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2014_place_populations = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2015_place_populations = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "\n",
    "'''clean ACS 2016 & 2017 dataframes (Test)\n",
    "    take a sample of 100 Places to score our model'''\n",
    "# identify 2016/2017 columns of interest (to measure against)\n",
    "test_col_of_i = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_16_ = load_acs_20l6[test_col_of_i]\n",
    "# realize ACS 2016 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_16_df_ = testd_16_.loc[testd_16_['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_17_ = load_acs_20l7[test_col_of_i]\n",
    "# realize ACS 2017 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_17_df_ = testd_17_.loc[testd_17_['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "# sample Baseline data for Places to evaluate model \n",
    "sample_one_hunnit = test_17_df_.sample(100)\n",
    "# list Places for conversion of other Datas\n",
    "sample_places = [place for place in sample_one_hunnit['GEO.display-label']]\n",
    "\n",
    "\n",
    "'''adjust Train dataframes to sampled Places'''\n",
    "# shrink Census DataFrame to sampled Places\n",
    "_s_census_ = census_place_populations.loc[census_place_populations['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2011 ACS df to sampled Places \n",
    "_s_acs_2011_ = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2012 ACS DataFrame to sampled Places \n",
    "_s_acs_2012_ = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2013 ACS df to Places in sample  \n",
    "_s_acs_2013_ = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2014 ACS DataFrame to sampled Places \n",
    "_s_acs_2014_ = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2015 ACS df to sampled Places \n",
    "_s_acs_2015_ = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(sample_places)]\n",
    "\n",
    "\n",
    "'''adjust Test dataframes to sampled Places'''\n",
    "# 2016 ACS df to sampled Places \n",
    "test_16_df = test_16_df_.loc[test_16_df_['GEO.display-label'].isin(sample_places)]\n",
    "# 2017 ACS DataFrame to sampled Places \n",
    "test_17_df = test_17_df_.loc[test_17_df_['GEO.display-label'].isin(sample_places)]\n",
    "\n",
    "\n",
    "'''forge Train DataFrame'''\n",
    "# set Census index to Places, and forget Place column \n",
    "s_census_ = _s_census_.copy().set_index(_s_census_['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# rename Census columns to years for later datetime conversion\n",
    "s_census_.columns = ['1970','1980','1990','2000','2010']\n",
    "\n",
    "# set 2011 index to Places \n",
    "s_acs_2011_ = _s_acs_2011_.copy().set_index(_s_acs_2011_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2011_.columns = ['no','2011']\n",
    "s_acs_2011_ = s_acs_2011_['2011']\n",
    "\n",
    "# set 2012 index to Places \n",
    "s_acs_2012_ = _s_acs_2012_.copy().set_index(_s_acs_2012_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2012_.columns = ['no','2012']\n",
    "s_acs_2012_ = s_acs_2012_['2012']\n",
    "\n",
    "# set 2013 index to Places \n",
    "s_acs_2013_ = _s_acs_2013_.copy().set_index(_s_acs_2013_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2013_.columns = ['no','2013']\n",
    "s_acs_2013_ = s_acs_2013_['2013']\n",
    "\n",
    "# set 2014 index to Places \n",
    "s_acs_2014_ = _s_acs_2014_.copy().set_index(_s_acs_2014_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2014_.columns = ['no','2014']\n",
    "s_acs_2014_ = s_acs_2014_['2014']\n",
    "\n",
    "# set 2015 index to Places \n",
    "s_acs_2015_ = _s_acs_2015_.copy().set_index(_s_acs_2015_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2015_.columns = ['no','2015']\n",
    "s_acs_2015_ = s_acs_2015_['2015']\n",
    "\n",
    "# forge Train DataFrame and convert NaN values to 0 (assumes population not measured is 0) \n",
    "train_df = pd.concat([s_census_,s_acs_2011_,s_acs_2012_,s_acs_2013_,s_acs_2014_,s_acs_2015_],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.0.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.0.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.0.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.0.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.0.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''forecast 2016 and 2017 populations using model for each sample Place'''\n",
    "# set out route for forecast tables\n",
    "out = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds = []\n",
    "# make DataFrame of column values as datetime\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)-95):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=3, freq='Y' )\n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    # tag and bag (forecast table)\n",
    "    out.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds.append([forecast[-2:].yhat.values[0],forecast[-2:].yhat.values[1]])\n",
    "\n",
    "    \n",
    "'''make Baseline predictions of 2016 and 2017 population on sample Places'''\n",
    "# set out route\n",
    "baseline_preds = []\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    # identify 2014 population\n",
    "    prior = int(df['2014'])\n",
    "    # identify 2015 population\n",
    "    past = int(df['2015'])\n",
    "    # calculate change\n",
    "    change = past - prior\n",
    "    # make 2016 prediction \n",
    "    p_16 = past + change\n",
    "    # make 2017 prediction \n",
    "    p_17 = p_16 + change\n",
    "    # pair prediction, tag & bag\n",
    "    baseline_preds.append([p_16,p_17])\n",
    "\n",
    "    \n",
    "'''pull actual measurements for 2016 and 2017 population for each sample Place'''\n",
    "# actual populations for 2016\n",
    "test_16 = [actual_population for actual_population in test_16_df.HC01_EST_VC01]\n",
    "# actual populations for 2017\n",
    "test_17 = [actual_population for actual_population in test_17_df.HC01_EST_VC01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "Train: 2680.804345697886\n",
      "Test: 2638\n",
      "Actual: 151\n",
      "\n",
      "Train: 1499.8874148477807\n",
      "Test: 1545\n",
      "Actual: 2627\n",
      "\n",
      "Train: 4010.011689241317\n",
      "Test: 3585\n",
      "Actual: 4422\n",
      "\n",
      "Train: 846.0395154060243\n",
      "Test: 632\n",
      "Actual: 757\n",
      "\n",
      "Train: 109.02902904466023\n",
      "Test: 68\n",
      "Actual: 252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''compare Train vs Test to Baseline vs Test\n",
    "    measure how close (%) Train is to Baseline in compairson to Test'''\n",
    "# track closeness of Train prediction to actual\n",
    "measure_2016_train = 0\n",
    "# track closeness of Baseline prediction to actual\n",
    "measure_2016_base = 0\n",
    "\n",
    "# start measuring 2016\n",
    "for i in range(len(test_16)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2680.804345697886"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][-2:].yhat.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO.display-label</th>\n",
       "      <th>HC01_EST_VC01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abanda CDP, Alabama</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbeville city, Alabama</td>\n",
       "      <td>2627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adamsville city, Alabama</td>\n",
       "      <td>4422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Addison town, Alabama</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Akron town, Alabama</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alabaster city, Alabama</td>\n",
       "      <td>32269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Albertville city, Alabama</td>\n",
       "      <td>21434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander City city, Alabama</td>\n",
       "      <td>14859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexandria CDP, Alabama</td>\n",
       "      <td>3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aliceville city, Alabama</td>\n",
       "      <td>3218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Allgood town, Alabama</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Altoona town, Alabama</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Andalusia city, Alabama</td>\n",
       "      <td>9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anderson town, Alabama</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Anniston city, Alabama</td>\n",
       "      <td>22441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Arab city, Alabama</td>\n",
       "      <td>8247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ardmore town, Alabama</td>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Argo town, Alabama</td>\n",
       "      <td>4145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ariton town, Alabama</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Arley town, Alabama</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ashford town, Alabama</td>\n",
       "      <td>2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ashland town, Alabama</td>\n",
       "      <td>2231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ashville city, Alabama</td>\n",
       "      <td>2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Athens city, Alabama</td>\n",
       "      <td>24461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Atmore city, Alabama</td>\n",
       "      <td>10067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Attalla city, Alabama</td>\n",
       "      <td>5934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Auburn city, Alabama</td>\n",
       "      <td>60318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Autaugaville town, Alabama</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Avon town, Alabama</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Axis CDP, Alabama</td>\n",
       "      <td>992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29545</th>\n",
       "      <td>San Isidro comunidad, Puerto Rico</td>\n",
       "      <td>6685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29546</th>\n",
       "      <td>San José comunidad, Puerto Rico</td>\n",
       "      <td>2599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29547</th>\n",
       "      <td>San Juan zona urbana, Puerto Rico</td>\n",
       "      <td>350402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29548</th>\n",
       "      <td>San Lorenzo zona urbana, Puerto Rico</td>\n",
       "      <td>8084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29549</th>\n",
       "      <td>San Sebastián zona urbana, Puerto Rico</td>\n",
       "      <td>9469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29550</th>\n",
       "      <td>Santa Bárbara comunidad, Puerto Rico</td>\n",
       "      <td>5261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29551</th>\n",
       "      <td>Santa Clara comunidad, Puerto Rico</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29552</th>\n",
       "      <td>Santa Isabel zona urbana, Puerto Rico</td>\n",
       "      <td>6378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29553</th>\n",
       "      <td>Santo Domingo comunidad, Puerto Rico</td>\n",
       "      <td>3753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29554</th>\n",
       "      <td>Stella comunidad, Puerto Rico</td>\n",
       "      <td>1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29555</th>\n",
       "      <td>Suárez comunidad, Puerto Rico</td>\n",
       "      <td>2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29556</th>\n",
       "      <td>Tallaboa comunidad, Puerto Rico</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29557</th>\n",
       "      <td>Tallaboa Alta comunidad, Puerto Rico</td>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29558</th>\n",
       "      <td>Tibes comunidad, Puerto Rico</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29559</th>\n",
       "      <td>Tiburones comunidad, Puerto Rico</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29560</th>\n",
       "      <td>Tierras Nuevas Poniente comunidad, Puerto Rico</td>\n",
       "      <td>3731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29561</th>\n",
       "      <td>Toa Alta zona urbana, Puerto Rico</td>\n",
       "      <td>3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>Toa Baja zona urbana, Puerto Rico</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>Trujillo Alto zona urbana, Puerto Rico</td>\n",
       "      <td>46226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>Utuado zona urbana, Puerto Rico</td>\n",
       "      <td>7706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>Vayas comunidad, Puerto Rico</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29566</th>\n",
       "      <td>Vázquez comunidad, Puerto Rico</td>\n",
       "      <td>1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29567</th>\n",
       "      <td>Vega Alta zona urbana, Puerto Rico</td>\n",
       "      <td>9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29568</th>\n",
       "      <td>Vega Baja zona urbana, Puerto Rico</td>\n",
       "      <td>24751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29569</th>\n",
       "      <td>Vieques comunidad, Puerto Rico</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29570</th>\n",
       "      <td>Vieques zona urbana, Puerto Rico</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29571</th>\n",
       "      <td>Villalba zona urbana, Puerto Rico</td>\n",
       "      <td>3528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29572</th>\n",
       "      <td>Yabucoa zona urbana, Puerto Rico</td>\n",
       "      <td>6720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29573</th>\n",
       "      <td>Yauco zona urbana, Puerto Rico</td>\n",
       "      <td>17627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29574</th>\n",
       "      <td>Yaurel comunidad, Puerto Rico</td>\n",
       "      <td>1255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    GEO.display-label HC01_EST_VC01\n",
       "1                                 Abanda CDP, Alabama           151\n",
       "2                             Abbeville city, Alabama          2627\n",
       "3                            Adamsville city, Alabama          4422\n",
       "4                               Addison town, Alabama           757\n",
       "5                                 Akron town, Alabama           252\n",
       "6                             Alabaster city, Alabama         32269\n",
       "7                           Albertville city, Alabama         21434\n",
       "8                        Alexander City city, Alabama         14859\n",
       "9                             Alexandria CDP, Alabama          3980\n",
       "10                           Aliceville city, Alabama          3218\n",
       "11                              Allgood town, Alabama           663\n",
       "12                              Altoona town, Alabama           768\n",
       "13                            Andalusia city, Alabama          9042\n",
       "14                             Anderson town, Alabama           259\n",
       "15                             Anniston city, Alabama         22441\n",
       "16                                 Arab city, Alabama          8247\n",
       "17                              Ardmore town, Alabama          1526\n",
       "18                                 Argo town, Alabama          4145\n",
       "19                               Ariton town, Alabama           772\n",
       "20                                Arley town, Alabama           477\n",
       "21                              Ashford town, Alabama          2211\n",
       "22                              Ashland town, Alabama          2231\n",
       "23                             Ashville city, Alabama          2490\n",
       "24                               Athens city, Alabama         24461\n",
       "25                               Atmore city, Alabama         10067\n",
       "26                              Attalla city, Alabama          5934\n",
       "27                               Auburn city, Alabama         60318\n",
       "28                         Autaugaville town, Alabama          1023\n",
       "29                                 Avon town, Alabama           465\n",
       "30                                  Axis CDP, Alabama           992\n",
       "...                                               ...           ...\n",
       "29545               San Isidro comunidad, Puerto Rico          6685\n",
       "29546                 San José comunidad, Puerto Rico          2599\n",
       "29547               San Juan zona urbana, Puerto Rico        350402\n",
       "29548            San Lorenzo zona urbana, Puerto Rico          8084\n",
       "29549          San Sebastián zona urbana, Puerto Rico          9469\n",
       "29550            Santa Bárbara comunidad, Puerto Rico          5261\n",
       "29551              Santa Clara comunidad, Puerto Rico          1097\n",
       "29552           Santa Isabel zona urbana, Puerto Rico          6378\n",
       "29553            Santo Domingo comunidad, Puerto Rico          3753\n",
       "29554                   Stella comunidad, Puerto Rico          1212\n",
       "29555                   Suárez comunidad, Puerto Rico          2165\n",
       "29556                 Tallaboa comunidad, Puerto Rico           807\n",
       "29557            Tallaboa Alta comunidad, Puerto Rico          1755\n",
       "29558                    Tibes comunidad, Puerto Rico           632\n",
       "29559                Tiburones comunidad, Puerto Rico          1925\n",
       "29560  Tierras Nuevas Poniente comunidad, Puerto Rico          3731\n",
       "29561               Toa Alta zona urbana, Puerto Rico          3636\n",
       "29562               Toa Baja zona urbana, Puerto Rico          1489\n",
       "29563          Trujillo Alto zona urbana, Puerto Rico         46226\n",
       "29564                 Utuado zona urbana, Puerto Rico          7706\n",
       "29565                    Vayas comunidad, Puerto Rico           591\n",
       "29566                  Vázquez comunidad, Puerto Rico          1947\n",
       "29567              Vega Alta zona urbana, Puerto Rico          9454\n",
       "29568              Vega Baja zona urbana, Puerto Rico         24751\n",
       "29569                  Vieques comunidad, Puerto Rico          2614\n",
       "29570                Vieques zona urbana, Puerto Rico          2481\n",
       "29571               Villalba zona urbana, Puerto Rico          3528\n",
       "29572                Yabucoa zona urbana, Puerto Rico          6720\n",
       "29573                  Yauco zona urbana, Puerto Rico         17627\n",
       "29574                   Yaurel comunidad, Puerto Rico          1255\n",
       "\n",
       "[29341 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_16_df.HC01_EST_VC01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0869565217391304"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census\n",
    "qtqt=s_census_.head().copy()\n",
    "qtqt=qtqt.set_index(qtqt['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "qtqt.columns=['1970','1980','1990','2000','2010']\n",
    "# 2011\n",
    "t_q_=s_acs_2011_.head().copy()\n",
    "t_q_=t_q_.set_index(t_q_['GEO.display-label'])\n",
    "t_q_.columns=['no','2011']\n",
    "t_q_=t_q_['2011']\n",
    "# 2015\n",
    "tqtq=s_acs_2015_.head().copy()\n",
    "tqtq=tqtq.set_index(tqtq['GEO.display-label'])\n",
    "tqtq.columns=['no','2015']\n",
    "tqtq=tqtq['2015']\n",
    "pd.concat([qtqt,t_q_,tqtq],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqtq=s_acs_2015_.head().copy()\n",
    "tqtq=tqtq.set_index(tqtq['GEO.display-label'])#['HC01_EST_VC01']\n",
    "tqtq.columns=['no','2015']\n",
    "tqtq=tqtq['2015']\n",
    "tqtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_q_=s_acs_2011_.head().copy()\n",
    "t_q_=t_q_.set_index(t_q_['GEO.display-label'])#['HC01_EST_VC01']\n",
    "t_q_.columns=['no','2011']\n",
    "t_q_=t_q_['2011']\n",
    "t_q_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtqt=s_census_.head().copy()\n",
    "qtqt=qtqt.set_index(qtqt['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "qtqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qtqt=qtqt.join(t_q_, how='outer')\n",
    "# qtqt=qtqt.join(tqtq, how='outer')\n",
    "pd.concat([qtqt,t_q_,tqtq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_acs_20l7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measureable_places.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in enumerate(census_place_populations['GEO.display-label']):\n",
    "#     if j not in acs_2011_place_populations['GEO.display-label']:\n",
    "#         print(i,j)\n",
    "len(census_place_populations['GEO.display-label']), len(acs_2011_place_populations['GEO.display-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[]\n",
    "for i,j in enumerate(acs_2012_place_populations['GEO.display-label']):\n",
    "    if j not in q:\n",
    "        q.append(j)\n",
    "    else:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=[]\n",
    "for i,j in enumerate(census_place_populations['GEO.display-label']):\n",
    "    if j not in o:\n",
    "        o.append(j)\n",
    "    else:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o[:116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(census_place_populations['GEO.display-label']),len(list(census_place_populations['GEO.display-label'])),len(set(census_place_populations['GEO.display-label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_place_populations#['GEO.display-label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_places),len(base_places),len(measureable_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''convert train years to reflect Places only seen in base_places'''\n",
    "# drop Census Places not found in ACS 2016 and ACS 2017 \n",
    "census_place_populations = pop_place_70_10_.loc[pop_place_70_10_['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2011_place_populations = load_acs_20l1.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2012_place_populations = load_acs_20l2.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2013_place_populations = load_acs_20l3.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2014_place_populations = load_acs_20l4.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2015_place_populations = load_acs_20l5.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "\n",
    "'''build Train dataframe'''\n",
    "\n",
    "\n",
    "# a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "base_places[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population by Place Census 1970-2010 measurements\n",
    "load_pop_by_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(base_places),len(set(base_places)\n",
    "load_acs_20l1['GEO.display-label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census70_10places = [place for place in ]\n",
    "tq=['Abanda CDP, Alabama','Abbeville city, Alabama','Adamsville city, Alabama','Abanda CDP, Alabama','Abbeville city, Alabama','Adamsville city, Alabama']\n",
    "len(tq),len((set(tq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['PLACE','STATE', 'AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "\n",
    "# shrink pop_by_place DataFrame to contain only information directly tied to end goal\n",
    "pop_by_place = load_pop_by_place[place_cols_of_interest]\n",
    "pop_by_place\n",
    "# drop rows without at lest one (1) Census measurement \n",
    "# len(pop_by_place.dropna(axis=0,thresh=3)),len(pop_by_place)\n",
    "# sum(no_name12.AV0AA2010.isnull()),len(no_name12.AV0AA2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.columns#PLACEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pop_by_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify Places in Census 1970-2010 DataFrame\n",
    "c70c10places = [load_pop_by_place[['PLACE','STATE']].values[_][0] +', '+ load_pop_by_place[['PLACE','STATE']].values[_][1] for _ in range(len(load_pop_by_place))]\n",
    "# identify Places measured in 2011 ACS\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label']]\n",
    "# identify Places measured in 2012 ACS\n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS\n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS\n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_place_70_10_.GEO_display_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_2011_place_populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
