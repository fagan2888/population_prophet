{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# total population by place\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc=pop_by_place.iloc[[18672]]\n",
    "nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place_ = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "pop_by_place_#.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - forget places with only 1 measurement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def population_by_place(years=20):\n",
    "# total population by place (1970 to 2010)\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "\"\"\"\n",
    "- generate DataFrame of population:\n",
    "    > from 1970 to 2010\n",
    "    > by unique place (use NHGISCODE as Id)\n",
    "- drop \n",
    "    > counties with less than 2 measurements\n",
    "        > can only predict counties which have been measured 2+ times \n",
    "- extract list of counties\n",
    "    > each as a DataFrame ready for prediction \n",
    "    > column0='ds' , column1='y'\n",
    "\"\"\"\n",
    "\n",
    "# df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "# generate list of remaining NHGISCODE codes \n",
    "codes_of_measureable_unique_places = [code for code in measureable_unique_places.NHGISCODE]\n",
    "# drop NHGISCODE column (25103 rows × 4 columns)\n",
    "measureable_unique_places = measureable_unique_places.drop('NHGISCODE',axis=1)\n",
    "\n",
    "# list of str column names as years (for conversion to datetime)\n",
    "year_only_columns = [i[5:] for i in measureable_unique_places.columns]\n",
    "# convert year_only_columns to DatetimeIndex of Timestamps\n",
    "dt_columns = pd.to_datetime(arg=year_only_columns)\n",
    "\n",
    "# convert dt_columns into dataframe \n",
    "datetime_df = pd.DataFrame(dt_columns).T\n",
    "# w/ columns, so concatable with measureable_unique_counties\n",
    "datetime_df.columns = measureable_unique_places.columns\n",
    "\n",
    "# generate list of remaining places (each as pd.Series)\n",
    "dfs_of_measureable_unique_places = [measureable_unique_places.iloc[place] for place in range(len(measureable_unique_places))]\n",
    "\n",
    "# add datetime_df to each dataframe as first row\n",
    "prophet_places = [pd.concat((datetime_df,pd.DataFrame(place).T),axis=0) for place in dfs_of_measureable_unique_places]\n",
    "# then transpose to 2 rows x 23 columns \n",
    "prophet_almost_ready_places = [place.T for place in prophet_places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fbprophet \n",
    "\n",
    "def population_by_place(years=20,n_places=1000,changepoint_prior=0.15,indicate=False):\n",
    "    # total population by place (1970 to 2010)\n",
    "    pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "    \"\"\"\n",
    "    inputs) \n",
    "    >> years\n",
    "        > number of years to forecast\n",
    "    >> places\n",
    "        > number of places to forecast +1 \n",
    "            >> e.g. 99 = first 100 places (max==25102)\n",
    "    >> changepoint_prior\n",
    "        > set changepoint_prior_scale for prophet model\n",
    "    >> indicate\n",
    "        > default False\n",
    "        > if True, print number of place forecasted after each forecast\n",
    "    >> time\n",
    "        > default False\n",
    "        > if True, prints time the function took to run right before returning output\n",
    "    \n",
    "    function: \n",
    "    >> generate DataFrame of population:\n",
    "        > from 1970 to 2010\n",
    "        > by unique place (use NHGISCODE as Id)\n",
    "    >> drop \n",
    "        > places with less than 2 measurements\n",
    "            > can only predict places which have been measured 2+ times \n",
    "    >> extract list of places\n",
    "        > each as a DataFrame ready for prediction \n",
    "        > column0='ds' , column1='y'\n",
    "    >> make and fit prophet model on each place\n",
    "    >> return prophet model's predictions\n",
    "        > of each place\n",
    "        > for {years} years\n",
    "    \"\"\"\n",
    "\n",
    "    # df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "    unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "    # drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "    measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "    # convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "    measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "    # generate list of remaining NHGISCODE codes \n",
    "    codes_of_measureable_unique_places = [code for code in measureable_unique_places.NHGISCODE]\n",
    "    # drop NHGISCODE column (25103 rows × 4 columns)\n",
    "    measureable_unique_places = measureable_unique_places.drop('NHGISCODE',axis=1)\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    year_only_columns = [i[5:] for i in measureable_unique_places.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=year_only_columns)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = measureable_unique_places.columns\n",
    "\n",
    "    # generate list of remaining places (each as pd.Series)\n",
    "    dfs_of_measureable_unique_places = [measureable_unique_places.iloc[place] for place in range(len(measureable_unique_places))]\n",
    "\n",
    "    # add datetime_df to each dataframe as first row\n",
    "    prophet_places = [pd.concat((datetime_df,pd.DataFrame(place).T),axis=0) for place in dfs_of_measureable_unique_places]\n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_places = [place.T for place in prophet_places]\n",
    "\n",
    "    # set collection of prophets \n",
    "    prophet_by_place = []\n",
    "\n",
    "    # run prophet model on first 1000 places\n",
    "    for place in range(len(prophet_almost_ready_places[:n_places])):\n",
    "        # make the prophet model\n",
    "        place_prophet = fbprophet.Prophet(changepoint_prior_scale=changepoint_prior)\n",
    "        # identify county \n",
    "        a = prophet_almost_ready_places[place]\n",
    "        # rename place df's columns to agree with prophet formatting\n",
    "        a.columns = ['ds','y']\n",
    "        # fit place on prophet model \n",
    "        b = place_prophet.fit(a)\n",
    "        # make a future dataframe for 20 years\n",
    "        place_forecast = place_prophet.make_future_dataframe( periods=1*years, freq='Y' )\n",
    "        # establish predictions\n",
    "        place_forecast = place_prophet.predict(place_forecast)\n",
    "        # add to collection \n",
    "        prophet_by_place.append(place_forecast)\n",
    "        # did we ask for indication (hint: do this if calculating for > 1000 places unless you enjoy anxiety)\n",
    "        if indicate==True:\n",
    "            # let us know the count\n",
    "            print(place)\n",
    "        \n",
    "    # return forecasts\n",
    "    return prophet_by_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_place = population_by_place(indicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_place[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "sample=[]\n",
    "for code in measureable_unique_places[:10].NHGISCODE:\n",
    "    a=pop_by_place.loc[pop_by_place['NHGISCODE'] == code]\n",
    "    sample.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample)):\n",
    "    o=sample[i][['PLACE','STATE']]\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abbeville city  Alabama; \n",
    "2,721 2586.673761\n",
    "2,699 2576.742940\n",
    "2,684 2538.960959\n",
    "2,654 2500.844134\n",
    "2,646 2462.401057\n",
    "2,627 2452.470236\n",
    "2,594 2414.688255\n",
    "\"\"\"\n",
    "print(by_place[0][['yhat']][5:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "\n",
    "unique_places = pop_by_place.copy()[['PLACE','STATE','AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "_measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "_measureable_unique_places = _measureable_unique_places.fillna(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_measureable_unique_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.NHGISCODE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.copy()[['PLACE','STATE']].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Abbeville city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "all_places = [place for place in pop_by_place.PLACE]\n",
    " \n",
    "\n",
    "def forecast_this(place,n_years=20):\n",
    "    a=0\n",
    "    for p in all_places:\n",
    "        if p == place:\n",
    "            a=pop_by_place.loc[pop_by_place['PLACE'] == place]\n",
    "        elif p == place + ' city':\n",
    "            a=pop_by_place.loc[pop_by_place['PLACE'] == place + ' city']\n",
    "    print(a.columns)\n",
    "#         # make the prophet model\n",
    "#         place_prophet = fbprophet.Prophet(changepoint_prior_scale=changepoint_prior)\n",
    "#         # identify county \n",
    "#         a = prophet_almost_ready_places[place]\n",
    "#         # rename place df's columns to agree with prophet formatting\n",
    "#         a.columns = ['ds','y']\n",
    "#         # fit place on prophet model \n",
    "#         b = place_prophet.fit(a)\n",
    "#         # make a future dataframe for 20 years\n",
    "#         place_forecast = place_prophet.make_future_dataframe( periods=1*n_years, freq='Y' )\n",
    "#         # establish predictions\n",
    "#         place_forecast = place_prophet.predict(place_forecast)\n",
    "#         # add to collection \n",
    "#         prophet_by_place.append(place_forecast)\n",
    "forecast_this('San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_places:\n",
    "    if i == 'Abbeville city':\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a17 = actual7.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a16 = actual6.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a15 = actual5.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a14 = actual4.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a13 = actual3.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a12 = actual2.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a11 = actual1.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "\n",
    "# len(a17),len(a16),len(a15),len(a14),len(a13),len(a12),len(a11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a17places = [place for place in a17['GEO.display-label']]\n",
    "a16places = [place for place in a16['GEO.display-label']]\n",
    "a15places = [place for place in a15['GEO.display-label']]\n",
    "a14places = [place for place in a14['GEO.display-label']]\n",
    "a13places = [place for place in a13['GEO.display-label']]\n",
    "a12places = [place for place in a12['GEO.display-label']]\n",
    "a11places = [place for place in a11['GEO.display-label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_places=[]\n",
    "for place in a17places:\n",
    "    if place in a16places:\n",
    "        if place in a15places:\n",
    "            if place in a14places:\n",
    "                if place in a13places:\n",
    "                    if place in a12places:\n",
    "                        if place in a11places:\n",
    "                            combo_places.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combo_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- convert dataframes to only contain shared values\n",
    "\"\"\"\n",
    "a17=a17.loc[a17['GEO.display-label'].isin(combo_places)]\n",
    "a16=a16.loc[a16['GEO.display-label'].isin(combo_places)]\n",
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "a14=a14.loc[a14['GEO.display-label'].isin(combo_places)]\n",
    "a13=a13.loc[a13['GEO.display-label'].isin(combo_places)]\n",
    "a12=a12.loc[a12['GEO.display-label'].isin(combo_places)]\n",
    "a11=a11.loc[a11['GEO.display-label'].isin(combo_places)]\n",
    "# len(a17),len(a16),len(a15),len(a14),len(a13),len(a12),len(a11)\n",
    "a17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "base=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# select columns of measurement\n",
    "get_base=base.copy()[['PLACE','STATE','AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "_measureable_base = get_base.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "_measureable_base = _measureable_base.fillna(0)\n",
    "_measureable_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_measureable_unique_places),len(_measureable_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "a14=a14.loc[a14['GEO.display-label'].isin(combo_places)]\n",
    "a13=a13.loc[a13['GEO.display-label'].isin(combo_places)]\n",
    "a12=a12.loc[a12['GEO.display-label'].isin(combo_places)]\n",
    "a11=a11.loc[a11['GEO.display-label'].isin(combo_places)]\n",
    "a15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split GEO.display-label into list [place, state]\n",
    "places_11_17 = [label.split(',') for label in a15['GEO.display-label']]\n",
    "# make DataFrame of Place, State for compairson to _measureable_unique_places and _measureable_base\n",
    "df_places_11_17 = pd.DataFrame(places_11_17,columns=['PLACE','STATE'])\n",
    "# add population estimates \n",
    "eleven_seventeen = [df_places_11_17,a11.HC01_EST_VC01,a12.HC01_EST_VC01,a13.HC01_EST_VC01,a14.HC01_EST_VC01,a15.HC01_EST_VC01]\n",
    "df_pop_by_place_11_17 = pd.concat(eleven_seventeen,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_by_place_11_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_measureable_unique_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_this(place,state,prior):\n",
    "    # set place state\n",
    "    place_w_state=place+', '+state\n",
    "    \n",
    "    # 1970-2010\n",
    "    sf70to10 = _measureable_unique_places.loc[_measureable_unique_places['PLACE'] == place]\n",
    "    # 2011\n",
    "    sf11 = a11.loc[a11['GEO.display-label'] == place_w_state]\n",
    "    # 2012\n",
    "    sf12 = a12.loc[a12['GEO.display-label'] == place_w_state]\n",
    "    # 2013\n",
    "    sf13 = a13.loc[a13['GEO.display-label'] == place_w_state]\n",
    "    # 2014\n",
    "    sf14 = a14.loc[a14['GEO.display-label'] == place_w_state]\n",
    "    # 2015\n",
    "    sf15 = a15.loc[a15['GEO.display-label'] == place_w_state]\n",
    "\n",
    "    # combine all population measurements \n",
    "    sf_pops = sf70to10,sf11.HC01_EST_VC01,sf12.HC01_EST_VC01,sf13.HC01_EST_VC01,sf14.HC01_EST_VC01,sf15.HC01_EST_VC01\n",
    "\n",
    "    # combine dataframes\n",
    "    combo_sf = pd.concat(sf_pops,axis=1,sort=True)\n",
    "    # fill lowest row NaN values with value from above rows (if exists)\n",
    "    combo_sf = combo_sf.fillna(method='ffill')\n",
    "    # drop rows with NaN values\n",
    "    combo_sf = combo_sf.dropna(axis=0,thresh=7)\n",
    "\n",
    "    # drop Place and State columns\n",
    "    combo_sf = combo_sf.drop( ['PLACE' , 'STATE'] ,axis=1 )\n",
    "\n",
    "    # change column names to years 1970-2010 & 2011-2015\n",
    "    combo_sf.columns = ['1970', '1980', '1990', '2000', '2010', '2011','2012','2013','2014','2015']\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    pre_dt_cols = [i for i in combo_sf.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=pre_dt_cols)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = combo_sf.columns\n",
    "\n",
    "    # add datetime_df as first row\n",
    "    prophet_place = pd.concat((datetime_df,combo_sf),axis=0) \n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_place = prophet_place.T\n",
    "\n",
    "    # convert df to short \n",
    "    a = prophet_almost_ready_place\n",
    "\n",
    "    # make the prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=prior)\n",
    "    \n",
    "    print(a)\n",
    "    if len(a.columns) == 3:\n",
    "        a = a.drop(a.columns[2],axis=1)\n",
    "        print(a)\n",
    "\n",
    "    # rename place df's columns to agree with prophet formatting\n",
    "    a.columns = ['ds','y']\n",
    "    print(a)\n",
    "    # fit place on prophet model \n",
    "    b = place_prophet.fit(a)\n",
    "\n",
    "    # make a future dataframe for 20 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=1*20, freq='Y' )\n",
    "    # establish predictions\n",
    "    sf_forecast = place_prophet.predict(place_forecast)\n",
    "    \n",
    "    # output predictions\n",
    "    return [sf_forecast,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "sf = prophet_this(place='Denver city',state='Colorado',prior=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "base_16 = int(sf[1].y[9]) + (int(sf[1].y[9])-int(sf[1].y[8]))\n",
    "base_17 = base_16 + (int(sf[1].y[9])-int(sf[1].y[8]))\n",
    "\n",
    "# predictions 2016 & 2017\n",
    "p_16 = sf[0]['yhat'][11]\n",
    "p_17 = sf[0]['yhat'][12]\n",
    "\n",
    "# actual 2016 & 2017\n",
    "sf16 = a16.loc[a16['GEO.display-label'] == 'Denver city, Colorado']\n",
    "sf17 = a17.loc[a17['GEO.display-label'] == 'Denver city, Colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_16 = int(sf16.HC01_EST_VC01)\n",
    "act_17 = int(sf17.HC01_EST_VC01)\n",
    "# actual - base 2016\n",
    "ab16 = act_16-base_16\n",
    "# pred - base 2016\n",
    "ap16 = act_16-p_16\n",
    "# pred - base 2017\n",
    "ab17 = act_17-base_17\n",
    "# pred - base 2017\n",
    "ap17 = act_17-p_17\n",
    "\n",
    "print(f\"{abs(ab16)}\\n{abs(int(ap16))}\\n\\n{abs(ab17)}\\n{abs(int(ap17))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "San Francisco city, California\n",
    "sf = prophet_this(place='San Francisco city',state='California')\n",
    "changepoint_prior_scale=0.05\n",
    "-2172 27830\n",
    "-66162 -24158\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Bentonville city, Arkansas\n",
    "sf = prophet_this(place='Bentonville city',state='Arkansas',prior=0.05)\n",
    "335 3508\n",
    "-9938 -5722\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "New York city, New York\n",
    "sf = prophet_this(place='New York city',state='New York',prior=0.15)\n",
    "-36636 -66472\n",
    "-441471 -429353\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "New Orleans city, New Orleans\n",
    "sf = prophet_this(place='New Orleans city',state='Louisiana',prior=0.15)\n",
    "-2083 44438\n",
    "-71863 -13624\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# san francisco forecast population through 2017\n",
    "sf[0][['ds','yhat']][:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf16.HC01_EST_VC01.values[0],sf17.HC01_EST_VC01.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_16 = 42499\n",
    "r_17 = 34022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_16-r_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_16-r_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_17-r_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_17-r_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_11 = a11.loc[a11['GEO.display-label'] == 'Pleasanton city, California']\n",
    "pl_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[]\n",
    "def prophet_this(place,state):\n",
    "    # set place state\n",
    "    place_w_state=place+', '+state\n",
    "    \n",
    "    # 1970-2010\n",
    "    sf70to10 = _measureable_unique_places.loc[_measureable_unique_places['PLACE'] == place]\n",
    "    # 2011\n",
    "    sf11 = a11.loc[a11['GEO.display-label'] == place_w_state]\n",
    "    # 2012\n",
    "    sf12 = a12.loc[a12['GEO.display-label'] == place_w_state]\n",
    "    # 2013\n",
    "    sf13 = a13.loc[a13['GEO.display-label'] == place_w_state]\n",
    "    # 2014\n",
    "    sf14 = a14.loc[a14['GEO.display-label'] == place_w_state]\n",
    "    # 2015\n",
    "    sf15 = a15.loc[a15['GEO.display-label'] == place_w_state]\n",
    "\n",
    "    # combine all population measurements \n",
    "    sf_pops = sf70to10,sf11.HC01_EST_VC01,sf12.HC01_EST_VC01,sf13.HC01_EST_VC01,sf14.HC01_EST_VC01,sf15.HC01_EST_VC01\n",
    "\n",
    "    # combine dataframes\n",
    "    combo_sf = pd.concat(sf_pops,axis=1,sort=True)\n",
    "    # fill lowest row NaN values with value from above rows (if exists)\n",
    "    combo_sf = combo_sf.fillna(method='ffill')\n",
    "    # drop rows with NaN values\n",
    "    combo_sf = combo_sf.dropna(axis=0,thresh=7)\n",
    "\n",
    "    # drop Place and State columns\n",
    "    combo_sf = combo_sf.drop(['PLACE', 'STATE'],axis=1)\n",
    "\n",
    "    # change column names to years 1970-2010 & 2011-2015\n",
    "    combo_sf.columns = ['1970', '1980', '1990', '2000', '2010', '2011','2012','2013','2014','2015']\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    pre_dt_cols = [i for i in combo_sf.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=pre_dt_cols)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = combo_sf.columns\n",
    "\n",
    "    # add datetime_df as first row\n",
    "    prophet_place = pd.concat((datetime_df,combo_sf),axis=0) \n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_place = prophet_place.T\n",
    "\n",
    "    # convert df to short \n",
    "    a = prophet_almost_ready_place\n",
    "\n",
    "    # make the prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "\n",
    "    # rename place df's columns to agree with prophet formatting\n",
    "    a.columns = ['ds','y']\n",
    "    # fit place on prophet model \n",
    "    b = place_prophet.fit(a)\n",
    "\n",
    "    # make a future dataframe for 20 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=1*20, freq='Y' )\n",
    "    # establish predictions\n",
    "    sf_forecast = place_prophet.predict(place_forecast)\n",
    "\n",
    "    # benchmark 2016 & 2017\n",
    "    base_16=int(prophet_almost_ready_place.y[9])+(int(prophet_almost_ready_place.y[9])-int(prophet_almost_ready_place.y[8]))\n",
    "    base_17=base_16+(int(prophet_almost_ready_place.y[9])-int(prophet_almost_ready_place.y[8]))\n",
    "    \n",
    "    # predictions 2016 & 2017\n",
    "    p_16 = sf_forecast['yhat'][11]\n",
    "    p_17 = sf_forecast['yhat'][12]\n",
    "    \n",
    "    # actual 2016 & 2017\n",
    "    sf16 = a16.loc[a16['GEO.display-label'] == place_w_state]\n",
    "    sf17 = a17.loc[a17['GEO.display-label'] == place_w_state]\n",
    "    \n",
    "    # output base 2016 vs pred 2016 ; base 2017 vs real 2017\n",
    "    return [[base_16-r_16,p_16-r_16],[base_17-r_17,p_17-r_17]]\n",
    "\n",
    "# run\n",
    "aaa=prophet_this(place='Bentonville city',state='Arkansas')\n",
    "# bbb=prophet_this(place='Fayetteville city',state='Arkansas')\n",
    "out.append(aaa)\n",
    "# out.append(bbb)\n",
    "# prophet \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# def mega_place_df():\n",
    "\"\"\"\n",
    ">> takes in \n",
    "    > Census 1970-2010 dataframe (1 df)\n",
    "        >> total population by Place measurements\n",
    "    > American Community Survey (ACS) 2011-2017 dataframes (7 dfs)\n",
    "        >> total population (age & sex) by Place \n",
    "        \n",
    ">> forges DataFrame of places that have \n",
    "    > at least one (1) recording for Census years 1970-2010\n",
    "    > at least one (1) recording for ACS years 2011-2015\n",
    "\n",
    ">> test our model v. base on\n",
    "    > random sample 100 Places\n",
    "    > random sample 100 Places from bottom half population size\n",
    "    > random sample 100 Places from top half population size\n",
    "\"\"\"\n",
    "\n",
    "'''load Train data'''\n",
    "# population by Place Census 1970-2010 measurements\n",
    "load_census_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "\n",
    "'''load Test data'''\n",
    "# population by Place ACS 2016\n",
    "load_acs_20l6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2017\n",
    "load_acs_20l7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''find common places across Census and each train ACS'''\n",
    "# identify Places measured in 2011 ACS [0 == 'Geography']\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2012 ACS \n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS \n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS \n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]\n",
    "\n",
    "# cross 2011-2015, keep coexisting Places\n",
    "train_places = [place for place in acs11places if place in acs12places and acs13places and acs14places and acs15places]\n",
    "\n",
    "\n",
    "'''find common places across 2016 & 2017 (test ACSs)'''\n",
    "# identify Places measured in 2016 ACS (29575) [0 == 'Geography']\n",
    "acs16places = [place for place in load_acs_20l6['GEO.display-label'][1:]]\n",
    "# identify Places measured in 2017 ACS (29577)\n",
    "acs17places = [place for place in load_acs_20l7['GEO.display-label']]\n",
    "\n",
    "# cross 2017 Places w/ 2016 Places, keep coexisting Places (29551)\n",
    "base_places = [place for place in acs17places if place in acs16places]\n",
    "\n",
    "\n",
    "'''find common Places across the Places our model will train on {train_places} \n",
    "    and the Places our model can predict on {base_places}'''\n",
    "# identify Places we can compare our predictions with\n",
    "measureable_places = [place for place in train_places if place in base_places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''clean Census 1970-2010 df (Train)'''\n",
    "# identify columns needed to make GEO.display-label column (so can pair with ACS DataFrames) \n",
    "for_geo_displays = ['PLACE','STATE']\n",
    "# pull those columns \n",
    "to_geo_displays = load_census_place[for_geo_displays]\n",
    "\n",
    "# mold PLACE column into list with Place formatted as is in GEO.display-label\n",
    "places_70_10 = [place + ', ' for place in to_geo_displays.PLACE]\n",
    "\n",
    "# list paired State for each Place\n",
    "states_70_10 = [state for state in to_geo_displays.STATE]\n",
    "\n",
    "# merge places_70_10 and states_70_10 into list formatted as GEO.display-label column\n",
    "GEO_display_label = [ places_70_10[i] + states_70_10[i] for i in range(len(places_70_10))]\n",
    "\n",
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "# set base dataframe using Census (1970-2010) measurements \n",
    "pop_place_70_10_ = load_census_place[place_cols_of_interest]\n",
    "\n",
    "# add GEO.display-label column from GEO_display_label list\n",
    "pop_place_70_10_['GEO.display-label'] = GEO_display_label\n",
    "\n",
    "\n",
    "'''clean American Community Survey (ACS) 2011-2015 dataframes (Train)'''\n",
    "# ID columns we will be using\n",
    "columns = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "# convert 2011\n",
    "acs_20l1 = load_acs_20l1[columns]\n",
    "# convert 2012\n",
    "acs_20l2 = load_acs_20l2[columns]\n",
    "# convert 2013\n",
    "acs_20l3 = load_acs_20l3[columns]\n",
    "# convert 2014\n",
    "acs_20l4 = load_acs_20l4[columns]\n",
    "# convert 2015\n",
    "acs_20l5 = load_acs_20l5[columns]\n",
    "\n",
    "\n",
    "'''convert Train years to reflect Places only seen in measureable_places'''\n",
    "# drop Census Places not ideal for measurement (29346)\n",
    "census_place_populations = pop_place_70_10_.loc[pop_place_70_10_['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2011_place_populations = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341)\n",
    "acs_2012_place_populations = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2013_place_populations = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2014_place_populations = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(measureable_places)]\n",
    "# drop 2011 ACS Places not ideal for measurement (29341) \n",
    "acs_2015_place_populations = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "\n",
    "'''clean ACS 2016 & 2017 dataframes (Test)\n",
    "    take a sample of 100 Places to score our model'''\n",
    "# identify 2016/2017 columns of interest (to measure against)\n",
    "test_col_of_i = ['GEO.display-label', 'HC01_EST_VC01']\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_16_ = load_acs_20l6[test_col_of_i]\n",
    "# realize ACS 2016 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_16_df = testd_16_.loc[testd_16_['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "# shrink ACS 2017 df to columns to measure against only \n",
    "testd_17_ = load_acs_20l7[test_col_of_i]\n",
    "# realize ACS 2017 combined measureable_places DataFrame (Baseline) dataframe \n",
    "test_17_df = testd_17_.loc[testd_17_['GEO.display-label'].isin(measureable_places)]\n",
    "\n",
    "# sample Baseline data for Places to evaluate model \n",
    "sample_one_hunnit = test_17_df.sample(100)\n",
    "# list Places for conversion of other Datas\n",
    "sample_places = [place for place in sample_one_hunnit['GEO.display-label']]\n",
    "\n",
    "\n",
    "'''adjust Train dataframes to sampled Places'''\n",
    "# shrink Census DataFrame to sampled Places\n",
    "_s_census_ = census_place_populations.loc[census_place_populations['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2011 ACS df to sampled Places \n",
    "_s_acs_2011_ = acs_20l1.loc[acs_20l1['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2012 ACS DataFrame to sampled Places \n",
    "_s_acs_2012_ = acs_20l2.loc[acs_20l2['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2013 ACS df to Places in sample  \n",
    "_s_acs_2013_ = acs_20l3.loc[acs_20l3['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2014 ACS DataFrame to sampled Places \n",
    "_s_acs_2014_ = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2015 ACS df to sampled Places \n",
    "_s_acs_2015_ = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(sample_places)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''forge Train DataFrame'''\n",
    "# set Census index to Places, and forget Place column \n",
    "s_census_ = _s_census_.copy().set_index(_s_census_['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# rename Census columns to years for later datetime conversion\n",
    "s_census_.columns = ['1970','1980','1990','2000','2010']\n",
    "\n",
    "# set 2011 index to Places \n",
    "s_acs_2011_ = _s_acs_2011_.copy().set_index(_s_acs_2011_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2011_.columns = ['no','2011']\n",
    "s_acs_2011_ = s_acs_2011_['2011']\n",
    "\n",
    "# set 2012 index to Places \n",
    "s_acs_2012_ = _s_acs_2012_.copy().set_index(_s_acs_2012_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2012_.columns = ['no','2012']\n",
    "s_acs_2012_ = s_acs_2012_['2012']\n",
    "\n",
    "# set 2013 index to Places \n",
    "s_acs_2013_ = _s_acs_2013_.copy().set_index(_s_acs_2013_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2013_.columns = ['no','2013']\n",
    "s_acs_2013_ = s_acs_2013_['2013']\n",
    "\n",
    "# set 2014 index to Places \n",
    "s_acs_2014_ = _s_acs_2014_.copy().set_index(_s_acs_2014_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2014_.columns = ['no','2014']\n",
    "s_acs_2014_ = s_acs_2014_['2014']\n",
    "\n",
    "# set 2015 index to Places \n",
    "s_acs_2015_ = _s_acs_2015_.copy().set_index(_s_acs_2015_['GEO.display-label'])\n",
    "# rename Census columns to years for later datetime conversion & forget Place column \n",
    "s_acs_2015_.columns = ['no','2015']\n",
    "s_acs_2015_ = s_acs_2015_['2015']\n",
    "\n",
    "# forge Train DataFrame and convert NaN values to 0 (assumes population not measured is 0) \n",
    "train_df = pd.concat([s_census_,s_acs_2011_,s_acs_2012_,s_acs_2013_,s_acs_2014_,s_acs_2015_],axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''forecast 2016 and 2017 populations using model for each sample Place'''\n",
    "# set out route for forecast tables\n",
    "out = []\n",
    "# set out route for 2016 & 2017 Train predictions\n",
    "train_preds = []\n",
    "# make DataFrame of column values as datetime\n",
    "datetimes = pd.DataFrame(data=pd.to_datetime(pd.Series(data=train_df.columns)))\n",
    "# go though each place in train_df\n",
    "for i in range(len(train_df)-95):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[i]\n",
    "    # add datetime values to DataFrame\n",
    "    df = pd.concat([df.reset_index(),datetimes],axis=1)\n",
    "    # use fbprophet to make Prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "    # rename Place df's columns to agree with prophet formatting\n",
    "    df.columns = ['drop','y','ds']\n",
    "    # adjust df ; forget index column (drop)\n",
    "    df = df[['ds','y']]\n",
    "    # fit place on prophet model \n",
    "    place_prophet.fit(df)\n",
    "    # make a future dataframe for 2016 & 2017 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=3, freq='Y' )\n",
    "    # establish predictions\n",
    "    forecast = place_prophet.predict(place_forecast)\n",
    "    # tag and bag (forecast table)\n",
    "    out.append(forecast)\n",
    "    # store 2016 and 2017 predictions\n",
    "    train_preds.append(forecast[-2:].yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make Baseline predictions of 2016 and 2017 population on sample Places'''\n",
    "# set out route\n",
    "baseline_preds = []\n",
    "# go though each place in train_df\n",
    "for j in range(len(train_df)):\n",
    "    # extract DataFrame for that place\n",
    "    df = train_df.iloc[j]\n",
    "    # identify 2014 population\n",
    "    prior = int(df['2014'])\n",
    "    # identify 2015 population\n",
    "    past = int(df['2015'])\n",
    "    # calculate change\n",
    "    change = past - prior\n",
    "    # make 2016 prediction \n",
    "    p_16 = past + change\n",
    "    # make 2017 prediction \n",
    "    p_17 = p_16 + change\n",
    "    # pair prediction, tag & bag\n",
    "    baseline_preds.append([p_16,p_17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pull actual measurements for 2016 and 2017 population for each sample Place'''\n",
    "test_df\n",
    "\n",
    "# shrink 2016 ACS DataFrame to sampled Places \n",
    "_s_acs_2016_ = acs_20l4.loc[acs_20l4['GEO.display-label'].isin(sample_places)]\n",
    "# shrink 2017 ACS df to sampled Places \n",
    "_s_acs_2017_ = acs_20l5.loc[acs_20l5['GEO.display-label'].isin(sample_places)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census\n",
    "qtqt=s_census_.head().copy()\n",
    "qtqt=qtqt.set_index(qtqt['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "qtqt.columns=['1970','1980','1990','2000','2010']\n",
    "# 2011\n",
    "t_q_=s_acs_2011_.head().copy()\n",
    "t_q_=t_q_.set_index(t_q_['GEO.display-label'])\n",
    "t_q_.columns=['no','2011']\n",
    "t_q_=t_q_['2011']\n",
    "# 2015\n",
    "tqtq=s_acs_2015_.head().copy()\n",
    "tqtq=tqtq.set_index(tqtq['GEO.display-label'])\n",
    "tqtq.columns=['no','2015']\n",
    "tqtq=tqtq['2015']\n",
    "pd.concat([qtqt,t_q_,tqtq],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqtq=s_acs_2015_.head().copy()\n",
    "tqtq=tqtq.set_index(tqtq['GEO.display-label'])#['HC01_EST_VC01']\n",
    "tqtq.columns=['no','2015']\n",
    "tqtq=tqtq['2015']\n",
    "tqtq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_q_=s_acs_2011_.head().copy()\n",
    "t_q_=t_q_.set_index(t_q_['GEO.display-label'])#['HC01_EST_VC01']\n",
    "t_q_.columns=['no','2011']\n",
    "t_q_=t_q_['2011']\n",
    "t_q_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtqt=s_census_.head().copy()\n",
    "qtqt=qtqt.set_index(qtqt['GEO.display-label'])[['AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "qtqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qtqt=qtqt.join(t_q_, how='outer')\n",
    "# qtqt=qtqt.join(tqtq, how='outer')\n",
    "pd.concat([qtqt,t_q_,tqtq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_acs_20l7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measureable_places.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in enumerate(census_place_populations['GEO.display-label']):\n",
    "#     if j not in acs_2011_place_populations['GEO.display-label']:\n",
    "#         print(i,j)\n",
    "len(census_place_populations['GEO.display-label']), len(acs_2011_place_populations['GEO.display-label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[]\n",
    "for i,j in enumerate(acs_2012_place_populations['GEO.display-label']):\n",
    "    if j not in q:\n",
    "        q.append(j)\n",
    "    else:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=[]\n",
    "for i,j in enumerate(census_place_populations['GEO.display-label']):\n",
    "    if j not in o:\n",
    "        o.append(j)\n",
    "    else:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o[:116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(census_place_populations['GEO.display-label']),len(list(census_place_populations['GEO.display-label'])),len(set(census_place_populations['GEO.display-label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_place_populations#['GEO.display-label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_places),len(base_places),len(measureable_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''convert train years to reflect Places only seen in base_places'''\n",
    "# drop Census Places not found in ACS 2016 and ACS 2017 \n",
    "census_place_populations = pop_place_70_10_.loc[pop_place_70_10_['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2011_place_populations = load_acs_20l1.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2012_place_populations = load_acs_20l2.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2013_place_populations = load_acs_20l3.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2014_place_populations = load_acs_20l4.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "# drop 2011 ACS Places not found in ACS 2016 and ACS 2017 \n",
    "acs_2015_place_populations = load_acs_20l5.loc[load_acs_20l1['GEO.display-label'].isin(combo_places)]\n",
    "\n",
    "'''build Train dataframe'''\n",
    "\n",
    "\n",
    "# a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "base_places[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population by Place Census 1970-2010 measurements\n",
    "load_pop_by_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(base_places),len(set(base_places)\n",
    "load_acs_20l1['GEO.display-label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census70_10places = [place for place in ]\n",
    "tq=['Abanda CDP, Alabama','Abbeville city, Alabama','Adamsville city, Alabama','Abanda CDP, Alabama','Abbeville city, Alabama','Adamsville city, Alabama']\n",
    "len(tq),len((set(tq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['PLACE','STATE', 'AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "\n",
    "# shrink pop_by_place DataFrame to contain only information directly tied to end goal\n",
    "pop_by_place = load_pop_by_place[place_cols_of_interest]\n",
    "pop_by_place\n",
    "# drop rows without at lest one (1) Census measurement \n",
    "# len(pop_by_place.dropna(axis=0,thresh=3)),len(pop_by_place)\n",
    "# sum(no_name12.AV0AA2010.isnull()),len(no_name12.AV0AA2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.columns#PLACEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pop_by_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify Places in Census 1970-2010 DataFrame\n",
    "c70c10places = [load_pop_by_place[['PLACE','STATE']].values[_][0] +', '+ load_pop_by_place[['PLACE','STATE']].values[_][1] for _ in range(len(load_pop_by_place))]\n",
    "# identify Places measured in 2011 ACS\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label']]\n",
    "# identify Places measured in 2012 ACS\n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS\n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS\n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_place_70_10_.GEO_display_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_2011_place_populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
