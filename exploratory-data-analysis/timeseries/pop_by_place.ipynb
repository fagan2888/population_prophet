{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# total population by place\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc=pop_by_place.iloc[[18672]]\n",
    "nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place_ = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "pop_by_place_#.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - forget places with only 1 measurement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def population_by_place(years=20):\n",
    "# total population by place (1970 to 2010)\n",
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "\"\"\"\n",
    "- generate DataFrame of population:\n",
    "    > from 1970 to 2010\n",
    "    > by unique place (use NHGISCODE as Id)\n",
    "- drop \n",
    "    > counties with less than 2 measurements\n",
    "        > can only predict counties which have been measured 2+ times \n",
    "- extract list of counties\n",
    "    > each as a DataFrame ready for prediction \n",
    "    > column0='ds' , column1='y'\n",
    "\"\"\"\n",
    "\n",
    "# df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "# generate list of remaining NHGISCODE codes \n",
    "codes_of_measureable_unique_places = [code for code in measureable_unique_places.NHGISCODE]\n",
    "# drop NHGISCODE column (25103 rows × 4 columns)\n",
    "measureable_unique_places = measureable_unique_places.drop('NHGISCODE',axis=1)\n",
    "\n",
    "# list of str column names as years (for conversion to datetime)\n",
    "year_only_columns = [i[5:] for i in measureable_unique_places.columns]\n",
    "# convert year_only_columns to DatetimeIndex of Timestamps\n",
    "dt_columns = pd.to_datetime(arg=year_only_columns)\n",
    "\n",
    "# convert dt_columns into dataframe \n",
    "datetime_df = pd.DataFrame(dt_columns).T\n",
    "# w/ columns, so concatable with measureable_unique_counties\n",
    "datetime_df.columns = measureable_unique_places.columns\n",
    "\n",
    "# generate list of remaining places (each as pd.Series)\n",
    "dfs_of_measureable_unique_places = [measureable_unique_places.iloc[place] for place in range(len(measureable_unique_places))]\n",
    "\n",
    "# add datetime_df to each dataframe as first row\n",
    "prophet_places = [pd.concat((datetime_df,pd.DataFrame(place).T),axis=0) for place in dfs_of_measureable_unique_places]\n",
    "# then transpose to 2 rows x 23 columns \n",
    "prophet_almost_ready_places = [place.T for place in prophet_places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fbprophet \n",
    "\n",
    "def population_by_place(years=20,n_places=1000,changepoint_prior=0.15,indicate=False):\n",
    "    # total population by place (1970 to 2010)\n",
    "    pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "    \"\"\"\n",
    "    inputs) \n",
    "    >> years\n",
    "        > number of years to forecast\n",
    "    >> places\n",
    "        > number of places to forecast +1 \n",
    "            >> e.g. 99 = first 100 places (max==25102)\n",
    "    >> changepoint_prior\n",
    "        > set changepoint_prior_scale for prophet model\n",
    "    >> indicate\n",
    "        > default False\n",
    "        > if True, print number of place forecasted after each forecast\n",
    "    >> time\n",
    "        > default False\n",
    "        > if True, prints time the function took to run right before returning output\n",
    "    \n",
    "    function: \n",
    "    >> generate DataFrame of population:\n",
    "        > from 1970 to 2010\n",
    "        > by unique place (use NHGISCODE as Id)\n",
    "    >> drop \n",
    "        > places with less than 2 measurements\n",
    "            > can only predict places which have been measured 2+ times \n",
    "    >> extract list of places\n",
    "        > each as a DataFrame ready for prediction \n",
    "        > column0='ds' , column1='y'\n",
    "    >> make and fit prophet model on each place\n",
    "    >> return prophet model's predictions\n",
    "        > of each place\n",
    "        > for {years} years\n",
    "    \"\"\"\n",
    "\n",
    "    # df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "    unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "    # drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "    measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "    # convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "    measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "    # generate list of remaining NHGISCODE codes \n",
    "    codes_of_measureable_unique_places = [code for code in measureable_unique_places.NHGISCODE]\n",
    "    # drop NHGISCODE column (25103 rows × 4 columns)\n",
    "    measureable_unique_places = measureable_unique_places.drop('NHGISCODE',axis=1)\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    year_only_columns = [i[5:] for i in measureable_unique_places.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=year_only_columns)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = measureable_unique_places.columns\n",
    "\n",
    "    # generate list of remaining places (each as pd.Series)\n",
    "    dfs_of_measureable_unique_places = [measureable_unique_places.iloc[place] for place in range(len(measureable_unique_places))]\n",
    "\n",
    "    # add datetime_df to each dataframe as first row\n",
    "    prophet_places = [pd.concat((datetime_df,pd.DataFrame(place).T),axis=0) for place in dfs_of_measureable_unique_places]\n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_places = [place.T for place in prophet_places]\n",
    "\n",
    "    # set collection of prophets \n",
    "    prophet_by_place = []\n",
    "\n",
    "    # run prophet model on first 1000 places\n",
    "    for place in range(len(prophet_almost_ready_places[:n_places])):\n",
    "        # make the prophet model\n",
    "        place_prophet = fbprophet.Prophet(changepoint_prior_scale=changepoint_prior)\n",
    "        # identify county \n",
    "        a = prophet_almost_ready_places[place]\n",
    "        # rename place df's columns to agree with prophet formatting\n",
    "        a.columns = ['ds','y']\n",
    "        # fit place on prophet model \n",
    "        b = place_prophet.fit(a)\n",
    "        # make a future dataframe for 20 years\n",
    "        place_forecast = place_prophet.make_future_dataframe( periods=1*years, freq='Y' )\n",
    "        # establish predictions\n",
    "        place_forecast = place_prophet.predict(place_forecast)\n",
    "        # add to collection \n",
    "        prophet_by_place.append(place_forecast)\n",
    "        # did we ask for indication (hint: do this if calculating for > 1000 places unless you enjoy anxiety)\n",
    "        if indicate==True:\n",
    "            # let us know the count\n",
    "            print(place)\n",
    "        \n",
    "    # return forecasts\n",
    "    return prophet_by_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_place = population_by_place(indicate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_place[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df by NHGISCODE with measurements by decade (31436 rows × 5 columns)\n",
    "unique_places = pop_by_place.copy()[['NHGISCODE','AV0AA1970','AV0AA1980','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "measureable_unique_places = measureable_unique_places.fillna(0)\n",
    "\n",
    "sample=[]\n",
    "for code in measureable_unique_places[:10].NHGISCODE:\n",
    "    a=pop_by_place.loc[pop_by_place['NHGISCODE'] == code]\n",
    "    sample.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample)):\n",
    "    o=sample[i][['PLACE','STATE']]\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Abbeville city  Alabama; \n",
    "2,721 2586.673761\n",
    "2,699 2576.742940\n",
    "2,684 2538.960959\n",
    "2,654 2500.844134\n",
    "2,646 2462.401057\n",
    "2,627 2452.470236\n",
    "2,594 2414.688255\n",
    "\"\"\"\n",
    "print(by_place[0][['yhat']][5:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "\n",
    "unique_places = pop_by_place.copy()[['PLACE','STATE','AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "_measureable_unique_places = unique_places.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "_measureable_unique_places = _measureable_unique_places.fillna(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_measureable_unique_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.NHGISCODE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.copy()[['PLACE','STATE']].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Abbeville city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "all_places = [place for place in pop_by_place.PLACE]\n",
    " \n",
    "\n",
    "def forecast_this(place,n_years=20):\n",
    "    a=0\n",
    "    for p in all_places:\n",
    "        if p == place:\n",
    "            a=pop_by_place.loc[pop_by_place['PLACE'] == place]\n",
    "        elif p == place + ' city':\n",
    "            a=pop_by_place.loc[pop_by_place['PLACE'] == place + ' city']\n",
    "    print(a.columns)\n",
    "#         # make the prophet model\n",
    "#         place_prophet = fbprophet.Prophet(changepoint_prior_scale=changepoint_prior)\n",
    "#         # identify county \n",
    "#         a = prophet_almost_ready_places[place]\n",
    "#         # rename place df's columns to agree with prophet formatting\n",
    "#         a.columns = ['ds','y']\n",
    "#         # fit place on prophet model \n",
    "#         b = place_prophet.fit(a)\n",
    "#         # make a future dataframe for 20 years\n",
    "#         place_forecast = place_prophet.make_future_dataframe( periods=1*n_years, freq='Y' )\n",
    "#         # establish predictions\n",
    "#         place_forecast = place_prophet.predict(place_forecast)\n",
    "#         # add to collection \n",
    "#         prophet_by_place.append(place_forecast)\n",
    "forecast_this('San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_places:\n",
    "    if i == 'Abbeville city':\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "actual1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a17 = actual7.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a16 = actual6.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a15 = actual5.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a14 = actual4.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a13 = actual3.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a12 = actual2.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "a11 = actual1.copy()[['GEO.display-label','HC01_EST_VC01']][1:]\n",
    "\n",
    "# len(a17),len(a16),len(a15),len(a14),len(a13),len(a12),len(a11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a17places = [place for place in a17['GEO.display-label']]\n",
    "a16places = [place for place in a16['GEO.display-label']]\n",
    "a15places = [place for place in a15['GEO.display-label']]\n",
    "a14places = [place for place in a14['GEO.display-label']]\n",
    "a13places = [place for place in a13['GEO.display-label']]\n",
    "a12places = [place for place in a12['GEO.display-label']]\n",
    "a11places = [place for place in a11['GEO.display-label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_places=[]\n",
    "for place in a17places:\n",
    "    if place in a16places:\n",
    "        if place in a15places:\n",
    "            if place in a14places:\n",
    "                if place in a13places:\n",
    "                    if place in a12places:\n",
    "                        if place in a11places:\n",
    "                            combo_places.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combo_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- convert dataframes to only contain shared values\n",
    "\"\"\"\n",
    "a17=a17.loc[a17['GEO.display-label'].isin(combo_places)]\n",
    "a16=a16.loc[a16['GEO.display-label'].isin(combo_places)]\n",
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "a14=a14.loc[a14['GEO.display-label'].isin(combo_places)]\n",
    "a13=a13.loc[a13['GEO.display-label'].isin(combo_places)]\n",
    "a12=a12.loc[a12['GEO.display-label'].isin(combo_places)]\n",
    "a11=a11.loc[a11['GEO.display-label'].isin(combo_places)]\n",
    "# len(a17),len(a16),len(a15),len(a14),len(a13),len(a12),len(a11)\n",
    "a17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "base=pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# select columns of measurement\n",
    "get_base=base.copy()[['PLACE','STATE','AV0AA1970','AV0AA1980','AV0AA1990','AV0AA2000','AV0AA2010']]\n",
    "# drop NaN rows @ thresh = 3 due to NHGISCODE being non-NaN (25103 rows × 5 columns ; 6333 non-measurable) \n",
    "_measureable_base = get_base.dropna(axis=0,thresh=3)\n",
    "# convert NaN values to 0 (note: there are 270 'dead' counties ('A00AA2010' == 0))\n",
    "_measureable_base = _measureable_base.fillna(0)\n",
    "_measureable_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_measureable_unique_places),len(_measureable_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "a14=a14.loc[a14['GEO.display-label'].isin(combo_places)]\n",
    "a13=a13.loc[a13['GEO.display-label'].isin(combo_places)]\n",
    "a12=a12.loc[a12['GEO.display-label'].isin(combo_places)]\n",
    "a11=a11.loc[a11['GEO.display-label'].isin(combo_places)]\n",
    "a15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split GEO.display-label into list [place, state]\n",
    "places_11_17 = [label.split(',') for label in a15['GEO.display-label']]\n",
    "# make DataFrame of Place, State for compairson to _measureable_unique_places and _measureable_base\n",
    "df_places_11_17 = pd.DataFrame(places_11_17,columns=['PLACE','STATE'])\n",
    "# add population estimates \n",
    "eleven_seventeen = [df_places_11_17,a11.HC01_EST_VC01,a12.HC01_EST_VC01,a13.HC01_EST_VC01,a14.HC01_EST_VC01,a15.HC01_EST_VC01]\n",
    "df_pop_by_place_11_17 = pd.concat(eleven_seventeen,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_by_place_11_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_measureable_unique_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_this(place,state,prior):\n",
    "    # set place state\n",
    "    place_w_state=place+', '+state\n",
    "    \n",
    "    # 1970-2010\n",
    "    sf70to10 = _measureable_unique_places.loc[_measureable_unique_places['PLACE'] == place]\n",
    "    # 2011\n",
    "    sf11 = a11.loc[a11['GEO.display-label'] == place_w_state]\n",
    "    # 2012\n",
    "    sf12 = a12.loc[a12['GEO.display-label'] == place_w_state]\n",
    "    # 2013\n",
    "    sf13 = a13.loc[a13['GEO.display-label'] == place_w_state]\n",
    "    # 2014\n",
    "    sf14 = a14.loc[a14['GEO.display-label'] == place_w_state]\n",
    "    # 2015\n",
    "    sf15 = a15.loc[a15['GEO.display-label'] == place_w_state]\n",
    "\n",
    "    # combine all population measurements \n",
    "    sf_pops = sf70to10,sf11.HC01_EST_VC01,sf12.HC01_EST_VC01,sf13.HC01_EST_VC01,sf14.HC01_EST_VC01,sf15.HC01_EST_VC01\n",
    "\n",
    "    # combine dataframes\n",
    "    combo_sf = pd.concat(sf_pops,axis=1,sort=True)\n",
    "    # fill lowest row NaN values with value from above rows (if exists)\n",
    "    combo_sf = combo_sf.fillna(method='ffill')\n",
    "    # drop rows with NaN values\n",
    "    combo_sf = combo_sf.dropna(axis=0,thresh=7)\n",
    "\n",
    "    # drop Place and State columns\n",
    "    combo_sf = combo_sf.drop( ['PLACE' , 'STATE'] ,axis=1 )\n",
    "\n",
    "    # change column names to years 1970-2010 & 2011-2015\n",
    "    combo_sf.columns = ['1970', '1980', '1990', '2000', '2010', '2011','2012','2013','2014','2015']\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    pre_dt_cols = [i for i in combo_sf.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=pre_dt_cols)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = combo_sf.columns\n",
    "\n",
    "    # add datetime_df as first row\n",
    "    prophet_place = pd.concat((datetime_df,combo_sf),axis=0) \n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_place = prophet_place.T\n",
    "\n",
    "    # convert df to short \n",
    "    a = prophet_almost_ready_place\n",
    "\n",
    "    # make the prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=prior)\n",
    "    \n",
    "    print(a)\n",
    "    if len(a.columns) == 3:\n",
    "        a = a.drop(a.columns[2],axis=1)\n",
    "        print(a)\n",
    "\n",
    "    # rename place df's columns to agree with prophet formatting\n",
    "    a.columns = ['ds','y']\n",
    "    print(a)\n",
    "    # fit place on prophet model \n",
    "    b = place_prophet.fit(a)\n",
    "\n",
    "    # make a future dataframe for 20 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=1*20, freq='Y' )\n",
    "    # establish predictions\n",
    "    sf_forecast = place_prophet.predict(place_forecast)\n",
    "    \n",
    "    # output predictions\n",
    "    return [sf_forecast,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "sf = prophet_this(place='Denver city',state='Colorado',prior=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark\n",
    "base_16 = int(sf[1].y[9]) + (int(sf[1].y[9])-int(sf[1].y[8]))\n",
    "base_17 = base_16 + (int(sf[1].y[9])-int(sf[1].y[8]))\n",
    "\n",
    "# predictions 2016 & 2017\n",
    "p_16 = sf[0]['yhat'][11]\n",
    "p_17 = sf[0]['yhat'][12]\n",
    "\n",
    "# actual 2016 & 2017\n",
    "sf16 = a16.loc[a16['GEO.display-label'] == 'Denver city, Colorado']\n",
    "sf17 = a17.loc[a17['GEO.display-label'] == 'Denver city, Colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_16 = int(sf16.HC01_EST_VC01)\n",
    "act_17 = int(sf17.HC01_EST_VC01)\n",
    "# actual - base 2016\n",
    "ab16 = act_16-base_16\n",
    "# pred - base 2016\n",
    "ap16 = act_16-p_16\n",
    "# pred - base 2017\n",
    "ab17 = act_17-base_17\n",
    "# pred - base 2017\n",
    "ap17 = act_17-p_17\n",
    "\n",
    "print(f\"{abs(ab16)}\\n{abs(int(ap16))}\\n\\n{abs(ab17)}\\n{abs(int(ap17))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "San Francisco city, California\n",
    "sf = prophet_this(place='San Francisco city',state='California')\n",
    "changepoint_prior_scale=0.05\n",
    "-2172 27830\n",
    "-66162 -24158\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Bentonville city, Arkansas\n",
    "sf = prophet_this(place='Bentonville city',state='Arkansas',prior=0.05)\n",
    "335 3508\n",
    "-9938 -5722\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "New York city, New York\n",
    "sf = prophet_this(place='New York city',state='New York',prior=0.15)\n",
    "-36636 -66472\n",
    "-441471 -429353\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "New Orleans city, New Orleans\n",
    "sf = prophet_this(place='New Orleans city',state='Louisiana',prior=0.15)\n",
    "-2083 44438\n",
    "-71863 -13624\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# san francisco forecast population through 2017\n",
    "sf[0][['ds','yhat']][:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf16.HC01_EST_VC01.values[0],sf17.HC01_EST_VC01.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_16 = 42499\n",
    "r_17 = 34022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_16-r_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_16-r_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_17-r_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_17-r_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_11 = a11.loc[a11['GEO.display-label'] == 'Pleasanton city, California']\n",
    "pl_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[]\n",
    "def prophet_this(place,state):\n",
    "    # set place state\n",
    "    place_w_state=place+', '+state\n",
    "    \n",
    "    # 1970-2010\n",
    "    sf70to10 = _measureable_unique_places.loc[_measureable_unique_places['PLACE'] == place]\n",
    "    # 2011\n",
    "    sf11 = a11.loc[a11['GEO.display-label'] == place_w_state]\n",
    "    # 2012\n",
    "    sf12 = a12.loc[a12['GEO.display-label'] == place_w_state]\n",
    "    # 2013\n",
    "    sf13 = a13.loc[a13['GEO.display-label'] == place_w_state]\n",
    "    # 2014\n",
    "    sf14 = a14.loc[a14['GEO.display-label'] == place_w_state]\n",
    "    # 2015\n",
    "    sf15 = a15.loc[a15['GEO.display-label'] == place_w_state]\n",
    "\n",
    "    # combine all population measurements \n",
    "    sf_pops = sf70to10,sf11.HC01_EST_VC01,sf12.HC01_EST_VC01,sf13.HC01_EST_VC01,sf14.HC01_EST_VC01,sf15.HC01_EST_VC01\n",
    "\n",
    "    # combine dataframes\n",
    "    combo_sf = pd.concat(sf_pops,axis=1,sort=True)\n",
    "    # fill lowest row NaN values with value from above rows (if exists)\n",
    "    combo_sf = combo_sf.fillna(method='ffill')\n",
    "    # drop rows with NaN values\n",
    "    combo_sf = combo_sf.dropna(axis=0,thresh=7)\n",
    "\n",
    "    # drop Place and State columns\n",
    "    combo_sf = combo_sf.drop(['PLACE', 'STATE'],axis=1)\n",
    "\n",
    "    # change column names to years 1970-2010 & 2011-2015\n",
    "    combo_sf.columns = ['1970', '1980', '1990', '2000', '2010', '2011','2012','2013','2014','2015']\n",
    "\n",
    "    # list of str column names as years (for conversion to datetime)\n",
    "    pre_dt_cols = [i for i in combo_sf.columns]\n",
    "    # convert year_only_columns to DatetimeIndex of Timestamps\n",
    "    dt_columns = pd.to_datetime(arg=pre_dt_cols)\n",
    "\n",
    "    # convert dt_columns into dataframe \n",
    "    datetime_df = pd.DataFrame(dt_columns).T\n",
    "    # w/ columns, so concatable with measureable_unique_counties\n",
    "    datetime_df.columns = combo_sf.columns\n",
    "\n",
    "    # add datetime_df as first row\n",
    "    prophet_place = pd.concat((datetime_df,combo_sf),axis=0) \n",
    "    # then transpose to 2 rows x 23 columns \n",
    "    prophet_almost_ready_place = prophet_place.T\n",
    "\n",
    "    # convert df to short \n",
    "    a = prophet_almost_ready_place\n",
    "\n",
    "    # make the prophet model\n",
    "    place_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "\n",
    "    # rename place df's columns to agree with prophet formatting\n",
    "    a.columns = ['ds','y']\n",
    "    # fit place on prophet model \n",
    "    b = place_prophet.fit(a)\n",
    "\n",
    "    # make a future dataframe for 20 years\n",
    "    place_forecast = place_prophet.make_future_dataframe( periods=1*20, freq='Y' )\n",
    "    # establish predictions\n",
    "    sf_forecast = place_prophet.predict(place_forecast)\n",
    "\n",
    "    # benchmark 2016 & 2017\n",
    "    base_16=int(prophet_almost_ready_place.y[9])+(int(prophet_almost_ready_place.y[9])-int(prophet_almost_ready_place.y[8]))\n",
    "    base_17=base_16+(int(prophet_almost_ready_place.y[9])-int(prophet_almost_ready_place.y[8]))\n",
    "    \n",
    "    # predictions 2016 & 2017\n",
    "    p_16 = sf_forecast['yhat'][11]\n",
    "    p_17 = sf_forecast['yhat'][12]\n",
    "    \n",
    "    # actual 2016 & 2017\n",
    "    sf16 = a16.loc[a16['GEO.display-label'] == place_w_state]\n",
    "    sf17 = a17.loc[a17['GEO.display-label'] == place_w_state]\n",
    "    \n",
    "    # output base 2016 vs pred 2016 ; base 2017 vs real 2017\n",
    "    return [[base_16-r_16,p_16-r_16],[base_17-r_17,p_17-r_17]]\n",
    "\n",
    "# run\n",
    "aaa=prophet_this(place='Bentonville city',state='Arkansas')\n",
    "# bbb=prophet_this(place='Fayetteville city',state='Arkansas')\n",
    "out.append(aaa)\n",
    "# out.append(bbb)\n",
    "# prophet \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29577, 29575, 29551, 29577, 29575, 29551)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import fbprophet \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# def mega_place_df():\n",
    "\"\"\"\n",
    ">> takes in \n",
    "    > Census 1970-2010 dataframe (1 df)\n",
    "        >> total population by Place measurements\n",
    "    > American Community Survey (ACS) 2011-2017 dataframes (7 dfs)\n",
    "        >> total population (age & sex) by Place \n",
    "        \n",
    ">> forges DataFrame of places that have \n",
    "    > at least one (1) recording for Census years 1970-2010\n",
    "    > at least one (1) recording for ACS years 2011-2015\n",
    "\n",
    ">> test our model v. base on\n",
    "    > random sample 100 Places\n",
    "    > random sample 100 Places from bottom half population size\n",
    "    > random sample 100 Places from top half population size\n",
    "\"\"\"\n",
    "\n",
    "'''load Train data'''\n",
    "# population by Place Census 1970-2010 measurements\n",
    "load_pop_by_place = pd.read_csv('../../data/NHGIS/nhgis0002_csv/nhgis0002_ts_nominal_place.csv',encoding='ISO-8859-1')\n",
    "# population by Place ACS 2011\n",
    "load_acs_20l1 = pd.read_csv('../../data/American_Community_Survey/ACS_11_5YR_S0101/ACS_11_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2012\n",
    "load_acs_20l2 = pd.read_csv('../../data/American_Community_Survey/ACS_12_5YR_S0101/ACS_12_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2013\n",
    "load_acs_20l3 = pd.read_csv('../../data/American_Community_Survey/ACS_13_5YR_S0101/ACS_13_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2014\n",
    "load_acs_20l4 = pd.read_csv('../../data/American_Community_Survey/ACS_14_5YR_S0101/ACS_14_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2015\n",
    "load_acs_20l5 = pd.read_csv('../../data/American_Community_Survey/ACS_15_5YR_S0101/ACS_15_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "'''load Test data'''\n",
    "# population by Place ACS 2016\n",
    "load_acs_20l6 = pd.read_csv('../../data/American_Community_Survey/ACS_16_5YR_S0101/ACS_16_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "# population by Place ACS 2017\n",
    "load_acs_20l7 = pd.read_csv('../../data/American_Community_Survey/ACS_17_5YR_S0101/ACS_17_5YR_S0101_with_ann.csv',encoding='ISO-8859-1',low_memory=False) \n",
    "\n",
    "'''build Train dataframe'''\n",
    "\n",
    "\n",
    "'''we need to make sure each Place we want to predict for \n",
    "    has an actual measurement in 2016 and 2017 so that we can evaluate the accuracy of our model'''\n",
    "# identify Places measured in 2016 ACS (29575)\n",
    "acs16places = [place for place in load_acs_20l6['GEO.display-label']]\n",
    "# identify Places measured in 2017 ACS (29577)\n",
    "acs17places = [place for place in load_acs_20l7['GEO.display-label']]\n",
    "# cross 2017 Places w/ 2016 Places, keep coexisting Places (29551)\n",
    "base_places = [place for place in acs17places if place in acs16places]\n",
    "\n",
    "a15=a15.loc[a15['GEO.display-label'].isin(combo_places)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# census70_10places = [place for place in ]\n",
    "tq=['Abanda CDP, Alabama','Abbeville city, Alabama','Adamsville city, Alabama','Abanda CDP, Alabama','Abbeville city, Alabama','Adamsville city, Alabama']\n",
    "len(tq),len((set(tq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns relevant to our end goal of predicting population for a given place\n",
    "place_cols_of_interest = ['PLACE','STATE', 'AV0AA1970', 'AV0AA1980', 'AV0AA1990', 'AV0AA2000', 'AV0AA2010']\n",
    "\n",
    "# shrink pop_by_place DataFrame to contain only information directly tied to end goal\n",
    "pop_by_place = load_pop_by_place[place_cols_of_interest]\n",
    "pop_by_place\n",
    "# drop rows without at lest one (1) Census measurement \n",
    "# len(pop_by_place.dropna(axis=0,thresh=3)),len(pop_by_place)\n",
    "# sum(no_name12.AV0AA2010.isnull()),len(no_name12.AV0AA2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_place.columns#PLACEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_name12.loc[no_name12.AV0AA2010.isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify Places in Census 1970-2010 DataFrame\n",
    "c70c10places = [load_pop_by_place[['PLACE','STATE']].values[_][0] +', '+ load_pop_by_place[['PLACE','STATE']].values[_][1] for _ in range(len(load_pop_by_place))]\n",
    "# identify Places measured in 2011 ACS\n",
    "acs11places = [place for place in load_acs_20l1['GEO.display-label']]\n",
    "# identify Places measured in 2012 ACS\n",
    "acs12places = [place for place in load_acs_20l2['GEO.display-label']]\n",
    "# identify Places measured in 2013 ACS\n",
    "acs13places = [place for place in load_acs_20l3['GEO.display-label']]\n",
    "# identify Places measured in 2014 ACS\n",
    "acs14places = [place for place in load_acs_20l4['GEO.display-label']]\n",
    "# identify Places measured in 2015 ACS\n",
    "acs15places = [place for place in load_acs_20l5['GEO.display-label']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
