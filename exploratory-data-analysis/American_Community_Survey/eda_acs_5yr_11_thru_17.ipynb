{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default cleaning method until proven otherwise\n",
    "def clean_census_frame(csv_path , head=False , reset=True , set_index=False ):\n",
    "    '''\n",
    "    inputs) \n",
    "        >> csv_path\n",
    "            > path to csv\n",
    "        >> head\n",
    "            > default=False\n",
    "                >> if != False\n",
    "                    > integer\n",
    "                        >> returns the first {head} rows (using .head() method) \n",
    "                            > instead of enitre dataframe\n",
    "        >> reset\n",
    "            > default=True\n",
    "                >> resets index after taking out rows\n",
    "            > if set to False\n",
    "                >> will not reset index\n",
    "        >> set_index\n",
    "            > default=False\n",
    "            > if != False\n",
    "                >> will set_index of new df to set_index\n",
    "    output)\n",
    "        >> dataframe cleaned like 2000 Census age&sex by 5-digit Zip Code (also how 2010 for same is cleaned)\n",
    "    how)\n",
    "        1. reads in csv , assumes it's large\n",
    "        2. makes a copy for editing \n",
    "            > and potential future use\n",
    "        3. locates readable column names  and non-readable names \n",
    "            > readable\n",
    "                    > e.g. Estimate; SEX AND AGE - Total population\n",
    "                >> assumes they are currently in row 0\n",
    "            > non-readable\n",
    "                    > e.g. HC01_VC03\n",
    "                >> assumes they are currently == dataframe.columns\n",
    "        4. replaces dataframe.columns (non-readable) with readable column names\n",
    "            > and drops the old 0th column (column where readable names were stored)\n",
    "        \n",
    "    '''\n",
    "    # load data\n",
    "    df = pd.read_csv( csv_path , low_memory=False )\n",
    "\n",
    "    # and copy\n",
    "    _df = df.copy()\n",
    "\n",
    "    # reset column names to current 0th row values\n",
    "    _df.columns = _df.iloc[0]\n",
    "    # new 2000 dataframe without row where values are from\n",
    "    clean_df = _df[1:]\n",
    "    \n",
    "    # default\n",
    "    if reset==True:\n",
    "        # reset index\n",
    "        clean_df = clean_df.reset_index()\n",
    "        \n",
    "    # set_index\n",
    "    if set_index:\n",
    "        clean_df = clean_df.set_index(set_index)\n",
    "    \n",
    "    if head:\n",
    "        # return first {head} rows of dataframe\n",
    "        return clean_df.head(head)\n",
    "    else:\n",
    "        # return dataframe\n",
    "        return clean_df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def test_non_unique(column_names):\n",
    "    '''\n",
    "    input) \n",
    "        >> list of column names {column_names}\n",
    "            > columns to check for duplicate instances\n",
    "    output)\n",
    "        >> indexed list of names occouring more than once \n",
    "    '''\n",
    "    # store first instance\n",
    "    first_occour = []\n",
    "    # store 2nd+ instance(s)\n",
    "    non_unique = []\n",
    "    # we're going to want index\n",
    "    for i,_ in enumerate(column_names):\n",
    "        # not first time\n",
    "        if _ not in first_occour:\n",
    "            first_occour.append(_)\n",
    "        # if not first, tag&bag\n",
    "        else:\n",
    "            non_unique.append([i,_])\n",
    "    # output index w/ non-first instances\n",
    "    return non_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bring_the_5yr_acs_2k11_thru_2k17():\n",
    "'''\n",
    "inputs)\n",
    "    >> list_of_paths\n",
    "        > paths to each raw dataframe\n",
    "output)\n",
    "    >> list of modified dataframes\n",
    "function)\n",
    "    1. load and copy data\n",
    "    2. \n",
    "'''\n",
    "# load 2011 \n",
    "y2k11 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_11_5YR_DP05_with_ann.csv',reset=False)\n",
    "# 2012\n",
    "y2k12 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_12_5YR_DP05_with_ann.csv',reset=False)\n",
    "#2013\n",
    "y2k13 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_13_5YR_DP05_with_ann.csv',reset=False)\n",
    "# 2014\n",
    "y2k14 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_14_5YR_DP05_with_ann.csv',reset=False)\n",
    "# 2015\n",
    "y2k15 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_15_5YR_DP05_with_ann.csv',reset=False)\n",
    "#2016\n",
    "y2k16 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_16_5YR_DP05_with_ann.csv',reset=False)\n",
    "#2017\n",
    "y2k17 = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_17_5YR_DP05_with_ann.csv',reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2011\n",
    "y11 = y2k11.copy()\n",
    "# 2012\n",
    "y12 = y2k12.copy()\n",
    "# 2013\n",
    "y13 = y2k13.copy()\n",
    "# 2014\n",
    "y14 = y2k14.copy()\n",
    "# 2015\n",
    "y15 = y2k15.copy()\n",
    "# 2016\n",
    "y16 = y2k16.copy()\n",
    "# 2017\n",
    "y17 = y2k17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''identify columns'''\n",
    "# 2011\n",
    "tags11 = y11.columns  \n",
    "# 2012\n",
    "tags12 = y12.columns  \n",
    "#2013\n",
    "tags13 = y13.columns  \n",
    "# 2014\n",
    "tags14 = y14.columns  \n",
    "# 2015\n",
    "tags15 = y15.columns  \n",
    "#2016\n",
    "tags16 = y16.columns  \n",
    "# 2017\n",
    "tags17 = y17.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tags11,tags12,tags13,tags14,tags15,tags16,tags17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in tags:\n",
    "    print(f'len = {len(tag)}\\nunique = {len(set(tag))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2011 == 2012\n",
    "if tags11.all() != tags12.all():\n",
    "    raise Exception('tags11 != tags12')\n",
    "\n",
    "# 2013 == 2014\n",
    "if tags13.all() != tags14.all():\n",
    "    raise Exception('tags13 != tags14')\n",
    "\n",
    "# 2015 == 2016\n",
    "if tags15.all() != tags16.all():\n",
    "    raise Exception('tags15 != tags16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''identify common columns'''\n",
    "# collection of columns appearing in all 7 dataframes 2011-2017\n",
    "\n",
    "# 2017\n",
    "a = [t for t in tags17 if t in tags11 & tags12 & tags13 & tags14 & tags15 & tags16]\n",
    "# 2012\n",
    "z = [iii for iii in tags12 if iii in tags11 & tags13 & tags14 & tags15 & tags16 & tags17]\n",
    "# 2011\n",
    "b = [tt for tt in tags11 if tt in tags12 & tags13 & tags14 & tags15 & tags16 & tags17]\n",
    "# 2014\n",
    "y = [ii for ii in tags14 if ii in tags11 & tags12 & tags13 & tags15 & tags16 & tags17]\n",
    "# 2013\n",
    "c = [ttt for ttt in tags13 if ttt in tags11 & tags12 & tags14 & tags15 & tags16 & tags17]\n",
    "# 2016\n",
    "x = [i for i in tags16 if i in tags11 & tags12 & tags13 & tags14 & tags15 & tags17]\n",
    "# 2015\n",
    "d = [tttt for tttt in tags15 if tttt in tags11 & tags12 & tags13 & tags14 & tags16 & tags17]\n",
    "\n",
    "# list of all common columns (in order of starting year 2011-2017)\n",
    "collect = [b,z,c,y,d,x,a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "# for year\n",
    "for _ in collect:\n",
    "    # record size, unique instances, and diffenrence\n",
    "    d.append((len(_),len(set(_)),len(_)-len(set(_))))\n",
    "    \n",
    "years = ['2011','2012','2013','2014','2015','2016','2017']\n",
    "cols = ['list','set','diff']\n",
    "\n",
    "q = pd.DataFrame(data=d,index=years,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - all same set len\n",
    "    - 2013-2017 have 8 non-unique than set len\n",
    "    - 2011 and 2012 have 8 more non-unique than 2013-2017 (16 total)\n",
    "- ***actions***:\n",
    "    - ideal\n",
    "        - leave 8 non-unique in all frames\n",
    "        - remove non-unique 9-16 from 2011 and 2012\n",
    "    - else\n",
    "        - remove all non-unique values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop each frame's uncommon columns, reset index\"\"\"\n",
    "# 2011\n",
    "k11 = y11.copy()\n",
    "k11 = k11[[i for i in a]].reset_index()\n",
    "# # # 2012\n",
    "k12 = y12.copy()\n",
    "k12 = k12[[i for i in a]].reset_index()\n",
    "# # # 2013\n",
    "k13 = y13.copy()\n",
    "k13 = k13[[i for i in a]].reset_index() #.drop(uncommon_13,axis=1).reset_index()\n",
    "# # # 2014\n",
    "k14 = y14.copy()\n",
    "k14 = k14[[i for i in a]].reset_index()\n",
    "# # # 2015\n",
    "k15 = y15.copy()\n",
    "k15 = k13[[i for i in a]].reset_index()  #.drop(uncommon_15,axis=1).reset_index()\n",
    "# # # 2016\n",
    "k16 = y16.copy()\n",
    "k16 = k16[[i for i in a]].reset_index()\n",
    "# # # 2017\n",
    "k17 = y17.copy()\n",
    "k17 = k17[[i for i in a]].reset_index()  # .drop(uncommon_17,axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = (k11.copy(),k12.copy(),k13.copy(),k14.copy(),k15.copy(),k16.copy(),k17.copy())\n",
    "for _ in group:\n",
    "    print(_.info(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k12.columns)):\n",
    "    if k12.columns[i] != k11.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k12.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k13.columns)):\n",
    "    if k13.columns[i] != k11.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k13.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k14.columns)):\n",
    "    if k14.columns[i] != k11.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k14.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k14.columns)):\n",
    "    if k14.columns[i] != k12.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k14.columns[i]}\\n!=\\n{k12.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k14.columns)):\n",
    "    if k14.columns[i] != k15.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k14.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k14.columns)):\n",
    "    if k14.columns[i] != k16.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k14.columns[i]}\\n!=\\n{k16.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k14.columns)):\n",
    "    if k14.columns[i] != k17.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k14.columns[i]}\\n!=\\n{k17.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k15.columns)):\n",
    "    if k15.columns[i] != k11.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k15.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k16.columns)):\n",
    "    if k16.columns[i] != k11.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k16.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k17.columns)):\n",
    "    if k17.columns[i] != k11.columns[i]:\n",
    "        # let us know where first non match is\n",
    "        raise Exception(f'{i}\\n{k17.columns[i]}\\n!=\\n{k11.columns[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k15.columns.all() == k14.columns.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - getting too interesting for ipynb\n",
    "- ***actins***:\n",
    "    - drop all duplicate columns\n",
    "    - unless dropping 8 from 2011/2012 somehow works\n",
    "- ***extra***:\n",
    "    - old fashon enumerated for loop, find columns that are 100% the same\n",
    "        - reset, restart, &continue below\n",
    "            - just because it's late, we're going to have fun and try to do it all in one run\n",
    "                - code as if was py with 1 attempt (aka note well or die)\n",
    "                - I want straight into model from here, nothing less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "STEP 0 >> imports; def clean\n",
    "\"\"\"\n",
    "\n",
    "# default cleaning method until proven otherwise\n",
    "def clean_census_frame(csv_path , head=False , reset=True , set_index=False ):\n",
    "    '''\n",
    "    inputs) \n",
    "        >> csv_path\n",
    "            > path to csv\n",
    "        >> head\n",
    "            > default=False\n",
    "                >> if != False\n",
    "                    > integer\n",
    "                        >> returns the first {head} rows (using .head() method) \n",
    "                            > instead of enitre dataframe\n",
    "        >> reset\n",
    "            > default=True\n",
    "                >> resets index after taking out rows\n",
    "            > if set to False\n",
    "                >> will not reset index\n",
    "        >> set_index\n",
    "            > default=False\n",
    "            > if != False\n",
    "                >> will set_index of new df to set_index\n",
    "    output)\n",
    "        >> dataframe cleaned like 2000 Census age&sex by 5-digit Zip Code (also how 2010 for same is cleaned)\n",
    "    how)\n",
    "        1. reads in csv , assumes it's large\n",
    "        2. makes a copy for editing \n",
    "            > and potential future use\n",
    "        3. locates readable column names  and non-readable names \n",
    "            > readable\n",
    "                    > e.g. Estimate; SEX AND AGE - Total population\n",
    "                >> assumes they are currently in row 0\n",
    "            > non-readable\n",
    "                    > e.g. HC01_VC03\n",
    "                >> assumes they are currently == dataframe.columns\n",
    "        4. replaces dataframe.columns (non-readable) with readable column names\n",
    "            > and drops the old 0th column (column where readable names were stored)\n",
    "        \n",
    "    '''\n",
    "    # load data\n",
    "    df = pd.read_csv( csv_path , low_memory=False )\n",
    "\n",
    "    # and copy\n",
    "    _df = df.copy()\n",
    "\n",
    "    # reset column names to current 0th row values\n",
    "    _df.columns = _df.iloc[0]\n",
    "    # new 2000 dataframe without row where values are from\n",
    "    clean_df = _df[1:]\n",
    "    \n",
    "    # default\n",
    "    if reset==True:\n",
    "        # reset index\n",
    "        clean_df = clean_df.reset_index()\n",
    "        \n",
    "    # set_index\n",
    "    if set_index:\n",
    "        clean_df = clean_df.set_index(set_index)\n",
    "    \n",
    "    if head:\n",
    "        # return first {head} rows of dataframe\n",
    "        return clean_df.head(head)\n",
    "    else:\n",
    "        # return dataframe\n",
    "        return clean_df\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 1 >> load data, reset; make copies\n",
    "'''\n",
    "\n",
    "# load \n",
    "# 2011 \n",
    "twenty_eleven = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_11_5YR_DP05_with_ann.csv')\n",
    "# 2012\n",
    "twenty_twelve = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_12_5YR_DP05_with_ann.csv')\n",
    "#2013\n",
    "twenty_thirteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_13_5YR_DP05_with_ann.csv')\n",
    "# 2014\n",
    "twenty_fourteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_14_5YR_DP05_with_ann.csv')\n",
    "# 2015\n",
    "twenty_fifteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_15_5YR_DP05_with_ann.csv')\n",
    "#2016\n",
    "twenty_sixteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_16_5YR_DP05_with_ann.csv')\n",
    "#2017\n",
    "twenty_seventeen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_17_5YR_DP05_with_ann.csv')\n",
    "\n",
    "# copy \n",
    "# 2011 \n",
    "y2k11 = twenty_eleven.copy()\n",
    "# 2012\n",
    "y2k12 = twenty_twelve.copy()\n",
    "#2013\n",
    "y2k13 = twenty_thirteen.copy()\n",
    "# 2014\n",
    "y2k14 = twenty_fourteen.copy()\n",
    "# 2015\n",
    "y2k15 = twenty_fifteen.copy()\n",
    "#2016\n",
    "y2k16 = twenty_sixteen.copy()\n",
    "#2017\n",
    "y2k17 = twenty_seventeen.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(common_tags), len(set(common_tags))] , [len(_test_tags_), len(set(_test_tags_))]\n",
    "len(set(tags11)),len(set(tags17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''identify non common columns for specific frames'''\n",
    "# 2011\n",
    "uncommon_11 = [tag for tag in tags11 if tag not in common_tags]\n",
    "# 2012\n",
    "uncommon_12 = [tag for tag in tags12 if tag not in common_tags]\n",
    "# 2013\n",
    "uncommon_13 = [tag for tag in tags13 if tag not in common_tags]\n",
    "# 2014\n",
    "uncommon_14 = [tag for tag in tags14 if tag not in common_tags]\n",
    "# 2015\n",
    "uncommon_15 = [tag for tag in tags15 if tag not in common_tags]\n",
    "# 2016\n",
    "uncommon_16 = [tag for tag in tags16 if tag not in common_tags]\n",
    "# 2017\n",
    "uncommon_17 = [tag for tag in tags17 if tag not in common_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect uncommon\n",
    "uncommon = [uncommon_11,uncommon_12,uncommon_13,uncommon_14,uncommon_15,uncommon_16,uncommon_17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uncommon_tag in uncommon:\n",
    "    print(f'len = {len(uncommon_tag)}\\nunique = {len(set(uncommon_tag))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop each frame's uncommon columns, reset index\"\"\"\n",
    "# 2011\n",
    "k11 = y11.copy().drop(uncommon_11,axis=1).reset_index()\n",
    "# # 2012\n",
    "k12 = y12.copy().drop(uncommon_12,axis=1).reset_index()\n",
    "# # 2013\n",
    "k13 = y13.copy().drop(uncommon_13,axis=1).reset_index()\n",
    "# # 2014\n",
    "k14 = y14.copy().drop(uncommon_14,axis=1).reset_index()\n",
    "# # 2015\n",
    "k15 = y15.copy().drop(uncommon_15,axis=1).reset_index()\n",
    "# # 2016\n",
    "k16 = y16.copy().drop(uncommon_16,axis=1).reset_index()\n",
    "# # 2017\n",
    "k17 = y17.copy().drop(uncommon_17,axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y11.info(),y13.info(),y15.info(),y17.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"don't forget, 2011 and 2012 have extra repeats, check for non-unique column instances in new dfs\"\"\"\n",
    "# 2011\n",
    "a=test_non_unique(y11.copy().columns)\n",
    "# 2012\n",
    "b=test_non_unique(y12.copy().columns)\n",
    "# 2013\n",
    "c=test_non_unique(y13.copy().columns)\n",
    "# 2014\n",
    "d=test_non_unique(y14.copy().columns)\n",
    "# 2015\n",
    "e=test_non_unique(y15.copy().columns)\n",
    "# 2016\n",
    "f=test_non_unique(y16.copy().columns)\n",
    "# 2017\n",
    "g=test_non_unique(y17.copy().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of all repeats which occour in all 7 dataframes (len==8)\n",
    "\"\"\"[[80, 'Estimate; SEX AND AGE - 18 years and over'],\n",
    "[81, 'Margin of Error; SEX AND AGE - 18 years and over'],\n",
    "[82, 'Percent; SEX AND AGE - 18 years and over'],\n",
    "[83, 'Percent Margin of Error; SEX AND AGE - 18 years and over'],\n",
    "[84, 'Estimate; SEX AND AGE - 65 years and over'],\n",
    "[85, 'Margin of Error; SEX AND AGE - 65 years and over'],\n",
    "[86, 'Percent; SEX AND AGE - 65 years and over'],\n",
    "[87, 'Percent Margin of Error; SEX AND AGE - 65 years and over']]\"\"\"\n",
    "common_repeats = [_ for _ in g if _ in [i for i in [a, b, c, d, e, f]]]\n",
    "\n",
    "# identify repeats occouring only in 2011 and 2012 (already checked are not unique to self)\n",
    "\"\"\"[[100, 'Estimate; RACE - One race'],\n",
    " [101, 'Margin of Error; RACE - One race'],\n",
    " [102, 'Percent; RACE - One race'],\n",
    " [103, 'Percent Margin of Error; RACE - One race'],\n",
    " [188, 'Estimate; RACE - Two or more races'],\n",
    " [189, 'Margin of Error; RACE - Two or more races'],\n",
    " [190, 'Percent; RACE - Two or more races'],\n",
    " [191, 'Percent Margin of Error; RACE - Two or more races']]\"\"\"\n",
    "first_two_only = [i for i in a if i not in common_repeats and i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{common_repeats}\\n\\n{first_two_only}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2011\n",
    "y11 = y11.copy().drop(first_two_only,axis=1).reset_index()\n",
    "# # 2012\n",
    "y12 = y12.copy().drop(first_two_only,axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y11.info(),y12.info(),y13.info(),y15.info(),y17.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [100,101,102,103,188,189,190,191]\n",
    "# adjust 2011\n",
    "y11 = y11.drop(y11.columns[instances], axis=1)\n",
    "# adjust 2012\n",
    "y12 = y12.drop(y12.columns[instances], axis=1)\n",
    "\n",
    "\n",
    "# return common_repeats,first_two_only  # [y11,y12,y13,y14,y15,y16,y17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = bring_the_5yr_acs_2k11_thru_2k17()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    print(i.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_000 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_000)):\n",
    "    print(f'{len(test_000[i])} rows x {len(test_000[i].iloc[1])} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of identical columns 2011 and 2012 is same number as all columns in 2011\n",
    "# and count of columns in 2012 is same as count in 2011\n",
    "if sum(test_000[0].columns == test_000[1].columns) == len(test_000[0].columns) and len(test_000[1].columns) == len(test_000[0].columns):\n",
    "    # number of columns for 2013 is same as number that are same between 2013 and 2014 and between 2014 and 2015 \n",
    "    if len(test_000[2].columns) == sum(test_000[2].columns == test_000[3].columns) & sum(test_000[3].columns == test_000[4].columns):\n",
    "        # number of columns for 2017 is same as number that are same between 2016 and 2014 and between 2017 and 2015 \n",
    "        if len(test_000[6].columns) == sum(test_000[5].columns == test_000[3].columns) & sum(test_000[6].columns == test_000[4].columns):\n",
    "            if len(test_000[0].columns) != len(test_000[5].columns) and len(test_000[5].columns) == len(test_000[5].columns):\n",
    "                print('pretty ok to assume\\n2011-2012 are identical and 2013-2017 are identical\\nbut 2011-2012 and 2013-2017 are different')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2k11 = test_000[0]\n",
    "_2k15 = test_000[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_2k11.columns), len(_2k15.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(_2k11.columns)), len(set(_2k15.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_15 = []\n",
    "repeat_15=0\n",
    "for i in _2k15.columns:\n",
    "    if i not in out_15:\n",
    "        out_15.append(i)\n",
    "    else:\n",
    "        print(i)\n",
    "        repeat_15+=1\n",
    "repeat_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_11 = []\n",
    "repeat_11 = 0\n",
    "for i in _2k11.columns:\n",
    "    if i not in out_11:\n",
    "        out_11.append(i)\n",
    "    else:\n",
    "        print(i)\n",
    "        repeat_11+=1        \n",
    "repeat_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''identify columns'''\n",
    "# 2011\n",
    "a = clean_census_frame('../data/acs/aff_download/ACS_11_5YR_DP05_with_ann.csv',reset=False)\n",
    "# 2012\n",
    "b = clean_census_frame('../data/acs/aff_download/ACS_12_5YR_DP05_with_ann.csv',reset=False)\n",
    "#2013\n",
    "c = clean_census_frame('../data/acs/aff_download/ACS_13_5YR_DP05_with_ann.csv',reset=False) \n",
    "# 2014\n",
    "d = clean_census_frame('../data/acs/aff_download/ACS_14_5YR_DP05_with_ann.csv',reset=False)\n",
    "# 2015\n",
    "e = clean_census_frame('../data/acs/aff_download/ACS_15_5YR_DP05_with_ann.csv',reset=False) \n",
    "#2016\n",
    "f = clean_census_frame('../data/acs/aff_download/ACS_16_5YR_DP05_with_ann.csv',reset=False) \n",
    "# 2017\n",
    "g = clean_census_frame('../data/acs/aff_download/ACS_17_5YR_DP05_with_ann.csv',reset=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [a,b,c,d,e,f,g]:\n",
    "    print(len(j.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''identify common columns'''\n",
    "# collection of columns appearing in all 7 dataframes 2011-2017\n",
    "common = set([tag for tag in g.columns if tag in a.columns & b.columns & c.columns & d.columns & e.columns & f.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''identify non common columns for specific frames'''\n",
    "# 2011\n",
    "u11 = [tag for tag in a.columns if tag not in common]\n",
    "# 2012\n",
    "u12 = [tag for tag in b.columns if tag not in common]\n",
    "# 2013\n",
    "u13 = [tag for tag in c.columns if tag not in common]\n",
    "# 2014\n",
    "u14 = [tag for tag in d.columns if tag not in common]\n",
    "# 2015\n",
    "u15 = [tag for tag in e.columns if tag not in common]\n",
    "# 2016\n",
    "u16 = [tag for tag in f.columns if tag not in common]\n",
    "# 2017\n",
    "u17 = [tag for tag in g.columns if tag not in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop each frame's uncommon columns, reset index\"\"\"\n",
    "# 2011\n",
    "aa = a.copy().drop(u11,axis=1).reset_index()\n",
    "# # 2012\n",
    "bb = b.copy().drop(u12,axis=1).reset_index()\n",
    "# # 2013\n",
    "cc = c.copy().drop(u13,axis=1).reset_index()\n",
    "# # 2014\n",
    "dd = d.copy().drop(u14,axis=1).reset_index()\n",
    "# # 2015\n",
    "ee = e.copy().drop(u15,axis=1).reset_index()\n",
    "# # 2016\n",
    "ff = f.copy().drop(u16,axis=1).reset_index()\n",
    "# # 2017\n",
    "gg = g.copy().drop(u17,axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[aa,bb,cc,dd,ee,ff,gg]\n",
    "for i in t:\n",
    "    print(len(i.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acol = aa.columns\n",
    "bcol=bb.columns\n",
    "ccol = cc.columns\n",
    "dcol=dd.columns\n",
    "ecol=ee.columns\n",
    "fcol=ff.columns\n",
    "gcol = gg.columns\n",
    "\n",
    "def test_non_unique(column_names):\n",
    "    first_occour = []\n",
    "    non_unique = []\n",
    "    for i,_ in enumerate(column_names):\n",
    "        if _ not in first_occour:\n",
    "            first_occour.append(_)\n",
    "        else:\n",
    "            non_unique.append([i,_])\n",
    "    return non_unique\n",
    "\n",
    "a=test_non_unique(acol)\n",
    "b=test_non_unique(bcol)\n",
    "c=test_non_unique(ccol)\n",
    "d=test_non_unique(dcol)\n",
    "e=test_non_unique(ecol)\n",
    "f=test_non_unique(fcol)\n",
    "g=test_non_unique(gcol)\n",
    "\n",
    "common_repeats = [_ for _ in g if _ in a]\n",
    "common_repeats = [i for i in b if i in common_repeats]\n",
    "common_repeats = [_ for _ in c if _ in common_repeats]\n",
    "common_repeats = [i for i in d if i in common_repeats]\n",
    "common_repeats = [_ for _ in e if _ in common_repeats]\n",
    "common_repeats = [i for i in f if i in common_repeats]\n",
    "common_repeats = [_ for _ in a if _ in common_repeats]\n",
    "# common_repeats \n",
    "for i in a:\n",
    "    if i in b:\n",
    "        if i not in c:\n",
    "            if i not in d:\n",
    "                if i not in e:\n",
    "                    if i not in f:\n",
    "                        if i not in g:\n",
    "                            print(i)\n",
    "common_repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***:\n",
    "    - the following occour as repeats in all years (2011-2017) \n",
    "        - [[80, 'Estimate; SEX AND AGE - 18 years and over'],\n",
    "        - [81, 'Margin of Error; SEX AND AGE - 18 years and over'],\n",
    "        - [82, 'Percent; SEX AND AGE - 18 years and over'],\n",
    "        - [83, 'Percent Margin of Error; SEX AND AGE - 18 years and over'],\n",
    "        - [84, 'Estimate; SEX AND AGE - 65 years and over'],\n",
    "        - [85, 'Margin of Error; SEX AND AGE - 65 years and over'],\n",
    "        - [86, 'Percent; SEX AND AGE - 65 years and over'],\n",
    "        - [87, 'Percent Margin of Error; SEX AND AGE - 65 years and over']]\n",
    "    - the following occour as repeats in 2011 and 2012 but no other years\n",
    "        - [100, 'Estimate; RACE - One race']\n",
    "        - [101, 'Margin of Error; RACE - One race']\n",
    "        - [102, 'Percent; RACE - One race']\n",
    "        - [103, 'Percent Margin of Error; RACE - One race']\n",
    "        - [188, 'Estimate; RACE - Two or more races']\n",
    "        - [189, 'Margin of Error; RACE - Two or more races']\n",
    "        - [190, 'Percent; RACE - Two or more races']\n",
    "        - [191, 'Percent Margin of Error; RACE - Two or more races']\n",
    "- ***actions***:\n",
    "    - remove the occourances only seen in 2011 and 2012\n",
    "        - ensure they lead to columns being equal (as assumed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***notes***;\n",
    "    - while different len, the column names are the same across the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [aa,bb,cc,dd,ee,ff,gg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa.columns), len(set(aa.columns)),len(aa.columns.unique())\n",
    "# pre set(common) on non-common lists (220, 204, 204)\n",
    "# post set(common) on non-common lists (220, 204, 204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [u11,u12,u13,u14,u15,u16,u17]\n",
    "y = out\n",
    "\n",
    "o = [len(_) for _ in x]\n",
    "so = [len(set(_)) for _ in x]\n",
    "\n",
    "co = [len(_.columns) for _ in y]\n",
    "cso = [len(set(_.columns)) for _ in y]\n",
    "\n",
    "years = ['2011','2012','2013','2014','2015','2016','2017']\n",
    "cols = ['list','set','diff']\n",
    "\n",
    "os = []\n",
    "for _ in range((len(o))):\n",
    "    os.append([o[_],so[_],o[_]-so[_]])\n",
    "    \n",
    "cos = []\n",
    "for _ in range((len(o))):\n",
    "    cos.append([co[_],cso[_],co[_]-cso[_]]) \n",
    "    \n",
    "q = pd.DataFrame(data=os,index=years,columns=cols)\n",
    "cq = pd.DataFrame(data=cos,index=years,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reallycommon = [_.columns for _ in y]\n",
    "# unicommon = [_.columns.unique() for _ in y]\n",
    "# # len(reallycommon[0]),len(unicommon[0])\n",
    "# r=reallycommon[0].drop(reallycommon.index[[79,80]])\n",
    "# ur=[unicommon][0][0]\n",
    "# for u,i in enumerate(r):\n",
    "#     if r[u]!=ur[u]:\n",
    "#         print(f'{u}\\n{r[u]}\\n{ur[u]}\\n')\n",
    "# # len(r),len(ur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gg.columns), len(set(gg.columns)),len(gg.columns.unique())\n",
    "# pre set(common) on non-common lists (212, 204, 204)\n",
    "# post set(common) on non-common lists (212, 204, 204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common), len(set(common)), len(common)\n",
    "# pre set(common) on non-common lists (211, 203, 203)\n",
    "# post set(common) on non-common lists (211, 203, 203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa['Geography']) , len(aa['Geography'].unique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_classification(n_classes=3, n_features=2, n_redundant=0,\n",
    "#                            n_informative=2, n_clusters_per_class=1,\n",
    "#                            class_sep=1, random_state=5)\n",
    "# print(y.shape)\n",
    "# _knn = KNearestNeighbors(4, cosine_distance)\n",
    "# _knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load 2011 \n",
    "# y2k11 = clean_census_frame('../data/acs/aff_download/ACS_11_5YR_DP05_with_ann.csv',reset=False)\n",
    "# # copy\n",
    "# y11 = (y2k11.copy())\n",
    "# # 2012\n",
    "# y2k12 = clean_census_frame('../data/acs/aff_download/ACS_12_5YR_DP05_with_ann.csv',reset=False)\n",
    "# y12 = y2k12.copy()\n",
    "# #2013\n",
    "# y2k13 = clean_census_frame('../data/acs/aff_download/ACS_13_5YR_DP05_with_ann.csv',reset=False)\n",
    "# y13 = y2k13.copy()\n",
    "# # 2014\n",
    "# y2k14 = clean_census_frame('../data/acs/aff_download/ACS_14_5YR_DP05_with_ann.csv',reset=False)\n",
    "# y14 = y2k14.copy()\n",
    "# # 2015\n",
    "# y2k15 = clean_census_frame('../data/acs/aff_download/ACS_15_5YR_DP05_with_ann.csv',reset=False)\n",
    "# y15 = y2k15.copy()\n",
    "# #2016\n",
    "# y2k16 = clean_census_frame('../data/acs/aff_download/ACS_16_5YR_DP05_with_ann.csv',reset=False)\n",
    "# y16 = y2k16.copy()\n",
    "# #2017\n",
    "# y2k17 = clean_census_frame('../data/acs/aff_download/ACS_17_5YR_DP05_with_ann.csv',reset=False)\n",
    "# y17 = y2k17.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
