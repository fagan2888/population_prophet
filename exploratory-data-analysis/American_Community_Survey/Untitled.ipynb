{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 0 >> imports; def clean_census & other functions\n",
    "\"\"\"\n",
    "\n",
    "# default cleaning method until proven otherwise\n",
    "def clean_census_frame(csv_path , head=False , reset=True , set_index=False ):\n",
    "    '''\n",
    "    inputs) \n",
    "        >> csv_path\n",
    "            > path to csv\n",
    "        >> head\n",
    "            > default=False\n",
    "                >> if != False\n",
    "                    > integer\n",
    "                        >> returns the first {head} rows (using .head() method) \n",
    "                            > instead of enitre dataframe\n",
    "        >> reset\n",
    "            > default=True\n",
    "                >> resets index after taking out rows\n",
    "            > if set to False\n",
    "                >> will not reset index\n",
    "        >> set_index\n",
    "            > default=False\n",
    "            > if != False\n",
    "                >> will set_index of new df to set_index\n",
    "    output)\n",
    "        >> dataframe cleaned like 2000 Census age&sex by 5-digit Zip Code (also how 2010 for same is cleaned)\n",
    "    how)\n",
    "        1. reads in csv , assumes it's large\n",
    "        2. makes a copy for editing \n",
    "            > and potential future use\n",
    "        3. locates readable column names  and non-readable names \n",
    "            > readable\n",
    "                    > e.g. Estimate; SEX AND AGE - Total population\n",
    "                >> assumes they are currently in row 0\n",
    "            > non-readable\n",
    "                    > e.g. HC01_VC03\n",
    "                >> assumes they are currently == dataframe.columns\n",
    "        4. replaces dataframe.columns (non-readable) with readable column names\n",
    "            > and drops the old 0th column (column where readable names were stored)\n",
    "        \n",
    "    '''\n",
    "    # load data\n",
    "    df = pd.read_csv( csv_path , low_memory=False )\n",
    "\n",
    "    # and copy\n",
    "    _df = df.copy()\n",
    "\n",
    "    # reset column names to current 0th row values\n",
    "    _df.columns = _df.iloc[0]\n",
    "    # new 2000 dataframe without row where values are from\n",
    "    clean_df = _df[1:]\n",
    "    \n",
    "    # default\n",
    "    if reset==True:\n",
    "        # reset index\n",
    "        clean_df = clean_df.reset_index()\n",
    "        \n",
    "    # set_index\n",
    "    if set_index:\n",
    "        clean_df = clean_df.set_index(set_index)\n",
    "    \n",
    "    if head:\n",
    "        # return first {head} rows of dataframe\n",
    "        return clean_df.head(head)\n",
    "    else:\n",
    "        # return dataframe\n",
    "        return clean_df\n",
    "\n",
    "'''\n",
    "STEP 1 >> load data, reset; make copies\n",
    "'''\n",
    "def load_copy_data(i):\n",
    "    '''\n",
    "    loads data\n",
    "    \n",
    "    input)\n",
    "        >> i\n",
    "            > if 0\n",
    "                >> .reset_index() after deleting row contining column names\n",
    "            > if 1\n",
    "                >> do not .reset_index()\n",
    "                \n",
    "    '''\n",
    "    if i==0:\n",
    "        # load with reset\n",
    "        # 2011 \n",
    "        twenty_eleven = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_11_5YR_DP05_with_ann.csv')\n",
    "        # 2012\n",
    "        # twenty_twelve = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_12_5YR_DP05_with_ann.csv')\n",
    "        #2013\n",
    "        # twenty_thirteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_13_5YR_DP05_with_ann.csv')\n",
    "        # 2014\n",
    "        # twenty_fourteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_14_5YR_DP05_with_ann.csv')\n",
    "        # 2015\n",
    "        # twenty_fifteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_15_5YR_DP05_with_ann.csv')\n",
    "        #2016\n",
    "        # twenty_sixteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_16_5YR_DP05_with_ann.csv')\n",
    "        #2017\n",
    "        # twenty_seventeen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_17_5YR_DP05_with_ann.csv')\n",
    "    if i==1:\n",
    "        # load without reset\n",
    "        # 2011 \n",
    "        twenty_eleven = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_11_5YR_DP05_with_ann.csv',reset=False)\n",
    "        # 2012\n",
    "        # twenty_twelve = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_12_5YR_DP05_with_ann.csv',reset=False)\n",
    "        #2013\n",
    "        # twenty_thirteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_13_5YR_DP05_with_ann.csv',reset=False)\n",
    "        # 2014\n",
    "        # twenty_fourteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_14_5YR_DP05_with_ann.csv',reset=False)\n",
    "        # 2015\n",
    "        # twenty_fifteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_15_5YR_DP05_with_ann.csv',reset=False)\n",
    "        #2016\n",
    "        # twenty_sixteen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_16_5YR_DP05_with_ann.csv',reset=False)\n",
    "        #2017\n",
    "        # twenty_seventeen = clean_census_frame('../../data/American_Community_Survey/aff_download/ACS_17_5YR_DP05_with_ann.csv',reset=False)\n",
    "    \n",
    "    # copy \n",
    "    # 2011 \n",
    "    _y2k11 = twenty_eleven.copy()\n",
    "    # 2012\n",
    "    # _y2k12 = twenty_twelve.copy()\n",
    "    #2013\n",
    "    # _y2k13 = twenty_thirteen.copy()\n",
    "    # 2014\n",
    "    # _y2k14 = twenty_fourteen.copy()\n",
    "    # 2015\n",
    "    # _y2k15 = twenty_fifteen.copy()\n",
    "    #2016\n",
    "    # _y2k16 = twenty_sixteen.copy()\n",
    "    #2017\n",
    "    # _y2k17 = twenty_seventeen.copy()\n",
    "    \n",
    "    # output list of copied frames\n",
    "    return _y2k11  # [_y2k11,_y2k12,_y2k13,_y2k14,_y2k15,_y2k16,_y2k17]\n",
    "\n",
    "\n",
    "def test_non_unique(column_names):\n",
    "    '''\n",
    "    input) \n",
    "        >> list of column names {column_names}\n",
    "            > columns to check for duplicate instances\n",
    "    output)\n",
    "        >> indexed list of names occouring more than once \n",
    "    '''\n",
    "    # store first instance\n",
    "    first_occour = []\n",
    "    # store 2nd+ instance(s)\n",
    "    non_unique = []\n",
    "    # we're going to want index\n",
    "    for i,_ in enumerate(column_names):\n",
    "        # not first time\n",
    "        if _ not in first_occour:\n",
    "            first_occour.append(_)\n",
    "        # if not first, tag&bag\n",
    "        else:\n",
    "            non_unique.append([i,_])\n",
    "    # output index w/ non-first instances\n",
    "    return non_unique\n",
    "\n",
    "\n",
    "def to_numeric_but(save_these_columns,dataframe):\n",
    "    '''\n",
    "    split into 2 df and rejoin after convert to int\n",
    "    \n",
    "    inputs:\n",
    "        >> save_these_columns=number of columns to save\n",
    "            > currently must include one end of df \n",
    "                >> might could run function multiple times to edit slices\n",
    "                >> single number, not range (yet)\n",
    "        >> dataframe\n",
    "            > dataframe to shif to numeric (but)\n",
    "    output:\n",
    "        >> concatted pd.DataFrame of \n",
    "            > og columns you chose to save\n",
    "            > columns converted to numeric\n",
    "    '''\n",
    "    # copy df for editing\n",
    "    k = dataframe.copy()\n",
    "\n",
    "    # columns to save\n",
    "    save_k = k[k.columns[:save_these_columns]]\n",
    "    # columns to edit\n",
    "    switch_k = k[k.columns[save_these_columns:]]\n",
    "\n",
    "    # edited columns  # coerce , ignore , raise\n",
    "    swapped_k = switch_k.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # new (edited) dataframe\n",
    "    new_k = pd.concat([save_k,switch_k],axis=1)\n",
    "\n",
    "    return new_k\n",
    "\n",
    "\n",
    "def geography_to_zipcode_ids_to_numeric(dataframe):\n",
    "    '''\n",
    "    convert \n",
    "        >> .Geography values \n",
    "            > like 'ZCTA5 00601' \n",
    "            > to int(00601)\n",
    "        >> .Id values\n",
    "            > like '8600000US00601' \n",
    "            > to int(860000000601)\n",
    "        >> .Id2 values\n",
    "            > like '00601'\n",
    "            > to int(00601)\n",
    "    '''\n",
    "    # copy\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # set old Geography\n",
    "    geo = df.Geography\n",
    "    # set old Id\n",
    "    _id = df.Id\n",
    "    # set old Id2\n",
    "    __id2 = df.Id2\n",
    "    \n",
    "    # make new 'Geography' values\n",
    "    new_geos = [int(i[-5:]) for i in geo]\n",
    "    # new 'Id' values\n",
    "    new_id = [int(''.join(i.split('US'))) for i in _id]\n",
    "    # new .Id2 instances\n",
    "    new__id2 = [int(d) for d in __id2]\n",
    "    \n",
    "    # convert dataframe\n",
    "    new_df = df.copy()\n",
    "    new_df.Geography = new_geos\n",
    "    new_df.Id = new_id\n",
    "    new_df.Id2 = new__id2\n",
    "    \n",
    "    # return new df\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(X, k=5, max_iter=1000):\n",
    "    \"\"\"Performs k means\n",
    "\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - k - number of clusters\n",
    "    - max_iter - maximum iterations\n",
    "\n",
    "    Returns:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "    centers = [tuple(pt) for pt in random.sample(list(X), k)]\n",
    "    for i in range(max_iter):\n",
    "        clusters = defaultdict(list)\n",
    "\n",
    "        for datapoint in X:\n",
    "            distances = [euclidean(datapoint, center) for center in centers]\n",
    "            center = centers[np.argmin(distances)]\n",
    "            clusters[center].append(datapoint)\n",
    "\n",
    "        new_centers = []\n",
    "        for center, pts in clusters.items():\n",
    "            new_center = np.mean(pts, axis=0)\n",
    "            new_centers.append(tuple(new_center))\n",
    "\n",
    "        if set(new_centers) == set(centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def sse(clusters):\n",
    "    \"\"\"Sum squared euclidean distance of all points to their cluster center\"\"\"\n",
    "    sum_squared_residuals = 0\n",
    "    for center, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            sum_squared_residuals += euclidean(pt, center)**2\n",
    "    return sum_squared_residuals\n",
    "\n",
    "\n",
    "def plot_k_sse(X, min_k, max_k):\n",
    "    \"\"\"Plots sse for values of k between min_k and max_k\n",
    "\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - min_k, max_k - smallest and largest k to plot sse for\n",
    "    \"\"\"\n",
    "    k_values = range(min_k, max_k+1)\n",
    "    sse_values = []\n",
    "    for k in k_values:\n",
    "        clusters = k_means(X, k=k)\n",
    "        sse_values.append(sse(clusters))\n",
    "    plt.plot(k_values, sse_values)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('sum squared error')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def turn_clusters_into_labels(clusters):\n",
    "    \"\"\"Converts clusters dict returned by k_means into X, y (labels)\n",
    "\n",
    "    Args:\n",
    "    - clusters - dict mapping cluster centers to observations\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    new_X = []\n",
    "    label = 0\n",
    "    for cluster, pts in clusters.items():\n",
    "        for pt in pts:\n",
    "            new_X.append(pt)\n",
    "            labels.append(label)\n",
    "        label += 1\n",
    "    return np.array(new_X), np.array(labels)\n",
    "\n",
    "\n",
    "def plot_k_silhouette(X, min_k, max_k):\n",
    "    \"\"\"Plots sse for values of k between min_k and max_k\n",
    "\n",
    "    Args:\n",
    "    - X - feature matrix\n",
    "    - min_k, max_k - smallest and largest k to plot sse for\n",
    "    \"\"\"\n",
    "    k_values = range(min_k, max_k+1)\n",
    "    silhouette_scores = []\n",
    "    for k in k_values:\n",
    "        clusters = k_means(X, k=k)\n",
    "        new_X, labels = turn_clusters_into_labels(clusters)\n",
    "        silhouette_scores.append(silhouette_score(new_X, labels))\n",
    "\n",
    "    plt.plot(k_values, silhouette_scores)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('silhouette score')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all_2d(X, feature_names, k=3):\n",
    "    \"\"\"Generates all possible 2d plots of observations color coded by cluster ID\"\"\"\n",
    "    pairs = list(combinations(range(X.shape[1]), 2))\n",
    "    fig, axes = plt.subplots((len(pairs) // 2), 2)\n",
    "    flattened_axes = [ax for ls in axes for ax in ls]\n",
    "\n",
    "    for pair, ax in zip(pairs, flattened_axes):\n",
    "        pair = np.array(pair)\n",
    "        plot_data_2d(X[:, pair], feature_names[pair], ax, k=k)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_2d(X, plot_labels, ax, k=3):\n",
    "    \"\"\"Generates single 2d plot of observations color coded by cluster ID\"\"\"\n",
    "    clusters = k_means(X, k=k)\n",
    "    new_X, labels = turn_clusters_into_labels(clusters)\n",
    "    ax.scatter(new_X[:, 0], new_X[:, 1], c=labels)\n",
    "    ax.set_xlabel(plot_labels[0])\n",
    "    ax.set_ylabel(plot_labels[1])\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     iris = datasets.load_iris()\n",
    "#     X = iris.data\n",
    "#     plot_k_sse(X, 2, 10)\n",
    "#     plot_k_silhouette(X, 2, 10)\n",
    "#     plot_all_2d(X, np.array(iris.feature_names), k=5)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load w/o reset\n",
    "f = load_copy_data(1)\n",
    "# copy for safeguard and hedge reload\n",
    "frames = f.copy()\n",
    "# extract copy of 2011 \n",
    "y2k11 = frames.copy()\n",
    "# examine (2011)\n",
    "y2k11.info()\n",
    "nuy11 = test_non_unique(y2k11)\n",
    "len(nuy11)\n",
    "# convert all but first 3 columns to numeric\n",
    "data = y2k11.copy()\n",
    "k2011 = to_numeric_but(save_these_columns=3,dataframe=data)\n",
    "k2011.info()\n",
    "nonuni = test_non_unique(k2011)\n",
    "len(nonuni)\n",
    "# now convert the first 3 columns\n",
    "adjust_first_3 = k2011.copy()\n",
    "_2011df_ = geography_to_zipcode_ids_to_numeric(adjust_first_3)\n",
    "# drop nans\n",
    "_2011df_ = _2011df_.dropnan(axis=1)  # , how='any'\n",
    "\n",
    "# Convert DataFrame to matrix\n",
    "mat = _2011df_.values\n",
    "# Using sklearn\n",
    "km = KMeans(n_clusters=5)\n",
    "km.fit(mat)\n",
    "# Get cluster assignment labels\n",
    "labels = km.labels_\n",
    "# Format results as a DataFrame\n",
    "results = pandas.DataFrame([dataset.index,labels]).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
